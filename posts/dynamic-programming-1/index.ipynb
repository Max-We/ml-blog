{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Value & Policy Iteration\"\n",
    "author: \"Maximilian Weichart\"\n",
    "date: \"February 19, 2024\"\n",
    "image: images/grid.png\n",
    "description: Python implementations for Value & Policy iteration, two fundamental concepts of Reinforcement Learning.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w_aABFZbunk2"
   },
   "source": [
    "In this article, I implemented a simple example for value and policy iteration from scratch in Python.\n",
    "\n",
    "::: {.callout-note}\n",
    "This article contains fewer explanations than usual. It is meant as a reference for other students getting into the subject, who are looking for a simple-as-possible reference implementation. The code can be improved quite a lot, but I wanted to provide these examples nonetheless instead of keeping them on my computer only.\n",
    ":::\n",
    "\n",
    "Because implement the concepts from scratch, the only `import` we'll use is NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "HGLCaUms9nou"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5DBgZJT39bUg"
   },
   "source": [
    "## MDP\n",
    "\n",
    "In order to do value- and policy iteration, we'll want to model the MDP. In this case, our \"world\" will be a 2D grid, where each cell has a reward value.\n",
    "\n",
    "To make life easier, I'll create a wrapper around the numpy array that contains the data, to introduce some convenient functions that simplify the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "79PoYwj172XQ"
   },
   "outputs": [],
   "source": [
    "class Grid:\n",
    "    def __init__(self, matrix):\n",
    "        self.matrix = np.array(matrix)\n",
    "\n",
    "    def get_val(self, state):\n",
    "        r, c = self.coord_2_index(state)\n",
    "        return self.matrix[r, c]\n",
    "\n",
    "    def set_val(self, state, value):\n",
    "        r, c = self.coord_2_index(state)\n",
    "        self.matrix[r, c] = value\n",
    "\n",
    "    def coord_2_index(self, state):\n",
    "        x, y = state\n",
    "        return len(self.matrix) - y, x - 1\n",
    "\n",
    "    def equals(self, grid):\n",
    "        return np.array_equal(self.matrix, grid.matrix)\n",
    "\n",
    "    def display(self):\n",
    "        print(self.matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wSEMCjQ2-KT1"
   },
   "source": [
    "To perform the updates using the Bellman equations, I'll create another class modelling the MDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "DYWRXjL674Lu"
   },
   "outputs": [],
   "source": [
    "class Mdp:\n",
    "    def __init__(self, states, actions, gamma):\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def T(s1, a, s2):\n",
    "        pass\n",
    "\n",
    "    def R(s1, a, s2):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LP-1SRdK-pkZ"
   },
   "source": [
    "Our grid-world example has a few specific transitions (NESW) and a simple reward function. So I'll create a new class that can model a grid-world of any size provided by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t-4swrRt-pBa"
   },
   "outputs": [],
   "source": [
    "class MdpGrid(Mdp):\n",
    "    def __init__(self, grid, terminal_states, gamma):\n",
    "        self.grid = grid\n",
    "        self.terminal_states = terminal_states\n",
    "        states = [(x, y) for x in range(1, len(self.grid.matrix) + 1) for y in range(1, len(self.grid.matrix[0]) + 1)]\n",
    "        actions = [\"N\", \"E\", \"S\", \"W\"]\n",
    "\n",
    "        super().__init__(states, actions, gamma)\n",
    "\n",
    "    def T(self, s1, a, s2):\n",
    "        if s1 in self.terminal_states:\n",
    "            return 0\n",
    "\n",
    "        transitions = {\n",
    "            \"N\": (0, 1),\n",
    "            \"E\": (1, 0),\n",
    "            \"S\": (0, -1),\n",
    "            \"W\": (-1, 0),\n",
    "        }\n",
    "        x, y = s1\n",
    "        x2, y2 = s2\n",
    "        dx, dy = transitions[a]  # Deterministic actions assumed\n",
    "        return int((x + dx, y + dy) == s2)\n",
    "\n",
    "    def R(self, s1, a, s2):\n",
    "        return self.grid.get_val(s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6B-8HlbR8Cm8"
   },
   "source": [
    "In this notebook, we'll experiment with a simple `4x4` grid. It has two rewards and ends as soon as the agent collects one of them.\n",
    "\n",
    "![Visualization of the ` MdpGrid4x4` world](images/grid.png){#fig-grid}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dRo-gZL676WF"
   },
   "outputs": [],
   "source": [
    "class MdpGrid4x4(MdpGrid):\n",
    "    def __init__(self, gamma):\n",
    "        matrix = [\n",
    "            [0, 0, 0, 1],\n",
    "            [0, 0, 0, -1],\n",
    "            [0, 0, 0, 0],\n",
    "            [0, 0, 0, 0],\n",
    "        ]\n",
    "        grid = Grid(matrix)\n",
    "        super().__init__(grid, [(4, 4), (4, 3)], gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BlujORDx8Rap"
   },
   "source": [
    "## Value Iteration\n",
    "\n",
    "$$\n",
    "V_{k+1}(s) \\leftarrow \\max_a \\sum_{s'} T(s, a, s') \\left[ R(s, a, s') + \\gamma V_k(s') \\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "oLmwn5yh8S1P"
   },
   "outputs": [],
   "source": [
    "class ValueIteration():\n",
    "    def __init__(self, mdp):\n",
    "        self.mdp = mdp\n",
    "\n",
    "    def V(self, s, k):\n",
    "        if k <= 0: return 0\n",
    "\n",
    "        values = []\n",
    "\n",
    "        for s2 in self.mdp.states:\n",
    "            for a in self.mdp.actions:\n",
    "                # Calculate components\n",
    "                t = self.mdp.T(s, a, s2)\n",
    "                r = self.mdp.R(s, a, s2)\n",
    "                v2 = self.V(s2, k - 1) if t > 0 else 0\n",
    "\n",
    "                # Insert components into formula\n",
    "                v = t * (r + self.mdp.gamma * v2)\n",
    "\n",
    "                # Save to results\n",
    "                values.append(v)\n",
    "\n",
    "        return max(values)\n",
    "\n",
    "    def run(self, k):\n",
    "        shape = self.mdp.grid.matrix.shape\n",
    "        values = Grid(np.zeros(shape))\n",
    "\n",
    "        for s in self.mdp.states:\n",
    "            values.set_val(s, np.around(self.V(s,k), 2))\n",
    "\n",
    "        return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ot-rQXu_8F5G"
   },
   "source": [
    "## Policy Iteration\n",
    "\n",
    "$$\n",
    "V^{\\pi_{i}}_{k+1}(s) \\leftarrow \\sum_{s'} T(s, \\pi_{i}(s), s') \\left[ R(s, \\pi_{i}(s), s') + \\gamma V^{\\pi_{i}}_{k}(s') \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "o6dLPQqp79lh"
   },
   "outputs": [],
   "source": [
    "class PolicyIteration():\n",
    "    def __init__(self, mdp):\n",
    "        self.mdp = mdp\n",
    "\n",
    "    def policy_evaluation(self, s, pi, k):\n",
    "        if k <= 0:\n",
    "            return 0\n",
    "        value_sum = 0\n",
    "        for s2 in self.mdp.states:\n",
    "            a = pi.get_val(s)\n",
    "            t = self.mdp.T(s, a, s2)\n",
    "            r = self.mdp.R(s, a, s2)\n",
    "            v2 = self.policy_evaluation(s2, pi, k - 1) if t > 0 else 0\n",
    "            value_sum += t * (r + self.mdp.gamma * v2)\n",
    "        return value_sum\n",
    "\n",
    "    def policy_improvement(self, pi, eval_k):\n",
    "        new_pi = Grid(pi.matrix.copy())\n",
    "        for s in self.mdp.states:\n",
    "            action_values = []\n",
    "            for a in self.mdp.actions:\n",
    "                pi_copy = Grid(new_pi.matrix.copy())\n",
    "                pi_copy.set_val(s, a)\n",
    "                v = self.policy_evaluation(s, pi_copy, eval_k)\n",
    "                action_values.append((v, a))\n",
    "\n",
    "            best_action = max(action_values, key=lambda x: x[0])[1]\n",
    "            new_pi.set_val(s, best_action)\n",
    "\n",
    "        return new_pi\n",
    "\n",
    "    def run(self, pi, policy_iterations, value_iterations):\n",
    "        iter_pi = pi\n",
    "        for i in range(policy_iterations):\n",
    "            new_pi = self.policy_improvement(iter_pi, value_iterations)\n",
    "            if new_pi.equals(iter_pi):\n",
    "                break\n",
    "            iter_pi = new_pi\n",
    "        return iter_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gonvuooc8H_K"
   },
   "source": [
    "## Examples\n",
    "\n",
    "Let's initialize a new grid world and run policy and value iteration on both of them to see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "P_aJsv5c7-3n"
   },
   "outputs": [],
   "source": [
    "mdp_4x4 = MdpGrid4x4(0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jzbbB3DB-LiZ",
    "outputId": "1a832c6b-5f30-4a6c-81f3-b82ca3499565"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9  0.95 1.   0.  ]\n",
      " [0.86 0.9  0.95 0.  ]\n",
      " [0.81 0.86 0.9  0.86]\n",
      " [0.   0.81 0.86 0.81]]\n"
     ]
    }
   ],
   "source": [
    "value_iter = ValueIteration(mdp_4x4)\n",
    "\n",
    "resulting_values = value_iter.run(5)\n",
    "resulting_values.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Aetm8L2c7_gK",
    "outputId": "13c1912e-86eb-41fe-eeae-9f2091aaa9c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['E' 'E' 'E' 'N']\n",
      " ['N' 'N' 'N' 'N']\n",
      " ['N' 'N' 'N' 'W']\n",
      " ['N' 'N' 'N' 'N']]\n"
     ]
    }
   ],
   "source": [
    "policy_iter = PolicyIteration(mdp_4x4)\n",
    "\n",
    "my_pi_matrix = [[\"N\" for _ in range(4)] for _ in range(4)]\n",
    "my_pi = Grid(my_pi_matrix)\n",
    "\n",
    "resulting_policy = policy_iter.run(my_pi, 15, 10)\n",
    "resulting_policy.display()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
