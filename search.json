[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Set-to-Set: An extension of the Seq-to-Seq paradigm using attention\n\n\n\n\n\nNot all data is sequential. How can neural networks be applied to non-sequential data like sets, when the Seq-to-Seq paradigm fails?\n\n\n\n\n\n\nOct 6, 2024\n\n\nMaximilian Weichart, Noah Meißner\n\n\n\n\n\n\n  \n\n\n\n\nUniform Sampling Inside a Circle\n\n\n\n\n\nAn alternative approach to sample points uniformly inside a circle.\n\n\n\n\n\n\nJul 1, 2024\n\n\nMaximilian Weichart\n\n\n\n\n\n\n  \n\n\n\n\nValue & Policy Iteration\n\n\n\n\n\nPython implementations for Value & Policy iteration, two fundamental concepts of Reinforcement Learning.\n\n\n\n\n\n\nFeb 19, 2024\n\n\nMaximilian Weichart\n\n\n\n\n\n\n  \n\n\n\n\nBackpropagation in RNNs\n\n\n\n\n\nHow to work out the derivatives and implement the training loop in pure Python.\n\n\n\n\n\n\nJan 31, 2024\n\n\nMaximilian Weichart\n\n\n\n\n\n\n  \n\n\n\n\nSimulated Annealing\n\n\n\n\n\nHow can we make Entropy work for - and not against us.\n\n\n\n\n\n\nNov 25, 2023\n\n\nMaximilian Weichart\n\n\n\n\n\n\n  \n\n\n\n\nUninformed Search Algorithms\n\n\n\n\n\nIntroduction to search algorithms - a fundamental for understanding artificial intelligence.\n\n\n\n\n\n\nSep 23, 2023\n\n\nMaximilian Weichart\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to RNNs\n\n\n\n\n\nRecurrent Neural Networks are a fundamental concept to understand and offer strengths that uni-directional neural networks lack. How can we use one to predict stock prices?\n\n\n\n\n\n\nAug 12, 2023\n\n\nMaximilian Weichart\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/set-to-set/index.html",
    "href": "posts/set-to-set/index.html",
    "title": "Set-to-Set: An extension of the Seq-to-Seq paradigm using attention",
    "section": "",
    "text": "Sequence-to-sequence (Seq-to-Seq) models are a common solution for tasks with a natural sequential structure, such as translating sentences into another language (Sutskever, Vinyals, and Le 2014). These models are particularly useful because they can handle variable-length inputs and outputs. But what happens when the underlying task has no order?\nSorting a set of numbers is a good example, which we explore in this article. Here, a Seq-to-Seq model would not work optimally because the order of the numbers in the input data could affect the result (Vinyals, Bengio, and Kudlur 2015). This highlights the limitations of Seq-to-Seq when it comes to processing sets rather than sequences.\nIn the following sections, we will explore how to develop an architecture that can handle such Set-to-Set tasks. The implementations presented are based on theory from several scientific papers, including concepts such as attention (Graves, Wayne, and Danihelka 2014), pointer networks (Vinyals, Fortunato, and Jaitly 2015), and the Read-Process-Write architecture (Vinyals, Bengio, and Kudlur 2015)."
  },
  {
    "objectID": "posts/set-to-set/index.html#motivation",
    "href": "posts/set-to-set/index.html#motivation",
    "title": "Set-to-Set: An extension of the Seq-to-Seq paradigm using attention",
    "section": "",
    "text": "Sequence-to-sequence (Seq-to-Seq) models are a common solution for tasks with a natural sequential structure, such as translating sentences into another language (Sutskever, Vinyals, and Le 2014). These models are particularly useful because they can handle variable-length inputs and outputs. But what happens when the underlying task has no order?\nSorting a set of numbers is a good example, which we explore in this article. Here, a Seq-to-Seq model would not work optimally because the order of the numbers in the input data could affect the result (Vinyals, Bengio, and Kudlur 2015). This highlights the limitations of Seq-to-Seq when it comes to processing sets rather than sequences.\nIn the following sections, we will explore how to develop an architecture that can handle such Set-to-Set tasks. The implementations presented are based on theory from several scientific papers, including concepts such as attention (Graves, Wayne, and Danihelka 2014), pointer networks (Vinyals, Fortunato, and Jaitly 2015), and the Read-Process-Write architecture (Vinyals, Bengio, and Kudlur 2015)."
  },
  {
    "objectID": "posts/set-to-set/index.html#setup",
    "href": "posts/set-to-set/index.html#setup",
    "title": "Set-to-Set: An extension of the Seq-to-Seq paradigm using attention",
    "section": "2 Setup",
    "text": "2 Setup\n\n\n\n\n\n\nNote\n\n\n\nIn order to understand the following content, we assume basic knowledge of neural networks, backpropagation and Seq-to-Seq models. We will rely on the PyTorch library for the implementation. We have provided further literature for each approach. This can be used both to deepen your knowledge and to go deeper into a topic if you are having difficulty understanding it.\n\n\n\n2.1 Dataset\nAs a benchmark for the models in this experiment, we will conduct a number-sorting experiment similar to Vinyals, Bengio, and Kudlur (2015) on arrays with a length \\(s\\) (controlled by a hyperparameter) containing numbers normalized to \\([-1;1]\\). The network will receive an unordered array \\(x\\) and has to learn to output an ordered array \\(y\\).\n\n\nHyperparameters\n# Dataset\nSEQ_LENGTH = 5\nNUM_SAMPLES = 2000\n\n# Model\nITEM_SIZE = 1\nEMBEDDING_SIZE = 32\nHIDDEN_SIZE = 32\nBATCH_SIZE = 256\nPROCESS_STEPS = 5 # RPW model\n\n# Training\nLR = 0.01\nN_EPOCHS = 250\n\n\n\nclass DigitSortDataset(Dataset):\n    def __init__(self, num_samples, seq_length):\n        self.num_samples = num_samples\n        self.seq_length = seq_length\n        self.data = self.generate_data()\n\n    def generate_data(self):\n        data = []\n        for _ in range(self.num_samples):\n            sequence = [random.random() for _ in range(self.seq_length)]\n            unsorted = torch.tensor(sequence, dtype=torch.float32)\n            sorted_seq = torch.sort(unsorted)[0]\n            data.append((unsorted, sorted_seq))\n        return data\n\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n\n\n\nDataloader creation\n# Create the datasets\ntrain_val_dataset = DigitSortDataset(num_samples=NUM_SAMPLES, seq_length=SEQ_LENGTH)\n\n# Split the train_val_dataset\ntrain_size = int(0.8 * len(train_val_dataset))\nval_size = len(train_val_dataset) - train_size\n\ntrain_dataset, val_dataset = random_split(\n    train_val_dataset, [train_size, val_size]\n)\n\n# Create DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n\n\nFor the rest of this article, we will be using arrays with \\(s=5\\), but the reader can adjust this value in the Jupyter Notebook. An example for a training pair \\((x,y)\\) from the data loader looks as follows:\n\n\nExample x: tensor([0.8946, 0.3203, 0.5510, 0.4022, 0.6636])\nExample y: tensor([0.3203, 0.4022, 0.5510, 0.6636, 0.8946])\n\n\n\n\n2.2 Training functions\nAs a final step for the setup, we will define a few functions which will be used to train and evaluate the models in the next few sections. You can ignore these functions for now, but if you’re interested in how they work, you can unfold the code-blok below.\n\n\nTraining,\ndef train(model, dataloader, use_cross_entropy):\n    criterion = nn.CrossEntropyLoss() if use_cross_entropy else nn.MSELoss()\n    optimizer = optim.Adam(model.parameters(), lr=LR)\n\n    model.train()\n    total_loss = 0\n    for epoch in range(N_EPOCHS):\n        total_loss = 0\n        for batch in dataloader:\n            unsorted, sorted_seq = batch\n\n            prediction = model(unsorted)\n\n            if use_cross_entropy: # cross entropy if target are indices\n                _, indices = torch.sort(unsorted, dim=-1)\n                target = torch.argsort(indices, dim=-1)\n            else: # MSE if target are values\n                target = sorted_seq\n\n            loss = criterion(prediction, target)\n\n            # Backward pass and optimize\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n    final_loss = total_loss / len(dataloader)\n    return model, final_loss\n\ndef probs_to_x(probabilities, unsorted_sequence):\n    indices = torch.argmax(probabilities, dim=-1)  # Shape: [255, 5]\n    batch_size, seq_len = unsorted_sequence.shape\n    batch_indices = torch.arange(batch_size).unsqueeze(1).expand(-1, seq_len)\n    return unsorted_sequence[batch_indices, indices]  # Shape: [255, 5]\n\ndef evaluate(model, dataloader):\n    model.eval()\n    n = 0\n    n_right = 0\n    total_divergence = 0\n    with torch.no_grad():\n        for batch in dataloader:\n            unsorted, sorted_seq = batch\n\n            output = model(unsorted)\n            if len(output.shape) == 3:\n                prediction = probs_to_x(output, unsorted)\n            else:\n                prediction = output\n\n            total_divergence += (sorted_seq - prediction).abs().sum().item()\n            n_right += (sorted_seq == prediction).sum().item()\n            n += sorted_seq.size(0) * sorted_seq.size(1)\n\n    accuracy = f\"{((n_right / n) * 100):.2f}%\"\n    avg_divergence = total_divergence / n\n    return accuracy, avg_divergence\n\ndef report_training_results(model_name, accuracy, divergence, embedding_size=EMBEDDING_SIZE):\n    \"\"\"Function report training results and hyperparameters in a table\"\"\"\n    data = {\n        'Metric': ['Model', 'Embedding Size', 'Sequence Length', 'Training Epochs', 'Accuracy', 'Avg. Divergence'],\n        'Value': [model_name, embedding_size, SEQ_LENGTH, N_EPOCHS, accuracy, divergence]\n    }\n    \n    df = pd.DataFrame(data)\n    return df\n\n\ndef inspect(model, dataloader, is_pointer_network):\n    \"\"\"Function to look at model predictions / target values directly\"\"\"\n    model.eval()\n    batch = next(iter(dataloader))\n    unsorted, sorted_seq = batch\n    prediction = model(unsorted)\n    if is_pointer_network:\n        prediction = probs_to_x(prediction, unsorted)\n\n    results = [\n        {\n            'Example Nr.': i,\n            'Type': t,\n            'Values': ','.join(f'{x:.4f}' for x in seq[i].tolist())\n        }\n        for i in [0, 5, 10]\n        for t, seq in zip(['input', 'prediction', 'target'], [unsorted, prediction, sorted_seq])\n    ]\n    \n    return pd.DataFrame(results)\n\ndef visualize_divergence(df):\n    unique_examples = df['Example Nr.'].unique()\n    example_index = unique_examples[0]\n    df_example = df[df['Example Nr.']==example_index]\n    arr_input = np.array([float(value) for value in df_example[df_example['Type'] == 'input']['Values'].values[0].split(',')])\n    arr_prediction = np.array([float(value) for value in df_example[df_example['Type'] == 'prediction']['Values'].values[0].split(',')])\n    arr_target = np.array([float(value) for value in df_example[df_example['Type'] == 'target']['Values'].values[0].split(',')])\n\n    arr_divergence = arr_prediction - arr_target\n    indices = np.arange(len(arr_divergence))\n\n    bar_width = 0.95  \n    fig, ax = plt.subplots(figsize=(6, 4))\n    \n    bars = ax.bar(indices, arr_divergence, bar_width, color='royalblue', edgecolor='black', alpha=0.7)\n\n    ax.set_xlabel('Index in Array', fontsize=12)\n    ax.set_ylabel('Divergence', fontsize=12)\n    ax.set_title('Divergence per Index'.format(example_index), fontsize=13)\n    ax.legend(['Divergence'], loc='upper right')\n    \n    ax.yaxis.grid(True, linestyle='--', alpha=0.7)\n\n    for bar in bars:\n        yval = bar.get_height()\n        ax.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 2), ha='center', va='bottom', fontsize=10)\n    \n    ax.set_ylim(min(arr_divergence) - 1, max(arr_divergence) + 1)\n\n    plt.tight_layout()\n    plt.show()"
  },
  {
    "objectID": "posts/set-to-set/index.html#models",
    "href": "posts/set-to-set/index.html#models",
    "title": "Set-to-Set: An extension of the Seq-to-Seq paradigm using attention",
    "section": "3 Models",
    "text": "3 Models\nNow we will introduce potential solutions to Set-to-Set problem1 introduced earlier.1 In our case of sorting numbers, it would be more accurate to call it a Set-to-Seq problem, but we’ll use “Set-to-Set” in this articles to emphasize the non-sequential data.\n\n3.1 Feed-Forward Network\nAs a baseline model, we will implement a feed-forward network with input- and output dimensions \\(s\\), which is one of the simplest types of neural networks (Bebis and Georgiopoulos 1994). The structure of the network consists of a single linear layer followed by a LeakyReLU activation function. We are using LeakyReLU because the input values can be positive and negative.\n\n\n\n\n\n\nFurther Reading\n\n\n\n\nDeep Learning Basics — Part 7 — Feed Forward Neural Networks (FFNN)- medium\nFeedforward neural network - geeksforgeeks\n\n\n\n\nclass LinearFF(nn.Module):\n    def __init__(self, s):\n        super().__init__()\n        self.l = nn.Linear(s, s, bias=False)\n        self.a = nn.LeakyReLU()\n\n    def forward(self, x):\n        return self.a(self.l(x))\n\nTraining this model achieves the following results on our evaluation-set.\n\n\n\n\n\n\n\nTable 1: Training & evaluation metrics for the feed-forward model\n\n\n\nMetric\nValue\n\n\n\n\n0\nModel\nFeed-Forward\n\n\n1\nEmbedding Size\nNone\n\n\n2\nSequence Length\n5\n\n\n3\nTraining Epochs\n250\n\n\n4\nAccuracy\n0.00%\n\n\n5\nAvg. Divergence\n0.095851\n\n\n\n\n\n\n\n\nThe evaluation of our model shows an accuracy score of 0%, mainly because the model struggles to predict Float32 values with full accuracy. As the accuracy metric alone doesn’t fully capture the model’s performance, we also calculated the average divergence as a complementary measure. This statistic reflects the positive or negative deviation of the model output from its corresponding target value at each index in the array.\n\n\n\n\n\nFigure 1: Concrete predictions and training pairs for the feed-forward model\n\n\n\n\nAdditionally, we will inspect a few samples from the evaluation set, combined with the model output.\n\n\n\n\n\n\n\n\n\nExample Nr.\nType\nValues\n\n\n\n\n0\n0\ninput\n0.0850,0.4038,0.4878,0.6103,0.6019\n\n\n1\n0\nprediction\n0.1571,0.3136,0.4480,0.5758,0.6981\n\n\n2\n0\ntarget\n0.0850,0.4038,0.4878,0.6019,0.6103\n\n\n3\n5\ninput\n0.5476,0.8797,0.8482,0.5416,0.5308\n\n\n4\n5\nprediction\n0.2360,0.4701,0.6863,0.8853,1.0755\n\n\n5\n5\ntarget\n0.5308,0.5416,0.5476,0.8482,0.8797\n\n\n6\n10\ninput\n0.9732,0.6948,0.5976,0.8599,0.2902\n\n\n7\n10\nprediction\n0.2506,0.4770,0.7003,0.8977,1.0958\n\n\n8\n10\ntarget\n0.2902,0.5976,0.6948,0.8599,0.9732\n\n\n\n\n\n\n\nWe can observe that the model seems to make reasonable predictions, but they are imprecise and often include made-up values which are not present in the input-array.\nBased on the evaluation, we noticed two further disadvantages of the model architecture in addition to performance:\n\nFixed input and output size (problem 1): The model works with a fixed size \\(s\\) for both inputs and outputs. This limits flexibility and makes it difficult to deal with varying data sets.\n\nSimultaneous processing of all inputs (problem 2): All inputs are used simultaneously to generate all outputs. However, in the case of sorting arrays, the output of a value and the previous values are dependent on each other. This also has the effect that an input value can appear several times in the output of the FF-network instead of just once.\n\nThese limitations contribute to the suboptimal performance of the model and highlight the need to revise the architecture to improve efficiency and accuracy.\n\n\n3.2 Seq-to-Seq\nTo find a solution to these problems, we have defined a Seq-to-Seq model using a recurrent architecture with an LSTM (Hochreiter and Schmidhuber 1997). It generates outputs autoregressive, meaning that the previous outputs from the model are used to produce the next output. Additionally, this autoregressive nature also allows us to process variable-sized inputs, eliminating both of the problems that the baseline feed-forward architecture in the previous section has.\n\n\n\n\n\n\nFurther Reading\n\n\n\n\nWhat is LSTM? Introduction to Long Short-Term Memory - medium\nWhat is LSTM – Long Short Term Memory? - geeksforgeeks\n\n\n\nThe Seq-to-Seq architecture, consisting of encoder and decoder, is visualized in Figure 2\n\n\n\nFigure 2: Seq-to-Seq architecture\n\n\nBefore we can generate outputs, the model must analyze the entire input. This is why we use an encoder, which processes the entire input sequence \\(x\\) and then passes the hidden state \\(e\\) to the decoder as context. The decoder receives the context \\(e\\) from the encoder and uses it to generate the output \\(y\\) in an autoregressive manner.\nBased on this architecture, we now define the model architecture in PyTorch.\n\nclass EncoderLstm(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(EncoderLstm, self).__init__()\n\n        # LSTM\n        self.lstm_cell = nn.LSTMCell(input_size, hidden_size)\n        self.hidden_size = hidden_size\n\n    def forward(self, xs, h, c, s):\n        hs = []\n        for t in range(s):\n1            xt = xs[:, t, :]\n            h, c = self.lstm_cell(xt, (h, c))\n            hs.append(h)\n\n        return torch.stack(hs, dim=1), (h, c)\n\n\nclass DecoderLstm(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(DecoderLstm, self).__init__()\n\n        # Variables\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n\n        # LSTM\n        self.lstm_cell = nn.LSTMCell(input_size, hidden_size)\n\n        # Output\n        self.fc = nn.Sequential(\n            nn.Linear(hidden_size, output_size),\n            nn.Sigmoid()  # find out why this helps?\n        )\n\n    def forward(self, xs, h, c, s):\n        ys = []\n        yt = torch.zeros((xs.shape[0], self.output_size))  # SOS\n\n        for t in range(s):\n2            h, c = self.lstm_cell(yt, (h, c))\n\n3            yt = self.fc(h)\n            ys.append(yt)\n\n        return torch.stack(ys, dim=1).squeeze(-1)\n        \n\nclass SeqToSeq(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(SeqToSeq, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, xs):\n        xs = xs.unsqueeze(-1)\n        b, s = xs.size()[0], xs.size()[1]\n        h, c = torch.zeros(b, self.encoder.hidden_size), torch.zeros(b, self.encoder.hidden_size)\n\n        hs, (h, c) = self.encoder(xs, h, c, s)\n4        return self.decoder(hs, h, c, s)\n\n\n1\n\nRead in data: First, all input values are read in one after the other (sequentially)\n\n2\n\nAutoregressive output generation: The decoder generates it’s next hidden state by feeding previous outputs back into the LSTM\n\n3\n\nLSTM Output Layer: The LSTM output is projected to a final output \\(y\\) using a linear layer\n\n4\n\nInitialize Decoder: The outputs (hidden states) and final hidden state from the encoder are used to initialize the decoder\n\n\n\n\nAfterward, we will use this architecture to train a new model.\n\n\n\n\n\n\n\nTable 2: Training & evaluation metrics for the Seq-to-Seq model\n\n\n\nMetric\nValue\n\n\n\n\n0\nModel\nLSTM\n\n\n1\nEmbedding Size\nNone\n\n\n2\nSequence Length\n5\n\n\n3\nTraining Epochs\n250\n\n\n4\nAccuracy\n0.00%\n\n\n5\nAvg. Divergence\n0.025922\n\n\n\n\n\n\n\n\n\n# echo: false\n# inspect(model, val_loader, False)\n\nLooking at the evaluation results, we observe an accuracy score of 0%, indicating that the precision problem of the Feed-Forward network (see Section 3.1) has not yet been resolved. However, the average divergence is improving, suggesting the Seq-to-Seq architecture had a positive effect on the performance.\n\n\n3.3 Seq-to-Seq with Embeddings\nTo improve the accuracy of the model, a logical next step would is to improve the input encoding via embeddings, instead of using the values as direct input to the model. Typically, an embedding layer can be implemented by a simple linear layer without bias and without an activation function.\n\n\n\n\n\n\nFurther Reading\n\n\n\n\nWhat are Embeddings and how do it work? - medium\nWhat Is Embedding and What Can You Do with It - towardsdatascience\n\n\n\n\nclass Embedder(nn.Module):\n    def __init__(self, embedding_size):\n        super().__init__()\n        self.lin = nn.Linear(1, embedding_size, bias=False)\n\n    def forward(self, xs):\n        return self.lin(xs)\n\nTo use this Embedder module in the previously defined Seq-to-Seq module, we will integrate it there.\n\nclass SeqToSeq(nn.Module):\n    def __init__(self, encoder, decoder, embedder=None):\n        super(SeqToSeq, self).__init__()\n        self.embedder = embedder\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, xs):\n        xs = xs.unsqueeze(-1)\n        b, s = xs.size()[0], xs.size()[1]\n        h, c = torch.zeros(b, self.encoder.hidden_size), torch.zeros(b, self.encoder.hidden_size)\n\n1        es = self.embedder(xs) if self.embedder else xs\n2        hs, (h, c) = self.encoder(es, h, c, s)\n        return self.decoder(hs, h, c, s)\n\n\n1\n\nEmbedding generation: Each of the elements from the input sequence are converted to embeddings\n\n2\n\nInput to the encoder: The embeddings are passed to the encoder (instead of the original input sequence)\n\n\n\n\n\n\n\n\n\n\n\nTable 3: Training & evaluation metrics for the embedding Seq-to-Seq model\n\n\n\nMetric\nValue\n\n\n\n\n0\nModel\nLSTM + Embeddings\n\n\n1\nEmbedding Size\n32\n\n\n2\nSequence Length\n5\n\n\n3\nTraining Epochs\n250\n\n\n4\nAccuracy\n0.00%\n\n\n5\nAvg. Divergence\n0.015684\n\n\n\n\n\n\n\n\nBy using embeddings, we can observe a continuous decrease in the average divergence.\n\n\n3.4 Seq-to-Seq with Attention\nAlthough the average divergence is improving, it is still not optimal. A common problem of Seq-to-Seq models is, that the hidden state of the encoder-LSTM can become a bottleneck for the entire model (Vinyals, Bengio, and Kudlur 2015).\nThis problem can be addressed using an attention-mechanism (Graves, Wayne, and Danihelka 2014), where we not only pass the hidden state \\(h\\) and the cell state \\(c\\) to the decoder, but also include additional information from the original input sequence. This allows the decoder to analyze and weight the input sequences again before making predictions for the outputs.\n\n3.4.1 How does the attention mechanism work?\nForemost, we would like to emphasize that there are different types of attentional mechanisms (Graves, Wayne, and Danihelka 2014). In this article, we will only focus on content based, specifically additive attention. For those interested in exploring other types of attention, additional resources are provided below.\n\n\n\n\n\n\nFurther Reading\n\n\n\n\nAttention and Self-Attention for NLP - slds-lmu\nAttention in transformers, visually explained | Chapter 6, Deep Learning - 3Blue1Brown\n\n\n\nThe following diagram displays how content-based attention is integrated in the encoder-decoder architecture.\n\n\n\nFigure 3: Encoder-decoder architecture with content-based attention\n\n\nTo understand what’s going on, we will first introduce the basics of content-based attention. Here, we will re-use the following variables from Figure 2:\n\n\\(d\\): decoder hidden state\n\\(e\\): encoder hidden state\n\\(o\\): encoder outputs\n\nContent-based attention as defined by Vinyals, Fortunato, and Jaitly (2015) consists of three steps2: First, we calculate a similarity-score \\(u\\) between the encoder outputs \\(o\\) and the decoder hidden states \\(d\\). Each of these scores indicates the relevance of a specific encoder output to the current decoder state.2 We use the same notation as in the paper here, with slight modifications\n\\[\nu_j = v^T \\tanh(W_1 o_j + W_2 d)  \n\\tag{1}\\]\nIn a second step, the softmax-function is applied to the attention-score, resulting in the attentions-weights \\(a\\).\n\\[a_j = \\operatorname{softmax}(u_j) \\tag{2}\\]\nFinally, a context is calculated using the attention-weights \\(a\\) and the encoder outputs \\(o\\).\n\\[d' = \\sum_{j=1}^n a_j o_j \\tag{3}\\]\nBy summing these values, we obtain our new hidden state \\(d'\\) for the decoder, which has now been enhanced with additional knowledge about the input sequence.\nGoing back to Figure 3, Equation 1 is displayed in red, Equation 2 in orange and Equation 3 in blue. We can see that without attention (grey colors), the decoder would be limited to the information provided in the encoder hidden state \\(e\\). But by using attention, the decoder can include information about the entire input sequence by comparing its own hidden state \\(d\\) to the encoder outputs \\(o\\) at each step while generating outputs. This should mitigate the bottleneck problem discussed in earlier sections (see Section 3.2).\n\n\n3.4.2 Implementation\nWe will now proceed by implementing the attention-mechanism in PyTorch.\n\nclass AdditiveAttention(nn.Module):\n    def __init__(self, key_size, query_size):\n        super(AdditiveAttention, self).__init__()\n        self.w1 = nn.Linear(key_size, query_size, bias=False)\n        self.w2 = nn.Linear(query_size, query_size, bias=False)\n        self.v = nn.Linear(query_size, 1, bias=False)\n        self.compress = nn.Sequential(\n            nn.Linear(query_size * 2, query_size),\n            nn.Tanh()\n        )\n\n    def forward(self, key, query, create_context, compress_context=False):\n1        u = self.v(torch.tanh(self.w1(key) + self.w2(query).unsqueeze(1))).squeeze(-1)\n2        a = F.softmax(u, dim=1)\n        if create_context:\n3            r = torch.bmm(a.unsqueeze(1), key).squeeze(1)\n            concat = torch.cat([query, r], dim=1)\n            context = self.compress(concat) if compress_context else concat\n            return context\n        else:\n            return a\n\n\n1\n\nCalculate the similarity-score \\(u\\) (see Equation 1)\n\n2\n\nCalculate the attention-weights \\(a\\) via softmax on \\(u\\) (see Equation 2)\n\n3\n\n“Combine” all the information into a single context, which will be used as the next decoder hidden state \\(d'\\) (see Equation 3)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nPlease ignore the concat and compress steps for now. These are used in the Read-Process-Write architecture introduced in the following sections.\n\n\nFinally, we will integrate the attention module into the decoder.\n\nclass DecoderLstm(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, use_attention):\n        super(DecoderLstm, self).__init__()\n\n        # Variables\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n\n        # LSTM\n        self.lstm_cell = nn.LSTMCell(input_size, hidden_size)\n\n        # Attention\n1        self.attention = AdditiveAttention(hidden_size, hidden_size) if use_attention else None\n\n        # Output\n        self.fc = nn.Sequential(\n            nn.Linear(hidden_size, output_size),\n            nn.Sigmoid()  # find out why this helps?\n        )\n\n    def forward(self, xs, h, c, s):\n        ys = []\n        yt = torch.zeros((xs.shape[0], self.output_size))  # SOS\n\n        for t in range(s):\n            h, c = self.lstm_cell(yt, (h, c))\n\n            if self.attention:\n2                context = self.attention(xs, h, create_context=True, compress_context=True)\n                yt = self.fc(context)\n            else:\n                yt = self.fc(h)\n            ys.append(yt)\n\n        return torch.stack(ys, dim=1).squeeze(-1)\n\n\n1\n\nInitialize the attention module\n\n2\n\nApply attention\n\n\n\n\nUsing this architecture, we will now train and evaluate an attention-based Seq-to-Seq model on the training data.\n\n\n\n\n\n\n\nTable 4: Training & evaluation metrics for the content-attention based Seq-to-Seq model\n\n\n\nMetric\nValue\n\n\n\n\n0\nModel\nLSTM + Embeddings + Attention\n\n\n1\nEmbedding Size\n32\n\n\n2\nSequence Length\n5\n\n\n3\nTraining Epochs\n250\n\n\n4\nAccuracy\n0.00%\n\n\n5\nAvg. Divergence\n0.010806\n\n\n\n\n\n\n\n\nWe can observe that the addition of the attention mechanism significantly reduces the average divergence, which indicates that the bottleneck-problem of the hidden state is being alleviated.\n\n\n\n3.5 Seq-to-Seq with Pointers\nAs previously outlined, the issue of the hidden state bottleneck was addressed through the introduction of attention. Nevertheless, this does not address the issue of precision (see Section 3.1): The problem arises because the network attempts to predict values in the ordered array directly, however because these are Float32 values, it is highly unlikely that the network will be able to output it in full precision.\nTo address this issue, we can modify the model to predict indices of the ordered array, instead of values: This means that we will train a model that understands how to rank items without needing to predict exact output values, thereby resolving the precision problem. This idea is known as a pointer network (Vinyals, Fortunato, and Jaitly 2015) and we will implement it in this section.\n\n\n\n\n\n\nFurther Reading\n\n\n\n\nPointer networks : What are they? - medium\nThe Power of Pointer Networks - hyperscience\n\n\n\nImplementing a pointer network is actually a simplified model of the content-based attention Seq-to-Seq model from Section 3.4. The diagram below displays the Seq-to-Seq architecture using a pointer network as a decoder.\n\n\n\nFigure 4: Encoder-decoder architecture with a pointer net\n\n\nThe pointer network is essentially content-based attention, but without calculating a context (Equation 3) and instead using the attention-weights (see Equation 2) as outputs. These attention-weights are a distribution over the input-vocabulary and can therefore serve as “pointers” to the input vocabulary. To use this distribution as a hidden-state for subsequent steps, we simply do a matrix multiplication with this distribution to convert it into a scalar-value.\n\nclass DecoderPointerLstm(nn.Module):\n    def __init__(self, hidden_size, use_attention):\n        super(DecoderPointerLstm, self).__init__()\n\n        # LSTM\n        self.lstm_cell = nn.LSTMCell(hidden_size, hidden_size)\n\n        # Attention\n        self.attention = AdditiveAttention(hidden_size, hidden_size) if use_attention else None\n        self.hidden_size = hidden_size\n\n    def forward(self, xs, h, c, s):\n        ys = []\n        yt = torch.zeros((xs.shape[0], self.hidden_size))  # SOS\n\n        for t in range(s):\n            h, c = self.lstm_cell(yt, (h, c))\n\n            # now returns a softmax distribution\n1            p = self.attention(xs, h, create_context=False)\n            ys.append(p)\n\n            # compile next input\n            # this could also be just the pointer distribution, but would restrict\n            # the model to a specific sequence length (not generalizable) so we compile\n            # a new state from it\n2            yt = torch.bmm(p.unsqueeze(1), xs).squeeze(1)\n\n        return torch.stack(ys, dim=1)\n\n\n1\n\nNo Context Vector Creation: Directly return the attention-weights which serve as pointers to the input-vocabulary\n\n2\n\nCompress softmax distribution: Calculate a scalar value to serve as the next hidden state for the decoder\n\n\n\n\nUsing this new architecture, we will train and evaluate a pointer-net based Seq-to-Seq model.\n\n\n\n\n\n\n\nTable 5: Training & evaluation metrics for the pointer Seq-to-Seq model\n\n\n\nMetric\nValue\n\n\n\n\n0\nModel\nLSTM + Embeddings + Attention + Pointer\n\n\n1\nEmbedding Size\n32\n\n\n2\nSequence Length\n5\n\n\n3\nTraining Epochs\n250\n\n\n4\nAccuracy\n93.05%\n\n\n5\nAvg. Divergence\n0.00228\n\n\n\n\n\n\n\n\nThe results improve significantly with the addition of the pointer mechanism: Not only is the accuracy metric working, indicating that we have solved the precision problem, but also the average divergence is decreasing as a side effect of the model predicting more precise values.\nComparing the model output with the target values directly (see Table 6), it is difficult to find wrong outputs due to the high accuracy. However, we can observe that when the model makes a mistake, the resulting value is still guaranteed to be present in the input array, due to the pointer mechanism.\n\n\n\n\n\n\n\nTable 6: Concrete predictions and training pairs for the pointer Seq-to-Seq model\n\n\n\nExample Nr.\nType\nValues\n\n\n\n\n0\n0\ninput\n0.0850,0.4038,0.4878,0.6103,0.6019\n\n\n1\n0\nprediction\n0.0850,0.4038,0.4878,0.6019,0.6103\n\n\n2\n0\ntarget\n0.0850,0.4038,0.4878,0.6019,0.6103\n\n\n3\n5\ninput\n0.5476,0.8797,0.8482,0.5416,0.5308\n\n\n4\n5\nprediction\n0.5416,0.5476,0.5476,0.8482,0.8797\n\n\n5\n5\ntarget\n0.5308,0.5416,0.5476,0.8482,0.8797\n\n\n6\n10\ninput\n0.9732,0.6948,0.5976,0.8599,0.2902\n\n\n7\n10\nprediction\n0.2902,0.5976,0.6948,0.8599,0.9732\n\n\n8\n10\ntarget\n0.2902,0.5976,0.6948,0.8599,0.9732\n\n\n\n\n\n\n\n\n\n\n3.6 Read-Process-Write Architecture\nThe Seq-to-Seq architecture combined with a pointer network already achieves good results. However, there is an important observation to be made: The Seq-to-Seq architecture is reading the inputs sequentially and generating outputs sequentially (see Figure 2). As a result, the order in which the input array is presented has an effect on the performance of the model because it changes the encoding. This property can be advantageous when dealing with sequential data, but not for Set2Set. Sets, by definition lack order and thus every set should be encoded identically (see Vinyals, Bengio, and Kudlur (2015)).\nRead-Process-Write Architecture, introduced by Vinyals, Bengio, and Kudlur (2015) addresses this problem by using an order invariant encoding via content-based attention in the encoder and a glimpse mechanism in the decoder for order invariant decoding.\nCompared to the previous sections, we will now continue with the notation from Vinyals, Bengio, and Kudlur (2015) with slight modifications so that the reader can easily compare our implementation to the paper.\n\n\\(m\\): Memory vector (previously embeddings)\n\\(q\\): Hidden state of the LSTM (previously \\(h\\))\n\\(q^*\\): Concatenation of the hidden State of the LSTM + attention readout: \\([q, r]\\)\n\nFigure 5 displays the Read-Process-Write architecture, excluding the glimpse mechanism, which will be described in a following section.\n\n\n\nFigure 5: Read-Process-Write architecture (simplified without glimpse-mechanism)\n\n\nAlthough this diagram is more verbose than Figure 2, in essence the Read-Process-Write architecture can be understood as an Encoder-Decoder model with embeddings. In fact, in our implementation, we use the same embedder introduced in Section 3.3 for this model. The encoder is called the Process block and the main difference to the Seq-toSeq encoder from Figure 2 is, that it can “see” the input sequence \\(m\\) only via an attention mechanism, instead of as input into the LSTM, resulting in an order invariant encoding. As a side effect, we can now also specify how many process steps we want to take, potentially even more than the sequence length \\(s\\) of the input array 3.3 In our implementation we’re using 5 processing steps, but these can be adjusted as seen fit\n\nclass ProcessEncoderLstm(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(ProcessEncoderLstm, self).__init__()\n\n        # LSTM\n        self.lstm_cell = nn.LSTMCell(hidden_size * 2, hidden_size)\n\n        # Attention\n        self.attention = AdditiveAttention(key_size=input_size, query_size=hidden_size)\n\n    def forward(self, ms, q_star, h, c, process_steps):\n1        for t in range(process_steps):\n2            q, c = self.lstm_cell(q_star, (h, c))\n\n3            q_star = self.attention(ms, q, create_context=True)\n\n        return q_star\n\n\n1\n\nIndependent lengths: The encoding is not dependent on the length of the input sequence anymore. This gives more flexibility and the potential to define the number of processing steps manually.\n\n2\n\nInput Modification: Instead of feeding the inputs \\(m\\) sequentially into the encoder LSTM, we’re using \\(q^*\\) as the input\n\n3\n\n\\(q^*\\) generation: Concatenation of the previous hidden state \\(q\\) and the attention readout from comparing the memory vector \\(m\\) with \\(q\\)\n\n\n\n\nAs shown in Figure 5, \\(q^*\\) from the Process block is used to initialize the Write block. The Write block is mostly identical to the decoder pointer-network introduced in Section 3.5, with the addition of a glimpse-mechanism, which was also introduced by Vinyals, Bengio, and Kudlur (2015). Glimpse is another content-based attention module which is calculated from \\(q^*\\) and the memory vector \\(m\\). The glimpse-value \\(g\\) is then passed as input \\(x\\) and \\(q^*\\) as the hidden state \\(h\\) to the LSTM. Although the authors only briefly describe the design-decision behind this mechanism, it increases the performance of the model significantly. While the decoder in the Seq-to-Seq architecture was able to rely on its previous outputs as inputs \\(x\\) for the following steps, in a non-sequential setting as in Set-to-Set, this is not the case, and the glimpse value \\(g\\) serves as a non-sequential replacement for it.\n\nclass WriteDecoderPointerLstm(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(WriteDecoderPointerLstm, self).__init__()\n\n        # LSTM\n        self.lstm_cell = nn.LSTMCell(input_size, hidden_size)\n        self.input_size = input_size\n\n        # Attention\n        self.attention = AdditiveAttention(key_size=input_size, query_size=hidden_size)\n        self.hidden_size = hidden_size\n\n        # Glimpse\n1        self.glimpse = AdditiveAttention(key_size=input_size, query_size=input_size)\n2        self.glimpse_projection = nn.Linear(hidden_size, input_size)\n\n    def forward(self, m, h, c, s):\n        ys = []\n\n        for t in range(s):\n            # glimpse\n            # h = q_star (from process-block)\n3            g = self.glimpse(m, self.glimpse_projection(h), create_context=True, compress_context=True)\n\n            h, c = self.lstm_cell(g, (h, c))\n\n            # returns softmax\n            y = self.attention(m, h, create_context=False)\n            ys.append(y)\n\n        return torch.stack(ys, dim=1)\n\n\n1\n\nInitialize glimpse: Glimpse is just another attention module\n\n2\n\nInitialize a projection layer: This makes the shapes of the hidden state \\(q^*\\) and memory vector \\(m\\) compatible\n\n3\n\nApply glimpse: Before generating a new hidden state, calculate the glimpse value \\(g\\) to use as the input \\(x\\) for the LSTM\n\n\n\n\nTo combine the Read, Process and Write modules into a single model, we will introduce the RPW class using pytorch.\n\nclass ProcessBlock(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super().__init__()\n        self.module = ProcessEncoderLstm(input_size=input_size, hidden_size=hidden_size)\n        self.hidden_size = hidden_size\n\n    def forward(self, ms, s):\n        bs = ms.size(0)\n        q_star = torch.zeros(bs, self.hidden_size * 2)\n        h = torch.zeros(bs, self.hidden_size)\n        c = torch.zeros(bs, self.hidden_size)\n\n        return self.module(ms, q_star, h, c, s)\n\n\nclass WriteBlock(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super().__init__()\n        self.module = WriteDecoderPointerLstm(input_size=input_size, hidden_size=hidden_size * 2)\n\n    def forward(self, m, q_star):\n        bs, s = m.size(0), m.size(1)\n        c = torch.zeros(bs, self.module.hidden_size)\n\n        return self.module(m, q_star, c, s)\n\n\nclass RPW(nn.Module):\n    def __init__(self, embedding_size, hidden_size, process_steps):\n        super().__init__()\n        self.read = Embedder(embedding_size=embedding_size)\n        self.process = ProcessBlock(input_size=embedding_size, hidden_size=hidden_size)\n        self.write = WriteBlock(input_size=embedding_size, hidden_size=hidden_size)\n        self.process_steps = process_steps\n\n    def forward(self, xs):\n        xs = xs.unsqueeze(-1)\n        m = self.read(xs)\n1        q_star = self.process(m, self.process_steps)\n2        pointers = self.write(m, q_star)\n\n        return pointers\n\n\n1\n\nGenerates a hidden state \\(q^*\\) which is used to initialize to Write module. This encoding is order invariant.\n\n2\n\nGenerates the pointer distribution using the newly added glimpse mechanism\n\n\n\n\nFinally, we will train and evaluate a new model with this architecture.\n\n\n\n\n\n\n\nTable 7: Training & evaluation metrics for the Read-Process-Write architecture\n\n\n\nMetric\nValue\n\n\n\n\n0\nModel\nRead-Process-Write (Pointer)\n\n\n1\nEmbedding Size\n32\n\n\n2\nSequence Length\n5\n\n\n3\nTraining Epochs\n250\n\n\n4\nAccuracy\n98.70%\n\n\n5\nAvg. Divergence\n0.00036\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Concrete predictions and training pairs for the Read-Process-Write model\n\n\n\n\nThe results show almost perfect accuracy values and a very low divergence. This indicates that the order invariant encoding / decoding, which is implemented via content-based attention, improves the performance. With more processing steps and longer training time, this performance can be improved even further."
  },
  {
    "objectID": "posts/set-to-set/index.html#summary",
    "href": "posts/set-to-set/index.html#summary",
    "title": "Set-to-Set: An extension of the Seq-to-Seq paradigm using attention",
    "section": "4 Summary",
    "text": "4 Summary\nThe Seq-to-Seq architecture is useful for processing sequential data of varying lengths, but faces challenges when dealing with unordered inputs or outputs. In this article, we highlight the strengths of the Read-Process-Write architecture, which is an extension of the classical Seq-to-Seq paradigm to a Set-to-Set paradigm, by building the final model from the ground up and comparing the performance improvements at each step. LSTMs are better at handling variable-length sequences than a baseline feed-forward networks (see Section 3.1), and can be combined into a Seq-to-Seq model with an encoder and decoder (see Section 3.2), potentially also using embeddings for improved performance (see Section 3.3). Incorporating an attention-mechanism can help to overcome the hidden-state bottleneck, significantly improving performance (see Section 3.4). The introduction of pointer networks, on the other hand, allows the model to be more accurate in its output, by predicting indices from the input vocabulary rather than the values directly (see Section 3.5). Finally, by processing the inputs for the encoder and decoder LSTM in a non-sequential manner via attention, the Read-Process-Write architecture (see Section 3.6) eliminates the sequential limitation of Seq-to-Seq models and results in a Set-to-Set architecture, which may be used to solve the problem of sorting numbers introduced in this article."
  },
  {
    "objectID": "posts/dynamic-programming-1/index.html",
    "href": "posts/dynamic-programming-1/index.html",
    "title": "Value & Policy Iteration",
    "section": "",
    "text": "In this article, I implemented a simple example for value and policy iteration from scratch in Python.\nBecause implement the concepts from scratch, the only import we’ll use is NumPy.\nimport numpy as np"
  },
  {
    "objectID": "posts/dynamic-programming-1/index.html#mdp",
    "href": "posts/dynamic-programming-1/index.html#mdp",
    "title": "Value & Policy Iteration",
    "section": "1 MDP",
    "text": "1 MDP\nIn order to do value- and policy iteration, we’ll want to model the MDP. In this case, our “world” will be a 2D grid, where each cell has a reward value.\nTo make life easier, I’ll create a wrapper around the numpy array that contains the data, to introduce some convenient functions that simplify the code.\n\nclass Grid:\n    def __init__(self, matrix):\n        self.matrix = np.array(matrix)\n\n    def get_val(self, state):\n        r, c = self.coord_2_index(state)\n        return self.matrix[r, c]\n\n    def set_val(self, state, value):\n        r, c = self.coord_2_index(state)\n        self.matrix[r, c] = value\n\n    def coord_2_index(self, state):\n        x, y = state\n        return len(self.matrix) - y, x - 1\n\n    def equals(self, grid):\n        return np.array_equal(self.matrix, grid.matrix)\n\n    def display(self):\n        print(self.matrix)\n\nTo perform the updates using the Bellman equations, I’ll create another class modelling the MDP.\n\nclass Mdp:\n    def __init__(self, states, actions, gamma):\n        self.states = states\n        self.actions = actions\n        self.gamma = gamma\n\n    def T(s1, a, s2):\n        pass\n\n    def R(s1, a, s2):\n        pass\n\nOur grid-world example has a few specific transitions (NESW) and a simple reward function. So I’ll create a new class that can model a grid-world of any size provided by the user.\n\nclass MdpGrid(Mdp):\n    def __init__(self, grid, terminal_states, gamma):\n        self.grid = grid\n        self.terminal_states = terminal_states\n        states = [(x, y) for x in range(1, len(self.grid.matrix) + 1) for y in range(1, len(self.grid.matrix[0]) + 1)]\n        actions = [\"N\", \"E\", \"S\", \"W\"]\n\n        super().__init__(states, actions, gamma)\n\n    def T(self, s1, a, s2):\n        if s1 in self.terminal_states:\n            return 0\n\n        transitions = {\n            \"N\": (0, 1),\n            \"E\": (1, 0),\n            \"S\": (0, -1),\n            \"W\": (-1, 0),\n        }\n        x, y = s1\n        x2, y2 = s2\n        dx, dy = transitions[a]  # Deterministic actions assumed\n        return int((x + dx, y + dy) == s2)\n\n    def R(self, s1, a, s2):\n        return self.grid.get_val(s2)\n\nIn this notebook, we’ll experiment with a simple 4x4 grid. It has two rewards and ends as soon as the agent collects one of them.\n\n\n\nFigure 1: Visualization of the MdpGrid4x4 world\n\n\n\nclass MdpGrid4x4(MdpGrid):\n    def __init__(self, gamma):\n        matrix = [\n            [0, 0, 0, 1],\n            [0, 0, 0, -1],\n            [0, 0, 0, 0],\n            [0, 0, 0, 0],\n        ]\n        grid = Grid(matrix)\n        super().__init__(grid, [(4, 4), (4, 3)], gamma)"
  },
  {
    "objectID": "posts/dynamic-programming-1/index.html#value-iteration",
    "href": "posts/dynamic-programming-1/index.html#value-iteration",
    "title": "Value & Policy Iteration",
    "section": "2 Value Iteration",
    "text": "2 Value Iteration\n\\[\nV_{k+1}(s) \\leftarrow \\max_a \\sum_{s'} T(s, a, s') \\left[ R(s, a, s') + \\gamma V_k(s') \\right]\n\\]\n\nclass ValueIteration():\n    def __init__(self, mdp):\n        self.mdp = mdp\n\n    def V(self, s, k):\n        if k &lt;= 0: return 0\n\n        values = []\n\n        for s2 in self.mdp.states:\n            for a in self.mdp.actions:\n                # Calculate components\n                t = self.mdp.T(s, a, s2)\n                r = self.mdp.R(s, a, s2)\n                v2 = self.V(s2, k - 1) if t &gt; 0 else 0\n\n                # Insert components into formula\n                v = t * (r + self.mdp.gamma * v2)\n\n                # Save to results\n                values.append(v)\n\n        return max(values)\n\n    def run(self, k):\n        shape = self.mdp.grid.matrix.shape\n        values = Grid(np.zeros(shape))\n\n        for s in self.mdp.states:\n            values.set_val(s, np.around(self.V(s,k), 2))\n\n        return values"
  },
  {
    "objectID": "posts/dynamic-programming-1/index.html#policy-iteration",
    "href": "posts/dynamic-programming-1/index.html#policy-iteration",
    "title": "Value & Policy Iteration",
    "section": "3 Policy Iteration",
    "text": "3 Policy Iteration\n\\[\nV^{\\pi_{i}}_{k+1}(s) \\leftarrow \\sum_{s'} T(s, \\pi_{i}(s), s') \\left[ R(s, \\pi_{i}(s), s') + \\gamma V^{\\pi_{i}}_{k}(s') \\right]\n\\]\n\nclass PolicyIteration():\n    def __init__(self, mdp):\n        self.mdp = mdp\n\n    def policy_evaluation(self, s, pi, k):\n        if k &lt;= 0:\n            return 0\n        value_sum = 0\n        for s2 in self.mdp.states:\n            a = pi.get_val(s)\n            t = self.mdp.T(s, a, s2)\n            r = self.mdp.R(s, a, s2)\n            v2 = self.policy_evaluation(s2, pi, k - 1) if t &gt; 0 else 0\n            value_sum += t * (r + self.mdp.gamma * v2)\n        return value_sum\n\n    def policy_improvement(self, pi, eval_k):\n        new_pi = Grid(pi.matrix.copy())\n        for s in self.mdp.states:\n            action_values = []\n            for a in self.mdp.actions:\n                pi_copy = Grid(new_pi.matrix.copy())\n                pi_copy.set_val(s, a)\n                v = self.policy_evaluation(s, pi_copy, eval_k)\n                action_values.append((v, a))\n\n            best_action = max(action_values, key=lambda x: x[0])[1]\n            new_pi.set_val(s, best_action)\n\n        return new_pi\n\n    def run(self, pi, policy_iterations, value_iterations):\n        iter_pi = pi\n        for i in range(policy_iterations):\n            new_pi = self.policy_improvement(iter_pi, value_iterations)\n            if new_pi.equals(iter_pi):\n                break\n            iter_pi = new_pi\n        return iter_pi"
  },
  {
    "objectID": "posts/dynamic-programming-1/index.html#examples",
    "href": "posts/dynamic-programming-1/index.html#examples",
    "title": "Value & Policy Iteration",
    "section": "4 Examples",
    "text": "4 Examples\nLet’s initialize a new grid world and run policy and value iteration on both of them to see the results.\n\nmdp_4x4 = MdpGrid4x4(0.95)\n\n\nvalue_iter = ValueIteration(mdp_4x4)\n\nresulting_values = value_iter.run(5)\nresulting_values.display()\n\n[[0.9  0.95 1.   0.  ]\n [0.86 0.9  0.95 0.  ]\n [0.81 0.86 0.9  0.86]\n [0.   0.81 0.86 0.81]]\n\n\n\npolicy_iter = PolicyIteration(mdp_4x4)\n\nmy_pi_matrix = [[\"N\" for _ in range(4)] for _ in range(4)]\nmy_pi = Grid(my_pi_matrix)\n\nresulting_policy = policy_iter.run(my_pi, 15, 10)\nresulting_policy.display()\n\n[['E' 'E' 'E' 'N']\n ['N' 'N' 'N' 'N']\n ['N' 'N' 'N' 'W']\n ['N' 'N' 'N' 'N']]"
  },
  {
    "objectID": "posts/simulated-annealing/index.html",
    "href": "posts/simulated-annealing/index.html",
    "title": "Simulated Annealing",
    "section": "",
    "text": "Life is full of solving problems. We are constantly searching for solutions, and in Artificial Intelligence there’s a whole subfield about search algorithms. In this article, I want to introduce the algorithm which fascinates me the most."
  },
  {
    "objectID": "posts/simulated-annealing/index.html#the-problem",
    "href": "posts/simulated-annealing/index.html#the-problem",
    "title": "Simulated Annealing",
    "section": "1 The problem",
    "text": "1 The problem\nSearch Problems are difficult to solve. But why is that? Essentially, it is because the things we hope to find are a lot less common than the things we do not want to find. Take, for example, a Rubik’s cube: It has 43 quintillion configurations, out of only 1 is the correct one. If we were to approach this problem without a strategy, the chances are 1:43 quintillion of finding the solution every time we make a turn. Entropy can be defined as the number of states that a system can have1. So in the example of the Rubik’s cube, it is low when we only consider a “solved” Rubik’s cube, but high when we consider an “unsolved” Rubik’s cube. Together with the fact that over time, entropy tends to a maximum2, it’s the reason that making random turns on the Rubik’s cube most likely will leave it in a worse state.1 this definition is simplistic and applies more specifically to the context of statistical mechanics2 The second law of thermodynamics\n\n\n\nSimplified overview over the problem: There are many “bad” states for the Rubik’s cube and only one “solved” state (red). The chances of picking it are really low.\n\n\nIt almost seems like most of our real-world problems exist because we want order (low entropy) while the world tends to move toward chaos (high entropy). If we could reverse these laws, even if it’s just for a moment, we could solve so many problems without any effort. If you had a button to inverse Entropy in our example, you could solve the Rubik’s cube by throwing it down the stairs.\nSimulated annealing is a search algorithm that was inspired by the annealing process in physics, which in fact leverages Entropy to solve problems. But how does it do it? Increasing Entropy, by its very definition, is just a consequence of probabilities. So to understand the magic, we have to look at the world from a statistical perspective because if we can change the probabilities, we can make Entropy work for - and not against us."
  },
  {
    "objectID": "posts/simulated-annealing/index.html#setup",
    "href": "posts/simulated-annealing/index.html#setup",
    "title": "Simulated Annealing",
    "section": "2 Setup",
    "text": "2 Setup\nBefore we try to understand how simulated annealing works, I want to simplify the problem and the tools we use to make it more approachable. When working with informed search algorithms like simulated annealing, you are working with the algorithm itself, which is a function, and also a heuristic (or cost) function. The cost function tells you how far away from your goal you are - not more, not less. It doesn’t say how to get closer, just how bad the current state is. And because it says how close we are to the goal, minimizing it is our main goal.\nBecause the Rubik’s cube is such a complicated example, let’s choose a simpler problem for now. Let’s just assume that our cost function3 is defined as follows:3 This function has been randomly chosen and includes a local and global minimum\n\\[f(x) = x^{2} + 10 * sin(x)+ 15\\]\n\ndef f(x):\n    return x**2 + 10 * np.sin(x) + 15\n\nFrom now on, we want to minimize the output of this cost function and therefore find a solution for \\(x\\) that makes \\(f(x)\\) as small as possible, ideally \\(0\\). We can plot how our cost function looks like:\n\n\n\n\n\nFigure 1: \\(f(x) = x^{2} + 10 * sin(x)+ 15\\) (plotted from -10 to 10)"
  },
  {
    "objectID": "posts/simulated-annealing/index.html#building-a-strategy",
    "href": "posts/simulated-annealing/index.html#building-a-strategy",
    "title": "Simulated Annealing",
    "section": "3 Building a strategy",
    "text": "3 Building a strategy\n\n3.1 Random\nTo solve this problem and to understand how simulated annealing does it so well, we should look at it from a statistical perspective. Perhaps the simplest approach to finding a solution is to pick a value randomly. How high would the probability of finding the global minimum at around -1.5 be? We can visualize this strategy using a probability density function (PDF).\n\ndef algorithm_a(f, iterations):\n  x_values = []\n  y_values = []\n  for t in range(iterations):\n1    x_values.append(random.uniform(-20, 20))\n    y_values.append(f(x_values[t]))\n\n  return x_values, y_values\n\n\n1\n\nAt each timestep we just select a random \\(x\\) for our function\n\n\n\n\n\n\n\n\n\nFigure 2: Probability density function for the ‘random’ strategy\n\n\n\n\nAs you can see, all values of x are equally likely to be picked as the solution, which is of course wrong. Ideally, we want our algorithm to find the solution more often than the wrong answers.\n\n\n3.2 Preference\nTo find the ideal solution more reliably, we could define our algorithm in a way, that it’s more likely that a cost-improvement will be accepted rather than a regression. The simplest way would be to accept worse solutions only sometimes, say 50% of the time:\n\ndef algorithm_b(f, iterations, initial_x):\n  x_values = [initial_x]\n  y_values = [f(initial_x)]\n\n  for t in range(iterations):\n    current_x, current_y = x_values[-1], y_values[-1]\n    new_x = random.uniform(-20, 20)\n    new_y = f(new_x)\n1    if new_y &lt; current_y or random.random() &lt; 0.50:\n      x_values.append(new_x)\n      y_values.append(new_y)\n\n  return x_values, y_values\n\n\n1\n\nOnly accept “worse” solutions 50% of the time\n\n\n\n\n\n\n\n\n\nFigure 3: Probability density function for the ‘preference’ strategy\n\n\n\n\nIt looks like this strategy helped the algorithm to identify the global minimum more reliably, but it’s still yielding many wrong results.\n\n\n3.3 Neighbors\nThe current implementation is inefficient because the progress we made at identifying the best solution will be reset on every time step. Imagine trying to solve a Rubik’s cube, but instead of slowly building up your solution, you choose an entirely new configuration every time you make a turn, instead of improving the current one. So instead of choosing an entirely new solution every time we make a move, let’s only consider neighboring solutions, that is solutions within a certain interval of the current one. Only considering neighboring states is one important concept that simulated annealing employs.\n\ndef algorithm_c(f, iterations, initial_x, p):\n  x_values = [initial_x]\n  y_values = [f(initial_x)]\n\n  for t in range(iterations):\n    current_x, current_y = x_values[-1], y_values[-1]\n1    new_x = current_x + random.uniform(-1, 1)\n    new_y = f(new_x)\n    if new_y &lt; current_y or random.random() &lt; p:\n      x_values.append(new_x)\n      y_values.append(new_y)\n\n  return x_values, y_values\n\n\n1\n\nOnly consider neighboring values of the current states by reducing the interval that we sample from to \\([-1, 1]\\), which are the neighboring states only. If we were to sample from \\([-20, 20]\\) like before, we’d lose our progress at every step.\n\n\n\n\n\n\n\n\n\nFigure 4: Probability density function for the ‘neighbour’ approach\n\n\n\n\nAs you can see, the probability that the algorithm yields the correct solution is a lot higher than for any other value. But it’s still not guaranteed, and a lot of the time it returns wrong answers. The variance is still too high. Ideally, we’d always want the highest point of the probability density function to be returned as our solution and ignore all the other values. However, we can’t just set the probability of accepting a worse state to 0 because this would lead to the local optimum quite often. Once we’re in the local optimum and don’t accept “worse” states anymore, there’d be no way out. So we have to find another approach to lead us to the global optimum.\nHow can we solve this? Instead of immediately setting the acceptance probability for worse states to 0%, we could start at 100% and gradually decrease it. Every time we decrease the probability, we become a little more deterministic and in that way we reject bad states more often gradually: At first, we reject the ridiculous states, like configurations of a Rubik’s cube that are completely mixed. After that, we lower the threshold of good states even further, so of all the not-so-bad states, we accept only the better ones.\nWe can imagine this process like filtering out the best solution gradually. The state space for the Rubik’s cube could be represented in the following image. At a high acceptance-probability, we do the rough work, filtering out all the terrible configurations, resulting in a better subset of configurations. After that, we refine our search, filtering out the even better states from our previous subset. We refine this process even further until we arrive at a 0% acceptance probability for worse states, at which point we just pick the perfect fit.\n\n\n\nFiltering step by step. Each rectangle represents a step in the filtering process which is getting refined gradually. The filtering continues until only the solution state remains.\n\n\nThis process works like a sieve: at each step we increase the quality and build upon our previous work. If we were to keep the acceptance-probability low, we’d have countless hit-or-miss results because we focus in on one particular configuration, before filtering out all the bad states. If we kept it fixed at a high value, we’d never arrive at an excellent solution because we’d never “filter” out the bad states.\nFrom now on, we’ll call this probability of accepting worse states “Temperature” because that’s the term used in the simulated annealing algorithm4. In our code, it’s written as t:4 Later we’ll expand this definition, but for now, it’s just called the Temperature.\n\ndef algorithm_d(f, iterations, initial_x):\n    x_values = [initial_x]\n    y_values = [f(initial_x)]\n1    t_values = [1]\n\n    for k in range(iterations):\n        current_x, current_y = x_values[-1], y_values[-1]\n        new_x = current_x + random.uniform(-1, 1)\n        new_y = f(new_x)\n2        if new_y &lt; current_y or random.random() &lt; t_values[-1]:\n            x_values.append(new_x)\n            y_values.append(new_y)\n\n3        t_values.append(1 - k / iterations)\n\n    return x_values, y_values, t_values\n\n\n1\n\nWe start with a temperature of 1 (100% acceptance probability)\n\n2\n\nThis change means the same as in our previous algorithms. Because we save the temperatures in an array (for visualization), we want to consider the latest entry.\n\n3\n\nWe decrease the temperature linearly\n\n\n\n\nThe results of this algorithm look as follows:\n\n\n\n\n\nFigure 5: Probability density function for the ‘falling-temperature’ approach\n\n\n\n\nWe can compare this to the solutions for different temperatures from algorithm_c in Section 3.3 with different fixed temperatures:\n\n\n\n\n\nFigure 6: Probability density function for the results of the ‘neighbors’ approach with different fixed temperatures\n\n\n\n\nLooking at this new PDF, we can notice a couple of things:\n\nThe results in Figure 5 are more reliable than the PDF with a fixed low temperature (0.01)\nThe results in Figure 5 are more concrete than the PDF with fixed high temperature (0.9)\n\nOur results are more reliable because we consider a wider array of possibilities than when starting with a really low acceptance probability, which would immediately focus in on a small section of the graph. Using algorithm_c with t=0.01 will lead to the global minimum quite often. But it’s also more concrete than the results we get from using a fixed high temperature. This is because once we reach a low temperature, we focus in on the details, refining our solution.\nWe can also take a look at how the results from our new algorithm develop over time. The following animation shows the PDF of the values picked by the algorithm in an interval of 10%. As expected, they start out very broad and incorrect, but over time it becomes more narrow and correct.\n\n\n\nProbability density function over time for a fractions of timesteps ‘falling-temperature-fractions’\n\n\n\n\n3.4 Quality\nAll the strategies we adapted so far lead to some good results. However, dropping the acceptance probability linearly is a simplification and is rarely the case in reality. You could think about what it’s like writing an essay: The outline probably takes less time than all the revisions and details. This is also known as the Pareto Rule (or 80-20 principle) and it shows that to be optimal, we need a more accurate approach for lowering the acceptance probability.\nTo solve this, we could come up with different temperature schedules, like a geometric decay. However, this doesn’t address the underlying problem and may only work for some cases.\nThus far, our acceptance probability was equal to the temperature when considering “worse” states. That means, that, no matter how bad the new state is, we will always accept it with a fixed probability (given by the current temperature). However, it could be quite efficient to take the quality of this worse state into consideration. For example, when solving a Rubik’s Cube, we prefer “bad” states over “terrible” states. So we should decrease the acceptance probability for states as they approach “terrible”.\nThe question becomes, how could we express this as a formula? Obviously, we want to integrate \\(\\Delta Cost\\) (the difference between y_old and y_new) into our acceptance probability, as this tells us how “bad” the proposed state is. However, \\(\\Delta Cost\\) can have any value in the range \\([0, \\inf[\\) so to normalize it as a probability, there’s a simple trick. We’ll just plug it into the function \\(e^{-x}\\).\n\n\n\n\n\nFigure 7: \\(e^{-x}\\) plotted from 0 to 5”\n\n\n\n\nThis normalizes the value of \\(\\Delta Cost\\) to \\([0, 1]\\) and \\(e^{- \\Delta Cost}\\) can therefore be used as a probability.\nThe only thing left to do it so integrate the temperature \\(T\\) into the formula. Just like \\(\\Delta Cost\\), we know that \\(T\\) can be in the range of \\([0, \\inf[\\), however in contrast to \\(\\Delta Cost\\), a high \\(T\\) should result in a high acceptance probability. To add \\(T\\) into the equation, we can divide \\(\\Delta Cost\\) in the exponent by \\(T\\). This can be expressed as the following formula:\n\\[\ne^{-\\frac{\\Delta Cost}{T}}\n\\]\nDividing by \\(T\\) effectively “weakens” the effect of \\(\\Delta C\\) when \\(T\\) is high, resulting in a high acceptance probability.\n\n3.4.1 Boltzmann distribution\nThe above explanation serves as an intuitive approach to simulated annealing. If you’re satisfied with this explanation, you can skip this section and jump right to the implementation in Section 3.4.3, if not, here’s my attempt to illustrate it with the theoretical background as well.\n\n\n\n\n\n\nNote\n\n\n\nTo be humble, I am not convinced that my understanding of the Boltzmann Distribution in simulated annealing is sufficient at this point. However, I decided to include this part in the article to encourage feedback and discussions about this concept so that I can ultimately learn and understand it better. My goal is therefore not to provide a perfect answer right now, but to improve this explanation in the future through your participation!\n\n\nLet’s think about what the acceptance probability is supposed to do: It should tell us how likely the state that we’re observing is getting us closer to the solution. And the solution in our case is the most likely outcome, as we can see in Figure 5. The thing we really want to know to be optimal is, how likely is it that our system is in the state that we’re observing? So if we know how likely a state is, then we know how good, that is how close to the solution it is.\nThe Boltzmann distribution gives us exactly that: it is a probability distribution that gives the probability that a system will be in a certain state as a function of that state’s energy and the temperature of the system5. It is defined as:5 Source: Wikipedia\n\\[\np_i \\propto e^{- \\frac{\\varepsilon_i}{kT}}\n\\]\n\n\\(\\varepsilon_i\\): Energy at a specific state \\(i\\)\n\nin our example “energy” = “cost” = y-value, or as previously defined: \\(\\Delta E = \\Delta Cost\\)\n\n\\(T\\): Temperature\n\\(k\\): Boltzmann constant, can be ignored and assumed to be \\(1\\)\n\n\n\n3.4.2 Boltzman factor\nThis is the entire distribution and ultimately, what we’re interested in. To calculate it, we use the Boltzmann factor, which is defined as:\n\\[\ne^{-\\frac{\\Delta E}{T}}\n\\]\n\n\\(\\Delta E\\): Difference in energy between two states\n\n\\(\\Delta E =\\) new_y - current_y in our code\n\n\nSo all we have to do now is plugging in the temperature and cost into the equation for the Boltzmann factor and use it as the acceptance probability when evaluating the states. This gives us the simulated annealing algorithm.\n\n\n3.4.3 Implementation\n\ndef simulated_annealing(f, iterations, initial_x):\n    x_values = [initial_x]\n    y_values = [f(initial_x)]\n    t_values = [1]\n\n    for k in range(iterations):\n        current_x, current_y = x_values[-1], y_values[-1]\n        new_x = current_x + random.uniform(-1, 1)\n        new_y = f(new_x)\n        \n        delta_e = new_y - current_y\n1        p = exp(-(delta_e)/t_values[-1])\n        \n        p_values.append(p)\n2        if delta_e &lt; 0 or random.random() &lt; p:\n            x_values.append(new_x)\n            y_values.append(new_y)\n\n        t_values.append(1 - k / iterations)\n\n    return x_values, y_values, t_values\n\n\n1\n\nThe Boltzmann factor, which is our acceptance probability for states that are worse than the current one\n\n2\n\nWe still always accept better states without asking, in the other cases we do so based on the Boltzmann factor. Together, this is called the “Metropolis acceptance criterion”\n\n\n\n\nRunning simulated annealing with our example problem yields the following result:\n\n\n\n\n\nFigure 8: Probability density function for the ‘simulated annealing’ algorithm\n\n\n\n\nThe probability of finding the solution with this approach is very high. It’s almost guaranteed now. By tuning the temperature schedule and running more iterations, these results would become even more apparent. We have therefore found a great solution to the problem."
  },
  {
    "objectID": "posts/simulated-annealing/index.html#summary",
    "href": "posts/simulated-annealing/index.html#summary",
    "title": "Simulated Annealing",
    "section": "4 Summary",
    "text": "4 Summary\nLeveraging the concepts discussed in this article, simulated annealing can be used not only to find solutions to simple problems as with our function \\(f(x)\\), but for any search problem, no matter how complex. As long as there’s a cost function and a temperature schedule, simulated annealing is guaranteed to find the optimal solution, given enough time and a temperature schedule that decreases slowly enough.\nThe concepts in this article have even more depth to them. For example, one could ask: why is the Boltzmann distribution defined the way it is? Or how could one find the optimal temperature for the algorithm? All these are questions that deserve their own time to discuss. This article should just give a well reasoned intuition of why it works at all.\nThank you, Vishal, Alex, and the rest of the FastAI MeetUp on Discord for providing feedback for this article."
  },
  {
    "objectID": "posts/rnn-1/index.html",
    "href": "posts/rnn-1/index.html",
    "title": "Introduction to RNNs",
    "section": "",
    "text": "Imagine how cool it would be if you could see the future. Or, at least, how stock prices develop in the next week. You would be rich very, very fast. But instead of doing all the work for yourself, how about developing a model that does the predictions for us? If we could somehow figure out how to build such a model, we’d never have to worry about money again. So let’s do it!\n\n\n\n\n\n\nNote\n\n\n\nThis notebook is based on Recurrent Neural Networks (RNNs), Clearly Explained!!! by StatQuest:\n\n\n\n\n\nLet’s take a look at some imaginary stock courses to get an idea of the data we’re working with.\n\n\n\n\n\nFigure 1: Examples for stock price changes (values are chosen purely arbitrary)\n\n\n\n\nIn this simplified example, we can see the stock of two companies change throughout the years. So how could we use a model to predict these changes?\n\n\n\nWhen we take a closer look at this kind of data, we can notice a couple of challenges that we have to solve.\n\n\nThe amount of data points for each stock can vary. In this example, there are 4 values for Googles, but only 3 values for OpenAIs stock. If you’ve worked with neural networks before, you know that this problem is not straightforward to solve. Usually, models expect a fixed number of inputs and produce a fixed number of outputs. However, our use case requires the model to work with different amounts of input data!\n\n\n\nThe values of our input data don’t necessarily form a straight line. In the example of OpenAI, the value decreases, then increases again. Because of this complexity, we won’t get very far with simple statistics like taking the mean or doing linear regression.\nSo how could we solve these problems?\n\n\n\n\n\n\nNote\n\n\n\nTake a moment and just brainstorm a couple of solutions. They don’t have to be perfect, but just ask yourself: How could you solve these two problems?"
  },
  {
    "objectID": "posts/rnn-1/index.html#stock-data",
    "href": "posts/rnn-1/index.html#stock-data",
    "title": "Introduction to RNNs",
    "section": "",
    "text": "Let’s take a look at some imaginary stock courses to get an idea of the data we’re working with.\n\n\n\n\n\nFigure 1: Examples for stock price changes (values are chosen purely arbitrary)\n\n\n\n\nIn this simplified example, we can see the stock of two companies change throughout the years. So how could we use a model to predict these changes?"
  },
  {
    "objectID": "posts/rnn-1/index.html#challenges",
    "href": "posts/rnn-1/index.html#challenges",
    "title": "Introduction to RNNs",
    "section": "",
    "text": "When we take a closer look at this kind of data, we can notice a couple of challenges that we have to solve.\n\n\nThe amount of data points for each stock can vary. In this example, there are 4 values for Googles, but only 3 values for OpenAIs stock. If you’ve worked with neural networks before, you know that this problem is not straightforward to solve. Usually, models expect a fixed number of inputs and produce a fixed number of outputs. However, our use case requires the model to work with different amounts of input data!\n\n\n\nThe values of our input data don’t necessarily form a straight line. In the example of OpenAI, the value decreases, then increases again. Because of this complexity, we won’t get very far with simple statistics like taking the mean or doing linear regression.\nSo how could we solve these problems?\n\n\n\n\n\n\nNote\n\n\n\nTake a moment and just brainstorm a couple of solutions. They don’t have to be perfect, but just ask yourself: How could you solve these two problems?"
  },
  {
    "objectID": "posts/rnn-1/index.html#setup",
    "href": "posts/rnn-1/index.html#setup",
    "title": "Introduction to RNNs",
    "section": "2.1 Setup",
    "text": "2.1 Setup\nFirst, we’ll introduce a couple of example stock data with more or less simple forms to work with:\n\nThree simple stocks, representing rising, falling and constant values\nA more complicated stock, which falls and rises\n\n\nstock_rising = [0, 0.5] # expected continuation: 1\nstock_constant = [0.5, 0.5] # expected continuation: 0.5\nstock_falling = [1, 0.5] # expected continuation: 0\nstock_curve = [1, 0.5, 0.5] # expected continuation: 1\n\n\n\n\n\n\n\nNote\n\n\n\nThe values in our dataset are normalized to 0 - 1.\n\n\nLet’s visualize the values, so we get a feeling of what’s going on.\n\n\nVisualization function\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_data(X, Y, title, dotted=True):\n    num_plots = len(X)\n    max_len = max([len(x) for x in X]) + 1\n    fig, axis = plt.subplots(1, num_plots, figsize=(max_len * num_plots, max_len))\n    if not isinstance(axis, np.ndarray):\n        axis = np.array([axis])\n\n    # Set aspect ratio and y-axis limits for all subplots\n    for ax in axis:\n        ax.set_aspect('auto', adjustable='box')\n        ax.set_ylim(0, 1)\n        ax.set_xlim(0, max_len)\n\n    # Input values\n    for i, x in enumerate(X):\n        space = np.linspace(0, max_len, len(x) + 1)\n        axis[i].plot(space, x + [np.nan], label=\"x\")\n\n    # Values to be predicted by the RNN\n    for i, y in enumerate(Y):\n        # rescale x-axis values\n        continuation_point = (len(X[i])-1) / len(X[i]) * max_len\n        axis[i].plot([continuation_point, max_len], [X[i][-1], y], linestyle='dotted' if dotted else 'solid', label=\"y\")\n\n    plt.suptitle(title)\n    plt.show()\n\n\n\n\n\n\n\nFigure 2: Input stock data arrays and the expected continuation"
  },
  {
    "objectID": "posts/rnn-1/index.html#rnn-1",
    "href": "posts/rnn-1/index.html#rnn-1",
    "title": "Introduction to RNNs",
    "section": "2.2 RNN",
    "text": "2.2 RNN\n\n2.2.1 Architecture\nGiven a sequence of input values \\(x\\), predicting an outcome \\(y\\) is not a problem. If you’ve worked with neural networks before, you’ve done it a thousand times. You define a sequence of layers and activation functions (to keep it simple we’ll use linear layers and relu only), as well as the number of inputs and outputs together with the correct weights, the model will solve the problem. The problem of the stock courses having a complex form1 can be easily solved with a neural network with enough layers.1 see Section 1.2.2\n\n\n\nFigure 3: Schema of a feed forward neural network with four inputs and one output\n\n\nThe problem that remains even in a classic feed forward neural network is, that it only works with a fixed number of inputs2. To make the model more flexible, RNNs use a “feedback loop”, which solves exactly that problem.2 see Section 1.2.1\nThink about how you make a prediction of one of these stock courses. If you’re like me, you will read the line from the beginning to the end and build up an overall feeling of the development. In the example of the Google stock, you could think: It starts out at $400, but it’s decreasing… and again, it’s decreasing - and so on. RNNs do a similar thing, while a normal feed-forward neural network looks at all the data points at the same time and comes to a conclusion all at once.\nImagine a more complex scenario: A stock with 10,000 values over multiple years. A neural network with a fixed input size would look at all the values at once, as if it had 10,000 eyes, and calculate the prediction. Intuitively, an RNN is much more like a human. It looks at each data point sequentially (“feedback loop”) and builds up an overall opinion (“hidden state”, \\(h\\)) of the stock, until it’s reached the last data point, at which it will output its prediction.\n\n\n\nFigure 4: Unrolled schema of an RNN that handles four inputs\n\n\nAs you can see, the RNN does the same thing over and over again, until it’s looked at all data points. This is why it can be summarized to be more concise, in a recursive version, which gives us the final architecture of an RNN:\n\n\n\nFigure 5: Concise schema of an RNN that handles four inputs. The input values are being passed sequentially, one by one)\n\n\n\n\n2.2.2 Implementation\nNow that we know how the architecture of an RNN looks like, let’s implement it in Python. Essentially, we want to implement the feedback loop, which consists of a linear layer and an activation function, and the output layer, which is another linear layer\n\ndef lin(x,w,b=0):\n    return x*w+b\n\ndef relu(x):\n    return max(x,0)\n\nclass MiniRnn():\n  w1 = 1.4\n  w2 = -0.5\n  w3 = 1.4\n    \n  def forward(self, X, i=0, h=0):\n1    l1 = lin(X[i], self.w1)\n      \n2    h = relu(l1 + h*self.w2)\n3    if(i+1 != len(X)):\n        return self.forward(X, i+1, h)\n    \n    # Output\n4    l2 = lin(h, self.w3)\n    return round(l2, 1)\n\nrnn = MiniRnn()\n\n\n1\n\nInput layer\n\n2\n\nHidden state & activation function (relu)\n\n3\n\nFeedback loop\n\n4\n\nOutput layer\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe weights w1, w2, w3 have been calculated using gradient descent. In the next part of this series, we will learn how to do that ourselves - for now we’ll just use them as they are.\n\n\nLet’s think about how the model will calculate its prediction.\n\nThe RNN iterates over all values in the input sequence (X) and starts with no memory (“hidden state”) at all because h=0 in the first iteration.\nAfter looking at a new value, it saves all its “thoughts” in the hidden state h, until it iterated over all elements.\nOnce it has looked at all values (and only then because this is the only return statement that terminates the recursion) the model forms its conclusion and returns the prediction\n\nIf you’re interested in a more formal definition of the RNN, check out part 2 of this series, where I will introduce some of the math related to RNNs.\n\n\n\n\n\n\nImportant\n\n\n\nIf you’ve watched the StatQuest video on RNNs, you may notice that relu activation function is being applied before its being multiplies with w2. If we followed the video, our implementation would be: h = max(l1 + h, 0) * w2. However, the implementation from the video is wrong, it is not how RNNs are defined.\nFor now, this is not too important, but we’ll see in part 2 why this makes a huge difference.\n\n\n\n\n2.2.3 Testing\nLet’s validate the model by looking at the outputs:\n\nr1 = rnn.forward(stock_rising)\nr2 = rnn.forward(stock_constant)\nr3 = rnn.forward(stock_falling)\nr4 = rnn.forward(stock_curve)\nr1,r2,r3,r4\n\n(1.0, 0.5, 0.0, 1.0)\n\n\n\n\n\n\n\nFigure 6: Input stock data arrays with the model prediction\n\n\n\n\nThe RNN can predict the next values for all the stocks, nice!\nTo understand each step in detail, here are the steps our RNN took in forward-pass for stock_falling.\n\nstock_falling\n\n[1, 0.5]\n\n\n\nl1 = relu(1 * 1.4 + 0 * -0.5)\nl2 = relu(0.5 * 1.4 + l1 * -0.5)\nl3 = 1.4 * l2\nprint(f\"Layers: l1: {l1} l2: {l2} l3: {l3}\\nResult: {l3}\")\n\nLayers: l1: 1.4 l2: 0.0 l3: 0.0\nResult: 0.0\n\n\nAs you can see, each layer uses the output from the previous layer and the same three weights are being shared across all calculations. Even if we passed in a sequence of 100 values, we’d only be using w1,w2 and w3 in the model.\n\n\n2.2.4 Validating\nUntil this point, I only used the examples from the StatQuest video. However, these are not the only values we should try out. What about other forms? It could also be that the model just learned to memorize the four input arrays and maps a single value onto each of them.\nTo validate if our RNN can actually generalize, let’s change the input data. We’ll still use values which are normalized to 0 - 1, but we’ll change their amplitude.\n\nsmall_rising = [0,0.1]\nsmall_constant = [0.1,0.1]\nsmall_falling = [1,0.9]\nsmall_curve = [0.2,00.1,0.1]\n\n\n\n\n\n\nFigure 7: Input validation data arrays with the expected continuation\n\n\n\n\nOur validation data still represents the same shapes, but with different values. This should make it harder for a model that just “fakes” the output by remembering the training data. Let’s see what our RNN does with it.\n\ns1 = rnn.forward(small_rising)\ns2 = rnn.forward(small_constant)\ns3 = rnn.forward(small_falling)\ns4 = rnn.forward(small_curve)\ns1,s2,s3,s4\n\n(0.2, 0.1, 0.8, 0.2)\n\n\n\n\n\n\n\nFigure 8: Input validation data arrays with the model prediction\n\n\n\n\nThe model can correctly predict all of our validation data! This means that it can generalize and find some basic patterns in the data and is not just remembering our training dataset. This is quite an achievement. With these few lines of code, we’re able to create a model that can predict any kind of rising / constant / falling sequence of values, without changing any parameter. Imagine how far this approach can go when we scale up the model and increase its complexity!"
  },
  {
    "objectID": "posts/uninformed-search/index.html",
    "href": "posts/uninformed-search/index.html",
    "title": "Uninformed Search Algorithms",
    "section": "",
    "text": "Search is a very broad term in the field of artificial intelligence. What seems so intuitive to us as humans, such as finding an efficient route from city A to city B when looking at a map, is not so straightforward when we want a computer to do the same thing.\nAt very first glance, a simple thing such as search may seem irrelevant when we want to create artificial intelligence. It may seem boring, over-simplistic or useless to solve this problem, but it turns out that most of what’s considered artificial intelligence today is per definition just a search problem and in fact, it is solved by one of these simple search algorithms - gradient descent - which is just as simple as any of its relatives, be it from the informed or uninformed, global or local category.\n\n\n\n\n\n\nNote\n\n\n\nThis article is based on the chapter about uninformed search in “Artificial Intelligence: A Modern Approach, 4th Edition” by Stuart Russell and Peter Norvig.\n\n\n\n\nThe first important distinction to make for understanding search is to differentiate between informed and uninformed search. You can think of the former, like searching for your phone in your living room when you have no idea where you left it. The latter is like searching for it while giving it a call, so you hear the general direction where it might be.\nThere are different kinds of uninformed search algorithms, but the ones we’ll be focusing on in this article are Depth-First, Breadth-First and Uniform-Cost search. Each section will briefly introduce the concept and follow up with a concise Python implementation that you can copy and play around with.\n\n\n\nWe’ll start by defining an example scenario for our search. A common search problem is finding a path to a goal state, for example, you may wonder how to find the quickest way from your home to work.\n\n\nGraph initialization\nnodes = ['A', 'B', 'C', 'D', 'E', 'F']\nedges = [\n    ('A', 'B', 2),\n    ('A', 'C', 10),\n    ('B', 'C', 3),\n    ('B', 'D', 4),\n    ('C', 'E', 2),\n    ('D', 'F', 3),\n    ('E', 'F', 2),\n    ('E', 'B', 2)\n]\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# Create an empty graph\ngraph = nx.Graph()\n\n# Add nodes to the graph\ngraph.add_nodes_from(nodes)\n\n# Add edges with associated costs\nfor edge in edges:\n    graph.add_edge(edge[0], edge[1], cost=edge[2])\n\n\n\n\nVisualization functions\nimport colorsys\n\n# Generates a color palette from fully-saturated to unsaturated with the\n# specified amount of steps.\ndef generate_color_gradient(num_steps):\n    hue = 0.4  # Neon Green\n    lightness = 0.5\n    saturation_step = 1.0 / num_steps\n\n    colors = []\n    for i in range(num_steps):\n        # Calculate the current saturation\n        saturation = (i+1) * saturation_step\n\n        # Convert HSL to RGB\n        rgb_color = colorsys.hls_to_rgb(hue, lightness, saturation)\n\n        # Convert RGB values to 8-bit integers\n        rgb_color = tuple(int(value * 255) for value in rgb_color)\n\n        colors.append(rgb_color)\n\n    colors.reverse() # Saturated -&gt; Unsaturated\n    node_colors = [(r/255, g/255, b/255) for r, g, b in colors] # Conversion for networkx\n\n    return node_colors\n\n# Visualizes a graph and tints all visited nodes with a gradient (earliest to latest)\ndef visualize(graph, visited_nodes=[], start_node=\"A\", end_node=\"F\"):\n    pos = nx.spring_layout(graph, seed=42)  # or nx.circular_layout(graph)\n\n    labels = {edge[:2]: edge[2] for edge in edges}  # Dictionary for edge labels\n    color_array = generate_color_gradient(len(visited_nodes)) if visited_nodes else []\n\n    node_colors = []\n    for node in graph.nodes():\n      if node in visited_nodes:\n        # Tint visited nodes from earliest to latest visit\n        node_colors.append(color_array[visited_nodes.index(node)])\n      elif node == start_node or node == end_node:\n        # If there are no visited nodes, mark start and goal with main colors\n        node_colors.append(generate_color_gradient(1)[0])\n      else:\n        # Default color for nodes\n        node_colors.append('silver')\n\n    nx.draw(graph, pos, with_labels=True, node_size=500, node_color=node_colors, font_size=10, font_color='black')\n    nx.draw_networkx_edge_labels(graph, pos, edge_labels=labels)\n\n    plt.axis('off')\n    plt.show()\n\ndef evaluate(algorithm, graph):\n  visited = algorithm(graph)\n  visualize(graph, visited)\n\n\n\n\n\n\n\nFigure 1: Graph of locations A-F, with green locations (A,F) being the start- and goal-states. The edges represent the cost of transitioning from one state to another."
  },
  {
    "objectID": "posts/uninformed-search/index.html#informed-vs-uninformed",
    "href": "posts/uninformed-search/index.html#informed-vs-uninformed",
    "title": "Uninformed Search Algorithms",
    "section": "",
    "text": "The first important distinction to make for understanding search is to differentiate between informed and uninformed search. You can think of the former, like searching for your phone in your living room when you have no idea where you left it. The latter is like searching for it while giving it a call, so you hear the general direction where it might be.\nThere are different kinds of uninformed search algorithms, but the ones we’ll be focusing on in this article are Depth-First, Breadth-First and Uniform-Cost search. Each section will briefly introduce the concept and follow up with a concise Python implementation that you can copy and play around with."
  },
  {
    "objectID": "posts/uninformed-search/index.html#setup",
    "href": "posts/uninformed-search/index.html#setup",
    "title": "Uninformed Search Algorithms",
    "section": "",
    "text": "We’ll start by defining an example scenario for our search. A common search problem is finding a path to a goal state, for example, you may wonder how to find the quickest way from your home to work.\n\n\nGraph initialization\nnodes = ['A', 'B', 'C', 'D', 'E', 'F']\nedges = [\n    ('A', 'B', 2),\n    ('A', 'C', 10),\n    ('B', 'C', 3),\n    ('B', 'D', 4),\n    ('C', 'E', 2),\n    ('D', 'F', 3),\n    ('E', 'F', 2),\n    ('E', 'B', 2)\n]\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# Create an empty graph\ngraph = nx.Graph()\n\n# Add nodes to the graph\ngraph.add_nodes_from(nodes)\n\n# Add edges with associated costs\nfor edge in edges:\n    graph.add_edge(edge[0], edge[1], cost=edge[2])\n\n\n\n\nVisualization functions\nimport colorsys\n\n# Generates a color palette from fully-saturated to unsaturated with the\n# specified amount of steps.\ndef generate_color_gradient(num_steps):\n    hue = 0.4  # Neon Green\n    lightness = 0.5\n    saturation_step = 1.0 / num_steps\n\n    colors = []\n    for i in range(num_steps):\n        # Calculate the current saturation\n        saturation = (i+1) * saturation_step\n\n        # Convert HSL to RGB\n        rgb_color = colorsys.hls_to_rgb(hue, lightness, saturation)\n\n        # Convert RGB values to 8-bit integers\n        rgb_color = tuple(int(value * 255) for value in rgb_color)\n\n        colors.append(rgb_color)\n\n    colors.reverse() # Saturated -&gt; Unsaturated\n    node_colors = [(r/255, g/255, b/255) for r, g, b in colors] # Conversion for networkx\n\n    return node_colors\n\n# Visualizes a graph and tints all visited nodes with a gradient (earliest to latest)\ndef visualize(graph, visited_nodes=[], start_node=\"A\", end_node=\"F\"):\n    pos = nx.spring_layout(graph, seed=42)  # or nx.circular_layout(graph)\n\n    labels = {edge[:2]: edge[2] for edge in edges}  # Dictionary for edge labels\n    color_array = generate_color_gradient(len(visited_nodes)) if visited_nodes else []\n\n    node_colors = []\n    for node in graph.nodes():\n      if node in visited_nodes:\n        # Tint visited nodes from earliest to latest visit\n        node_colors.append(color_array[visited_nodes.index(node)])\n      elif node == start_node or node == end_node:\n        # If there are no visited nodes, mark start and goal with main colors\n        node_colors.append(generate_color_gradient(1)[0])\n      else:\n        # Default color for nodes\n        node_colors.append('silver')\n\n    nx.draw(graph, pos, with_labels=True, node_size=500, node_color=node_colors, font_size=10, font_color='black')\n    nx.draw_networkx_edge_labels(graph, pos, edge_labels=labels)\n\n    plt.axis('off')\n    plt.show()\n\ndef evaluate(algorithm, graph):\n  visited = algorithm(graph)\n  visualize(graph, visited)\n\n\n\n\n\n\n\nFigure 1: Graph of locations A-F, with green locations (A,F) being the start- and goal-states. The edges represent the cost of transitioning from one state to another."
  },
  {
    "objectID": "posts/rnn-2/index.html",
    "href": "posts/rnn-2/index.html",
    "title": "Backpropagation in RNNs",
    "section": "",
    "text": "Visualization functions\ndef plot_zic_zac(X, Y, title, dotted=True):\n  figsize = len(X[0]) + 1\n  fig, axis = plt.subplots(1, len(X), figsize=(figsize * len(X), figsize))\n  if not isinstance(axis, np.ndarray):\n    axis = np.array([axis])\n\n  # Set aspect ratio and y-axis limits for all subplots\n  for ax in axis:\n      ax.set_aspect('auto', adjustable='box')\n      ax.set_ylim(0, 1)\n      ax.set_xlim(0, figsize)\n\n  space = np.linspace(0, figsize, figsize)\n\n  # Input values\n  for i, x in enumerate(X):\n    axis[i].plot(space, x + [np.nan], label=\"x\")\n\n  # Values to be predicted by the RNN\n  continuation_point = figsize / (figsize - 1) * (figsize - 2)\n  for i, y in enumerate(Y):\n    axis[i].plot([continuation_point, figsize], [X[i][-1],y], linestyle='dotted' if dotted else 'solid', label=\"y\")\n\n  plt.suptitle(title)\n  plt.legend()\n  plt.show()\n\ndef plot_simple(X, Y, title, dotted=True):\n    num_plots = len(X)\n    max_len = max([len(x) for x in X]) + 1\n    fig, axis = plt.subplots(1, num_plots, figsize=(max_len * num_plots, max_len))\n    if not isinstance(axis, np.ndarray):\n        axis = np.array([axis])\n\n    # Set aspect ratio and y-axis limits for all subplots\n    for ax in axis:\n        ax.set_aspect('auto', adjustable='box')\n        ax.set_ylim(0, 1)\n        ax.set_xlim(0, max_len)\n\n    # Input values\n    for i, x in enumerate(X):\n        space = np.linspace(0, max_len, len(x) + 1)\n        axis[i].plot(space, x + [np.nan], label=\"x\")\n\n    # Values to be predicted by the RNN\n    for i, y in enumerate(Y):\n        # rescale x-axis values\n        continuation_point = (len(X[i])-1) / len(X[i]) * max_len\n        axis[i].plot([continuation_point, max_len], [X[i][-1], y], linestyle='dotted' if dotted else 'solid', label=\"y\")\n\n    plt.suptitle(title)\n    plt.show()\nIn Part 1 of this series, we implemented an RNN that can predict simple patterns. However, we used parameters from a pretrained model and didn’t learn how to train it on our own. Also, our RNN failed to generalize onto more complex patterns like the following zigzag shape.\ndef lin(x, w, b=0):\n  return x*w+b\n\nclass MiniRnn():\n  w1 = 1.8 # weight linear layer\n  w2 = -0.5 # weight feedback loop\n  w3 = 1.1 # weight output layer\n  b = 0.0\n\n  def forward(self, X, i=0, f=0):\n    # Linear layer + non-linearity\n    l1 = lin(X[i], self.w1, self.b + f)\n    relu = abs(l1)\n\n    # Feedback loop\n    if(i+1 != len(X)):\n      f = relu * self.w2\n      return self.forward(X, i+1, f)\n\n    # Final output\n    l2 = lin(relu, self.w3, self.b)\n    return round(l2, 1)\nrnn = MiniRnn()\nTo make our model perform well on the zigzag dataset, we want to train it using backpropagation. But how does backpropagation work for RNNs? Will it solve our problem with the zigzag shaped data? The goal of this article is to give a concise answer to this question and to provide you with a small model that you can play around by yourself.\nIn this article we will discuss how to compute the gradients mathematically and afterward, we’ll implement these concepts in our model code - without the help of PyTorch."
  },
  {
    "objectID": "posts/rnn-2/index.html#forward",
    "href": "posts/rnn-2/index.html#forward",
    "title": "Backpropagation in RNNs",
    "section": "1 Forward",
    "text": "1 Forward\nFirst, we have to think about what the model is doing in a mathematical way, so we’ll translate the forward pass into equations. Let’s remember how the model was being displayed in the previous part of this series.\n\n\n\nFigure 1: Unrolled schema of an RNN that handles four inputs\n\n\nThe most important thing to note is, that we actually have multiple outputs (\\(y_0\\) to \\(y_2\\) and output value (y) in this image). Of course, we are only interested in the last one because this is our prediction, but to define a formula for the RNN we can’t forget about the previous ones. The simplest way to do this is to introduce a suffix for the which output we want to reference, called a “timestep”.\nIf you’re familiar with programming, you can imagine it like this: The outputs of our RNN are all collected in an array, say y. For example: y = [1, 2, 3, 4]. To get a specific output, you’d index into the array like so: y[0]. In this case, we’d take the model output \\(y\\) at timestep \\(0\\), which we can write as \\(y_0\\) or \\(y_t\\) more generally, \\(t\\) being shorthand for “timestep”.\nNow, let’s write down some formulas to define our model.\n\n\n\n\n\n\nTip\n\n\n\nIf you want to get as much value out of this article as possible, I recommend that you try to do it by yourself as a practise before continuing reading the solutions.\n\n\n\n1.1 Loss\nThe loss function, in our case MSE, can be used for a number of variables, but in our case we only have two, the prediction \\(\\hat{y}_t\\) and the label \\(y_t\\), so we define it as the following\n\\[\nL = MSE(y_t, \\hat{y}_t) =  \\frac{1}{2} \\cdot (y_t - \\hat{y}_t)^2\n\\tag{1}\\]\n\n\n1.2 Output state\nWe know that \\(y_t\\) is the label in our training data, but now we have to specify how \\(\\hat{y}_t\\) is being calculated from the input data \\(x\\) and the weights \\(w_{1-3}\\). Figure 3 displays a schematic overview of the weights and states inside our RNN.\n\n\n\nFigure 2: Schema of an RNN with labels for the weights\n\n\nLet’s translate this process back-to front into formulas. The last step of calculating \\(\\hat{y}\\) is pretty simple, it’s just multiplying the hidden state with \\(w_3\\).\n\\[\n\\hat{y}_t = h_t \\cdot w_3\n\\tag{2}\\]\n\n\n1.3 Hidden state\nOur definition of \\(\\hat{y}_t\\) includes \\(h_t\\), which we’ll try to define now. This is a little bit more complicated, but in essence we’re just doing the following steps:\n\nMultiply the input \\(x_t\\) with \\(w_1\\)\nAdd the result to the previous hidden state \\(h_{t-1}\\)\nApply the activation function, in our case \\(\\text{ReLU}\\)\n\n\\[\nh_t = ReLU(w_1 \\cdot x_t + h_{t-1} \\cdot w_2)\n\\tag{3}\\]"
  },
  {
    "objectID": "posts/rnn-2/index.html#sec-backward",
    "href": "posts/rnn-2/index.html#sec-backward",
    "title": "Backpropagation in RNNs",
    "section": "2 Backward",
    "text": "2 Backward\nGiven Equation 1, Equation 2 and Equation 3, we now want to find out how to calculate the derivatives for $w_{1-3} $step by step so we can implement the process in our model. I’ll try to provide footnotes to the relevant derivative rules where appropriate for readers who are new to the subject.\nThe gradients we’re interested in are those for the variables \\(w_{1-3}\\). Therefore, what we’re looking for is the following:\n\\[\n\\frac{\\partial L}{\\partial w_3};\n\\frac{\\partial L}{\\partial w_2};\n\\frac{\\partial L}{\\partial w_1}\n\\]\n\n2.1 Loss\nTo calculate these derivatives, we first need to derive the loss, in our case the MSE in respect to \\(\\hat{y}_t\\), because \\(\\hat{y}_t\\) is the model prediction and “leads to” all the weights. The derivative1 is pretty straight forward, as the exponent and the fraction cancel out nicely.1 Power rule\n\\[\n\\frac{{\\partial \\text{{L}}}}{{\\partial \\hat{y}_t}}(y_t, \\hat{y}_t) = 2 \\cdot \\frac{1}{2} \\left( y_t - \\hat{y}_t \\right) = y_t - \\hat{y}_t \\tag{4}\n\\]\nImplementing this in Python gives us the following code.\n\ndef mse(x, y):\n  return ((x - y)/2)**2\n\ndef d_mse(x, y):\n  return x - y\n\n\n\n2.2 Output state\nThis alone doesn’t suffice, as we have to continue deriving until we find \\(w_{1-3}\\). The third weight can be derived2 quickly, following from Equation 2.2 Product rule\n\\[\n\\frac{\\partial \\hat{y}_t}{\\partial w_3} = h_t\n\\]\n\n\n2.3 Hidden state\nThe last step is deriving \\(w_{1-2}\\), which are included in Equation 3. Applying the chain-rule, we get the following results (\\(ReLU'\\) means the derivative of the ReLU function).\n\\[\n\\frac{\\partial h_t}{\\partial w_1} = \\text{ReLU}'(h_t) \\cdot x_t\n\\]\n\\[\n\\frac{\\partial h_t}{\\partial w_2} = \\text{ReLU}'(h_t) \\cdot h_{t-1}\n\\]\n\n\n2.4 ReLU\n\\[\n\\text{{ReLU}}(x) = \\begin{cases} x, & \\text{if } x &gt; 0 \\\\ 0, & \\text{otherwise} \\end{cases}\n\\]\nConsidering both cases, we trivially calculate the derivatives3 for each of them.3 Line rule, Constant rule\n\\[\n\\text{ReLU}'(x) = \\begin{cases} 1, & \\text{if } x &gt; 0 \\\\ 0, & \\text{otherwise} \\end{cases}\n\\]\n\ndef relu(x):\n  return max(x,0)\n\ndef d_relu(x):\n  return x &gt; 0\n\n\n\n2.5 Timesteps\nAt this point, we know how to calculate the gradients when we’re at a specific timestep \\(h_t\\). However, we’d like to calculate the gradients in respect to the loss, so based on the chain rule we want to find out: \\[\n\\frac{\\partial L}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial w_3}\n\\] \\[\n\\frac{\\partial L}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial h_t} \\frac{\\partial h_t}{\\partial w_2};\n\\] \\[\n\\frac{\\partial L}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial h_t} \\frac{\\partial h_t}{\\partial w_1};\n\\]\nWe have already calculated all the derivatives, except \\(\\frac{\\partial \\hat{y}}{\\partial h_t}\\). What does this term mean exactly? Let’s have a look at the unrolled network once again.\n\n\n\nFigure 3: Schema of an RNN with labels for the weights\n\n\nConsidering Equation 2, the deriving4 in the case of the last hidden state \\(h_n\\) is straightforward.4 Product rule\n\\[\n\\frac{\\partial \\hat{y}}{\\partial h_{n}} = w_2\n\\]\nHowever, for all previous hidden states \\(h_t\\) the gradient depends on the following state \\(h_{t+1}\\). This is because the gradient “flows” from the loss to the weights. For example, to calculate the gradient for \\(h_2\\) (Figure 3), you need to first calculate the gradient for \\(h_3\\) because this is the only “path” that leads to the loss, which is the source of the gradient. So when calculating the gradients, we have to do it step by step, always depending on the later layers \\(h_{t+1}\\). This is also referred to “backpropagation through time”.\n\\[\nh_{t+1} = ReLU(h_t \\cdot w_2 + x_3 \\cdot w_1)\n\\]\n\\[\n\\frac{\\partial h_{t+1}}{h_{t}} = ReLU'(h_t) \\cdot w_2\n\\]\nAnd putting it all together, we recursively define the derivative in respect to \\(h_{t+1}\\). This calculation will be done until we arrive at \\(h_n\\), where we can stop.\n\\[\n\\frac{\\partial \\hat{y}}{\\partial h_t} = \\frac{\\partial \\hat{y}}{\\partial h_{t+1}} \\frac{\\partial h_{t+1}}{\\partial h_{t}}  = \\frac{\\partial \\hat{y}}{\\partial h_{t+1}} \\cdot ReLU'(h_t) \\cdot w_2\n\\]\n\n\n2.6 All together\nSo now we know how to “find a path” from \\(L\\) to any of \\(w_{1-3}\\). However, to calculate the derivative in respect to \\(w_{1-2}\\), we need to consider all the “paths” at once:\n\n\n\nFigure 4: Schema of the RNN depicting the need for multivariable function derivative\n\n\nThis is because the weights \\(w_{1,2}\\) are used during all timesteps for the calculation for \\(\\hat{y}\\). When calculating the derivatives in respect to the weights, we can’t just pick one path and calculate it for this path only, we have to consider all of them at the same time because all of the “paths” influence \\(L\\). Using the derivative rule for variables in a multivariable function, we finally arrive at the following equations.\n\\[\n\\frac{\\partial L}{\\partial w_2} = \\frac{\\partial L}{\\partial \\hat{y}} \\sum_t{\\frac{\\partial \\hat{y}}{\\partial h_t} \\frac{\\partial h_t}{\\partial w_2}} ;\n\\]\n\\[\n\\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial \\hat{y}} \\sum_t{\\frac{\\partial \\hat{y}}{\\partial h_t} \\frac{\\partial h_t}{\\partial w_1}} ;\n\\]"
  },
  {
    "objectID": "posts/rnn-2/index.html#implementation",
    "href": "posts/rnn-2/index.html#implementation",
    "title": "Backpropagation in RNNs",
    "section": "3 Implementation",
    "text": "3 Implementation\nTo implement all of this in Python, we’ll first refactor our MiniRNN, introducing the data structures for \\(h_t\\) and \\(x_t\\) which are necessary for calculating the partial derivatives.\n\n\nRefactoring MiniRnn\nclass MiniRnn():\n  # Initialize weights\n  w1 = torch.rand(1) # linear layer (W_xh)\n  w2 = torch.rand(1) # feedback loop (W_hh)\n  w3 = torch.rand(1) # output layer (W_hy)\n1  w1.g, w2.g, w3.g = 0.0, 0.0, 0.0\n\n  # Initialize value structures (used for derivatives)\n2  X = None\n3  h = {0: torch.tensor([0.0])}\n\n  def forward(self, X):\n    self.X = X\n\n    # Run through the RNN\n    for i, x in enumerate(X):\n      self.h[i + 1] = relu(self.w1 * x + self.w2 * self.h[i])\n      y = self.w3 * self.h[i + 1]\n\n    return y\n\n  # Utility functions for resetting the class\n  def clear_states(self):\n    self.h = {0:torch.tensor([0.0])}\n    self.X = None\n\n  def zero_grads(self):\n    self.w1.g, self.w2.g, self.w3.g = 0.0, 0.0, 0.0\n\n\n\n1\n\nInitialize the gradients to 0\n\n2\n\nData structure for saving the inputs \\(x_{t}\\)\n\n3\n\nData structure for saving the hidden states \\(h_{t}\\)\n\n\n\n\nFinally, we’ll implement the equations for the partial derivatives from Section 2 in our model to calculate the gradients in the backwards function55 Partly based on https://victorzhou.com/blog/intro-to-rnns/#7-the-backward-phase\n\nclass MiniRnn():\n  # Initialize weights\n  w1 = torch.rand(1) # linear layer (W_xh)\n  w2 = torch.rand(1) # feedback loop (W_hh)\n  w3 = torch.rand(1) # output layer (W_hy)\n  w1.g, w2.g, w3.g = 0.0, 0.0, 0.0\n\n  # Initialize value structures (used for derivatives)\n  X = None\n  h = {0: torch.tensor([0.0])}\n\n  def forward(self, X):\n    self.X = X\n\n    # Run through the RNN\n    for i, x in enumerate(X):\n      self.h[i + 1] = relu(self.w1 * x + self.w2 * self.h[i])\n      y = self.w3 * self.h[i + 1]\n\n    return y\n\n  def backward(self, loss_g):\n    n = len(self.X)\n1    self.w3g = self.h[n] * loss_g\n2    d_h = self.w3 * loss_g\n\n    for i in reversed(range(n)):\n3        tmp = d_h * d_relu(self.h[i+1])\n\n4        self.w2.g += tmp * self.h[i]\n5        self.w1.g += tmp * self.X[i]\n\n        d_h = tmp * self.w2\n\n  def clear_states(self):\n    self.h = {0:torch.tensor([0.0])}\n    self.X = None\n\n  def zero_grads(self):\n    self.w1.g, self.w2.g, self.w3.g = 0.0, 0.0, 0.0\n\n\n1\n\nCalculate \\(\\frac{\\partial L}{\\partial w_3}\\)\n\n2\n\nCalculate \\(\\frac{\\partial \\hat{y}}{\\partial h_{n}}\\)\n\n3\n\nIntroduce temporary variable for less redundant code\n\n4\n\nCalculate \\(\\frac{\\partial h_t}{\\partial w_2}\\)\n\n5\n\nCalculate \\(\\frac{\\partial h_t}{\\partial w_1}\\)\n\n\n\n\n\n3.1 Training\nTo try out whether our implementation is correct, we can train this model on the previous examples of rising and falling data series.\n\nrnn = MiniRnn()\n\nX = [[0, 0.5], [0.5,0.5],[1,0.5]]\ny = [1,0.5,0]\n\ndef train(model, epochs=20000, lr=0.001):\n  for e in range(epochs):\n    for i, x in enumerate(X):\n      d_loss = d_mse(model.forward(x), y[i])\n\n      model.backward(d_loss)\n      model.w1 -= model.w1.g * lr\n      model.w2 -= model.w2.g * lr\n      model.w3 -= model.w3.g * lr\n\n      model.zero_grads()\n\ntrain(rnn)\n\n\n\n3.2 Validation\nLet’s run our model on the examples from the previous article and see how it performs.\n\n\n\n\n\nFigure 5: Input validation data arrays with the expected continuation\n\n\n\n\n\n\n\n\n\nFigure 6: Input validation data arrays with the model prediction\n\n\n\n\nWe can also take a look at what the weights look like:\n\nrnn.w1, rnn.w2, rnn.w3\n\n(tensor([2.3161]), tensor([-0.4927]), tensor([0.8070]))"
  },
  {
    "objectID": "posts/rnn-2/index.html#zigzag-problem",
    "href": "posts/rnn-2/index.html#zigzag-problem",
    "title": "Backpropagation in RNNs",
    "section": "4 Zigzag problem",
    "text": "4 Zigzag problem\nNow let’s train our model on the zigzag shapes and see what it predicts on the data.\n\n\nTraining on zigzag data\nrnn = MiniRnn()\n\nX = [data_zic_zac, data_zic_zac2]\ny = [1,0]\n\ndef train(model, epochs=20000, lr=0.001):\n  for e in range(epochs):\n    for i, x in enumerate(X):\n      d_loss = d_mse(model.forward(x), y[i])\n\n      model.backward(d_loss)\n      model.w1 -= model.w1.g * lr\n      model.w2 -= model.w2.g * lr\n      model.w3 -= model.w3.g * lr\n\n      model.zero_grads()\n\ntrain(rnn)\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Zigzag validation data arrays with the model prediction\n\n\n\n\nThe predictions look better than the one with the previous weights, but they’re far from perfect, so we can’t really say that the model learned the shape.\nOf course, we could experiment with more shapes and all the hyperparameters, but at this point it’s probably best to switch to the successor of the RNN, the LSTM (Long-Term Short-Term Memory) architecture. These kinds of model are in a few ways similar to the RNN, but without some of its drawbacks. They should be able to capture more complicated shapes, and I’d like to introduce them in another article to you."
  },
  {
    "objectID": "posts/uniform-circle-sampling/index.html",
    "href": "posts/uniform-circle-sampling/index.html",
    "title": "Uniform Sampling Inside a Circle",
    "section": "",
    "text": "YouTube recommendations can sometimes lead to surprisingly insightful videos. One such example is the video The BEST Way to Find a Random Point in a Circle by nubDotDev, which was originally submitted to 3blue1brown’s Summer of Math Exposition. This video addresses an interesting problem: How would you write an algorithm to sample points uniformly within a circle?\n\n\n\nUniformly sampled points inside a circle\n\n\nBefore reading this post, I recommend watching the video, as it covers rejection sampling and polar coordinate methods to solve this problem. In the following sections, I will present another solution that neither relies on rejection sampling nor polar coordinates, but instead utilizes the equation of a circle directly. Why would you want to know about all this? I’m not sure, but maybe this simple problem stimulates your curiosity like it did for me, so I recommend doing it for this purpose alone.\n\n\n\n\n\n\nNote\n\n\n\nAs already mentioned, there exist many solutions to this problem, like the rejection-sampling method. This post is about the result of my own ideas of thinking about the problem, but of course, these are not new and can be found in the existing approaches."
  },
  {
    "objectID": "posts/uniform-circle-sampling/index.html#naive-approach",
    "href": "posts/uniform-circle-sampling/index.html#naive-approach",
    "title": "Uniform Sampling Inside a Circle",
    "section": "2.1 Naive Approach",
    "text": "2.1 Naive Approach\nThe equation for a circle of radius \\(r\\) centered at the origin is given by:\n\\[\ny^2 + x^2 = r^2\n\\tag{1}\\]\nUsing Equation 1, random points on a circle can be sampled by selecting random values for \\(x \\in [0,r]\\) and calculating the corresponding \\(y\\) value (or vice versa). This only gives us points lying on the circle, whereas our objective is to sample uniformly from the entire area within the circle. This can be achieved by additionally sampling the radius as \\(r_{\\text{sampled}} \\in [0,r]\\). To summarize, the following procedure can be followed as our first attempt to sample points uniformly within a circle of radius \\(r = 1\\):\n\nSample \\(r_{\\text{sampled}} \\in [0,1]\\)\nSample \\(x \\in [0, r_{\\text{sampled}}]\\)\nCalculate \\(y\\) (Equation 1)\nPlot the point \\((x,y)\\)\n\nTo keep the code simple, we’ll just create a quarter circle, that is use \\(x,y \\in [0,1]\\).\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Number of points to sample\nnum_points = 1000\n\n# Arrays to store the sampled points\nx_points = []\ny_points = []\n\nfor _ in range(num_points):\n    r = np.random.uniform(0, 1)\n    x = np.random.uniform(0, r)\n    y = np.sqrt(r**2 - x**2)\n    x_points.append(x)\n    y_points.append(y)\n\n# Plotting the points\nplt.figure(figsize=(6, 6))\nplt.scatter(x_points, y_points, color='turquoise', s=2)\nplt.xlim(0, 1)\nplt.ylim(0, 1)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Initial Sampling - Points Clustered Around Center')\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "posts/uniform-circle-sampling/index.html#circumference-sampling",
    "href": "posts/uniform-circle-sampling/index.html#circumference-sampling",
    "title": "Uniform Sampling Inside a Circle",
    "section": "2.2 Circumference sampling",
    "text": "2.2 Circumference sampling\nThis method is simple, but it has a couple of problems, resulting in a non-uniform density of points. The most obvious problem is, that the points are denser around the circles’ center. The problem arises because the circumference of the circle grows with \\(r_{\\text{sampled}}\\) but the number of samples \\(k\\) doesn’t, as shown in this image.\n\n\n\nAs the circumference grows, the number of samples remains the same, resulting in a non-uniform density of samples.\n\n\nTo fix this, we can simply generate more points as the circle grows. Specifically, we’ll use the circumference \\(c\\) as a scaling factor.\n\\[\nc = 2 \\pi r_{\\text{sampled}}\n\\tag{2}\\]\n\ndef points_based_on_circumference(r, scale=1):\n    return int(np.ceil(2 * np.pi * r * scale))\n\nx_points = []\ny_points = []\nnum_radii = 1000\n\nfor _ in range(num_radii):\n    r = np.random.uniform(0, 1)\n    num_points = points_based_on_circumference(r)\n    for _ in range(num_points):\n        x = np.random.uniform(0, r)\n        y = np.sqrt(r**2 - x**2)\n        x_points.append(x)\n        y_points.append(y)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(x_points, y_points, color='turquoise', s=2)\nplt.xlim(0, 1)\nplt.ylim(0, 1)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Improved Distribution with Circumference-Based Sampling')\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "posts/uniform-circle-sampling/index.html#inverse",
    "href": "posts/uniform-circle-sampling/index.html#inverse",
    "title": "Uniform Sampling Inside a Circle",
    "section": "2.3 Inverse",
    "text": "2.3 Inverse\nNow we notice the second problem: It seems like there are fewer points at the bottom of the diagram. To explain what’s going on, it’s easier to look at a single circle generated with this approach.\n\n\nCode\n# Number of points to sample\nnum_points = 100\n\n# Arrays to store the sampled points\nx_points = []\ny_points = []\n\nfor _ in range(num_points):\n    # Sample r from [0, 1]\n    r = 1\n\n    # Sample x from [0, r]\n    x = np.random.uniform(0, r)\n\n    # Calculate y based on the circle formula X^2 + Y^2 = r^2\n    y = np.sqrt(r**2 - x**2)\n\n\n    # Append the point to the arrays\n    x_points.append(x)\n    y_points.append(y)\n\n# Create the plot\nplt.figure(figsize=(6, 6))\nplt.scatter(x_points, y_points, color='turquoise', s=5)\n\n# Set the limits and labels\nplt.xlim(0, 1)\nplt.ylim(0, 1)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Points on a Circle')\nplt.grid(True)\n\n# Show the plot\nplt.show()\n\n\n\n\n\nBecause the circle is drawn by calculating \\(y\\) via \\(x\\), which is uniformly sampled, there are simply more values on the “roof” of the circle than on the side, because from the perspective of \\(x\\), the area is wider. We can fix t his problem by also considering the perspective of \\(y\\), that is inverting \\(x\\) and \\(y\\) by chance.\n\nimport random\n\n# Number of points to sample\nnum_points = 100\n\n# Arrays to store the sampled points\nx_points = []\ny_points = []\n\nfor _ in range(num_points):\n    # Sample r from [0, 1]\n    r = 1\n\n    if random.random() &lt; 0.5:\n      # Sample x from [0, r]\n      x = np.random.uniform(0, r)\n\n      # Calculate y based on the circle formula X^2 + Y^2 = r^2\n      y = np.sqrt(r**2 - x**2)\n    else:\n      y = np.random.uniform(0, r)\n      x = np.sqrt(r**2 - y**2)\n\n\n    # Append the point to the arrays\n    x_points.append(x)\n    y_points.append(y)\n\n# Create the plot\nplt.figure(figsize=(6, 6))\nplt.scatter(x_points, y_points, color='turquoise', s=5)\n\n# Set the limits and labels\nplt.xlim(0, 1)\nplt.ylim(0, 1)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Points on a Circle')\nplt.grid(True)\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "posts/uniform-circle-sampling/index.html#full-circle",
    "href": "posts/uniform-circle-sampling/index.html#full-circle",
    "title": "Uniform Sampling Inside a Circle",
    "section": "2.4 Full circle",
    "text": "2.4 Full circle\nUsing the \\(c\\) as a factor for the number of samples \\(k\\) and by inversing \\(x\\) and \\(y\\) by chance, we can finally generate a full circle. This is done by sampling \\(x \\in [-r_{\\text{sampled}}, r_{\\text{sampled}}]\\) and mirroring the point on the x-axis by chance.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Function to determine the number of points based on the circumference\ndef points_based_on_circumference(r, scale=1):\n    return int(np.ceil(2 * np.pi * r * scale))\n\n# Arrays to store the sampled points\nx_points = []\ny_points = []\n\n# Number of different radii to sample\nnum_radii = 1000\n\nfor _ in range(num_radii):\n    # Sample r from [0, 1]\n    r = np.random.uniform(0, 1)\n\n    # Number of points to sample for this radius\n    num_points = points_based_on_circumference(r)\n\n    for _ in range(num_points):\n        if random.random() &lt; 0.5:\n          # Sample x from [0, r]\n          x = np.random.uniform(-r, r)\n\n          # Calculate y based on the circle formula X^2 + Y^2 = r^2\n          y = np.sqrt(r**2 - x**2)\n          if np.random.rand() &gt; 0.5:\n            y = -y\n        else:\n          y = np.random.uniform(-r, r)\n          x = np.sqrt(r**2 - y**2)\n\n          if np.random.rand() &gt; 0.5:\n            x = -x\n\n        # Randomly decide the sign of y to cover both the top and bottom halves\n\n\n        # Append the point to the arrays\n        x_points.append(x)\n        y_points.append(y)\n\n# Create the plot\nplt.figure(figsize=(6, 6))\nplt.scatter(x_points, y_points, color='turquoise', s=2)\n\n# Set the limits and labels\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Points on Circles with Different Radii')\nplt.grid(True)\n\n# Show the plot\nplt.show()\n\n\n\n\nWith a bit more refactoring, we can also make the code more compact and reusable.\n\n\nRefactored code\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport math\n\ntarget_r = 10\n\nr_granularity = 500\nc_granularity = 1\n\n# Function to determine the number of points based on the circumference\ndef points_based_on_circumference(r, granularity):\n    return int(np.ceil(2 * np.pi * r * granularity))\n\n# Arrays to store the sampled points\nx_points = []\ny_points = []\n\n# Number of different radii to sample\nfor _ in range(r_granularity):\n    r = np.random.uniform(0, target_r)\n\n    # Number of points to sample for this radius\n    num_points = points_based_on_circumference(r, c_granularity)\n\n    for _ in range(num_points):\n        # Sample and calculate vars\n        dep = np.random.uniform(-r, r)\n        indep = np.sqrt(r**2 - dep**2)\n\n        # Mirror on x-axis\n        if np.random.rand() &gt; 0.5:\n          indep = -indep\n\n        # Assign indep. and dep. var\n        x, y = (dep, indep) if np.random.rand() &gt; 0.5 else (indep, dep)\n\n        # Append the point to the arrays\n        x_points.append(x)\n        y_points.append(y)"
  },
  {
    "objectID": "posts/uniform-circle-sampling/index.html#blurring-the-lines",
    "href": "posts/uniform-circle-sampling/index.html#blurring-the-lines",
    "title": "Uniform Sampling Inside a Circle",
    "section": "2.5 Blurring the lines",
    "text": "2.5 Blurring the lines\nHowever, with this approach, we will encounter a final problem. For small values of r_granularity, it becomes clear that the points are sampled on separate circles.\n\n\nCode\ntarget_r = 10\n\nr_granularity = 25\nc_granularity = 5\n\n# Function to determine the number of points based on the circumference\ndef points_based_on_circumference(r, granularity):\n    return int(np.ceil(2 * np.pi * r * granularity))\n\n# Arrays to store the sampled points\nx_points = []\ny_points = []\n\n# Number of different radii to sample\nfor _ in range(r_granularity):\n    r = np.random.uniform(0, target_r)\n\n    # Number of points to sample for this radius\n    num_points = points_based_on_circumference(r, c_granularity)\n\n    for _ in range(num_points):\n        # Sample and calculate vars\n        dep = np.random.uniform(-r, r)\n        indep = np.sqrt(r**2 - dep**2)\n\n        # Mirror on x-axis\n        if np.random.rand() &gt; 0.5:\n          indep = -indep\n\n        # Assign indep. and dep. var\n        x, y = (dep, indep) if np.random.rand() &gt; 0.5 else (indep, dep)\n\n        # Append the point to the arrays\n        x_points.append(x)\n        y_points.append(y)\n\n# Create the plot\nplt.figure(figsize=(6, 6))\nplt.scatter(x_points, y_points, color='turquoise', s=2)\n\n# Set the limits and labels\nplt.xlim(-target_r, target_r)\nplt.ylim(-target_r, target_r)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('target_r = 10, r_granularity = 25, c_granularity = 5')\nplt.grid(True)\n\n# Show the plot\nplt.show()\n\n\n\n\n\nTo solve this problem, we can replace the scaling factor, which is calculated using the circumference \\(c\\) with a probability. So instead of deterministically drawing \\(k = c_{\\text{sampled}} = 2 \\pi r_{\\text{sampled}}\\) points on a circle with radius \\(r_{\\text{sampled}}\\) we will only draw one point with probability \\(p = \\frac{c_{\\text{sampled}}}{c}\\).\n\ntarget_r = 1\n\nr_granularity = target_r * 10000\n\n# Function to determine the number of points based on the circumference\ndef p_acc_based_on_circumference(r):\n    return (2 * np.pi * r) / (2 * np.pi * target_r)\n\n# Arrays to store the sampled points\nx_points = []\ny_points = []\n\n# Number of different radii to sample\nfor _ in range(r_granularity):\n    r = np.random.uniform(0, target_r)\n\n    # Number of points to sample for this radius\n    p_acc = p_acc_based_on_circumference(r)\n\n    if np.random.rand() &lt; p_acc:\n        # Sample and calculate vars\n        dep = np.random.uniform(-r, r)\n        indep = np.sqrt(r**2 - dep**2)\n\n        # Mirror on x-axis\n        if np.random.rand() &gt; 0.5:\n          indep = -indep\n\n        # Assign indep. and dep. var\n        x, y = (dep, indep) if np.random.rand() &gt; 0.5 else (indep, dep)\n\n        # Append the point to the arrays\n        x_points.append(x)\n        y_points.append(y)\n\n# Create the plot\nplt.figure(figsize=(6, 6))\nplt.scatter(x_points, y_points, color='turquoise', s=2)\n\n# Set the limits and labels\nplt.xlim(-target_r, target_r)\nplt.ylim(-target_r, target_r)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Final algorithm')\nplt.grid(True)\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "On this blog I’m sharing project implementations and guides on topics related to machine learning."
  }
]