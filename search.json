[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Uniform Sampling Inside a Circle\n\n\n\n\n\nAn alternative approach to sample points uniformly inside a circle.\n\n\n\n\n\n\nJul 1, 2024\n\n\nMaximilian Weichart\n\n\n\n\n\n\n  \n\n\n\n\nValue & Policy Iteration\n\n\n\n\n\nPython implementations for Value & Policy iteration, two fundamental concepts of Reinforcement Learning.\n\n\n\n\n\n\nFeb 19, 2024\n\n\nMaximilian Weichart\n\n\n\n\n\n\n  \n\n\n\n\nBackpropagation in RNNs\n\n\n\n\n\nHow to work out the derivatives and implement the training loop in pure Python.\n\n\n\n\n\n\nJan 31, 2024\n\n\nMaximilian Weichart\n\n\n\n\n\n\n  \n\n\n\n\nSimulated Annealing\n\n\n\n\n\nHow can we make Entropy work for - and not against us.\n\n\n\n\n\n\nNov 25, 2023\n\n\nMaximilian Weichart\n\n\n\n\n\n\n  \n\n\n\n\nUninformed Search Algorithms\n\n\n\n\n\nIntroduction to search algorithms - a fundamental for understanding artificial intelligence.\n\n\n\n\n\n\nSep 23, 2023\n\n\nMaximilian Weichart\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to RNNs\n\n\n\n\n\nRecurrent Neural Networks are a fundamental concept to understand and offer strengths that uni-directional neural networks lack. How can we use one to predict stock prices?\n\n\n\n\n\n\nAug 12, 2023\n\n\nMaximilian Weichart\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/uniform-circle-sampling/index.html#naive-approach",
    "href": "posts/uniform-circle-sampling/index.html#naive-approach",
    "title": "Uniform Sampling Inside a Circle",
    "section": "3.1 Naive Approach",
    "text": "3.1 Naive Approach\nThe equation for a circle of radius \\(r\\) centered at the origin is given by:\n\\[\ny^2 + x^2 = r^2\n\\tag{1}\\]\nUsing Equation 1, random points on the circumference of a circle can be sampled by selecting random values for \\(x \\in [0,r]\\) and calculating the corresponding \\(y\\)) value (or vice versa). However, Equation 1 yields all points on a circle of a specified radius, whereas our objective is to sample uniformly from the entire area within the circle. This can be achieved by additionally sampling the radius as \\(r_{\\text{sampled}} \\in [0,r]\\). To summarize, the following procedure can be followed as out first attempt to sample points uniformly within a circle of radius \\(r = 1\\):\n\nSample \\(r_{\\text{sampled}} \\in [0,1]\\)\nSample \\(x \\in [0, r_{\\text{sampled}}]\\)\nCalculate \\(y\\) (Equation 1)\nPlot the point \\((x,y)\\)\n\nTo keep the code simple, we’ll just create a quarter circle, that is use \\(x,y \\in [0,1]\\).\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Number of points to sample\nnum_points = 1000\n\n# Arrays to store the sampled points\nx_points = []\ny_points = []\n\nfor _ in range(num_points):\n    r = np.random.uniform(0, 1)\n    x = np.random.uniform(0, r)\n    y = np.sqrt(r**2 - x**2)\n    x_points.append(x)\n    y_points.append(y)\n\n# Plotting the points\nplt.figure(figsize=(6, 6))\nplt.scatter(x_points, y_points, color='turquoise', s=2)\nplt.xlim(0, 1)\nplt.ylim(0, 1)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Initial Sampling - Points Clustered Around Center')\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "posts/uniform-circle-sampling/index.html#circumference-sampling",
    "href": "posts/uniform-circle-sampling/index.html#circumference-sampling",
    "title": "Uniform Sampling Inside a Circle",
    "section": "3.2 Circumference sampling",
    "text": "3.2 Circumference sampling\nThis method is simple, but it has a couple of problems, resulting in a non-uniform density of points. The most obvious problem is, that the points are denser around the circles’ center. The problem arises because the circumference of the circle grows with \\(r_{\\text{sampled}}\\) but the number of samples \\(k\\) doesn’t, as shown in this image.\n\n\n\nAs the circumference grows, the number of samples remains the same, resulting in a non-uniform density of samples.\n\n\nTo fix this, we can simply generate more points as the circle grows. Specifically, we’ll use the circumference \\(c\\) as a scaling factor.\n\\[\nc = 2 \\pi r_{\\text{sampled}}\n\\tag{2}\\]\n\ndef points_based_on_circumference(r, scale=1):\n    return int(np.ceil(2 * np.pi * r * scale))\n\nx_points = []\ny_points = []\nnum_radii = 1000\n\nfor _ in range(num_radii):\n    r = np.random.uniform(0, 1)\n    num_points = points_based_on_circumference(r)\n    for _ in range(num_points):\n        x = np.random.uniform(0, r)\n        y = np.sqrt(r**2 - x**2)\n        x_points.append(x)\n        y_points.append(y)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(x_points, y_points, color='turquoise', s=2)\nplt.xlim(0, 1)\nplt.ylim(0, 1)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Improved Distribution with Circumference-Based Sampling')\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "posts/uniform-circle-sampling/index.html#inverse",
    "href": "posts/uniform-circle-sampling/index.html#inverse",
    "title": "Uniform Sampling Inside a Circle",
    "section": "3.3 Inverse",
    "text": "3.3 Inverse\nNow we notice the second problem: It seems like there are fewer points on the bottom of the circle. To explain what’s going on, it’s easier to look at a single circle generated with this approach.\n\n\nCode\n# Number of points to sample\nnum_points = 100\n\n# Arrays to store the sampled points\nx_points = []\ny_points = []\n\nfor _ in range(num_points):\n    # Sample r from [0, 1]\n    r = 1\n\n    # Sample x from [0, r]\n    x = np.random.uniform(0, r)\n\n    # Calculate y based on the circle formula X^2 + Y^2 = r^2\n    y = np.sqrt(r**2 - x**2)\n\n\n    # Append the point to the arrays\n    x_points.append(x)\n    y_points.append(y)\n\n# Create the plot\nplt.figure(figsize=(6, 6))\nplt.scatter(x_points, y_points, color='turquoise', s=5)\n\n# Set the limits and labels\nplt.xlim(0, 1)\nplt.ylim(0, 1)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Points on a Circle')\nplt.grid(True)\n\n# Show the plot\nplt.show()\n\n\n\n\n\nBecause the circle is drawn by calculating \\(y\\) via \\(x\\), which is uniformly sampled, there are simply more values on the “roof” of the circle than on the side, because from the perspective of \\(x\\), the area is wider. We can fix t his problem by also considering the perspective of \\(y\\), that is inverting \\(x\\) and \\(y\\) by chance.\n\nimport random\n\n# Number of points to sample\nnum_points = 100\n\n# Arrays to store the sampled points\nx_points = []\ny_points = []\n\nfor _ in range(num_points):\n    # Sample r from [0, 1]\n    r = 1\n\n    if random.random() &lt; 0.5:\n      # Sample x from [0, r]\n      x = np.random.uniform(0, r)\n\n      # Calculate y based on the circle formula X^2 + Y^2 = r^2\n      y = np.sqrt(r**2 - x**2)\n    else:\n      y = np.random.uniform(0, r)\n      x = np.sqrt(r**2 - y**2)\n\n\n    # Append the point to the arrays\n    x_points.append(x)\n    y_points.append(y)\n\n# Create the plot\nplt.figure(figsize=(6, 6))\nplt.scatter(x_points, y_points, color='turquoise', s=5)\n\n# Set the limits and labels\nplt.xlim(0, 1)\nplt.ylim(0, 1)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Points on a Circle')\nplt.grid(True)\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "posts/uniform-circle-sampling/index.html#full-circle",
    "href": "posts/uniform-circle-sampling/index.html#full-circle",
    "title": "Uniform Sampling Inside a Circle",
    "section": "3.4 Full circle",
    "text": "3.4 Full circle\nUsing the \\(c\\) as a factor for the number of samples \\(k\\) and by inversing \\(x\\) and \\(y\\) by chance, we can finally generate a full circle. This is done by sampling \\(x \\in [-r_{\\text{sampled}}, r_{\\text{sampled}}]\\) and mirroring the point on the x-axis by chance.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Function to determine the number of points based on the circumference\ndef points_based_on_circumference(r, scale=1):\n    return int(np.ceil(2 * np.pi * r * scale))\n\n# Arrays to store the sampled points\nx_points = []\ny_points = []\n\n# Number of different radii to sample\nnum_radii = 1000\n\nfor _ in range(num_radii):\n    # Sample r from [0, 1]\n    r = np.random.uniform(0, 1)\n\n    # Number of points to sample for this radius\n    num_points = points_based_on_circumference(r)\n\n    for _ in range(num_points):\n        if random.random() &lt; 0.5:\n          # Sample x from [0, r]\n          x = np.random.uniform(-r, r)\n\n          # Calculate y based on the circle formula X^2 + Y^2 = r^2\n          y = np.sqrt(r**2 - x**2)\n          if np.random.rand() &gt; 0.5:\n            y = -y\n        else:\n          y = np.random.uniform(-r, r)\n          x = np.sqrt(r**2 - y**2)\n\n          if np.random.rand() &gt; 0.5:\n            x = -x\n\n        # Randomly decide the sign of y to cover both the top and bottom halves\n\n\n        # Append the point to the arrays\n        x_points.append(x)\n        y_points.append(y)\n\n# Create the plot\nplt.figure(figsize=(6, 6))\nplt.scatter(x_points, y_points, color='turquoise', s=2)\n\n# Set the limits and labels\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Points on Circles with Different Radii')\nplt.grid(True)\n\n# Show the plot\nplt.show()\n\n\n\n\nWith a bit more refactoring, we can also make the code more compact and reusable.\n\n\nRefactored code\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport math\n\ntarget_r = 10\n\nr_granularity = 500\nc_granularity = 1\n\n# Function to determine the number of points based on the circumference\ndef points_based_on_circumference(r, granularity):\n    return int(np.ceil(2 * np.pi * r * granularity))\n\n# Arrays to store the sampled points\nx_points = []\ny_points = []\n\n# Number of different radii to sample\nfor _ in range(r_granularity):\n    r = np.random.uniform(0, target_r)\n\n    # Number of points to sample for this radius\n    num_points = points_based_on_circumference(r, c_granularity)\n\n    for _ in range(num_points):\n        # Sample and calculate vars\n        dep = np.random.uniform(-r, r)\n        indep = np.sqrt(r**2 - dep**2)\n\n        # Mirror on x-axis\n        if np.random.rand() &gt; 0.5:\n          indep = -indep\n\n        # Assign indep. and dep. var\n        x, y = (dep, indep) if np.random.rand() &gt; 0.5 else (indep, dep)\n\n        # Append the point to the arrays\n        x_points.append(x)\n        y_points.append(y)"
  },
  {
    "objectID": "posts/uniform-circle-sampling/index.html#blurring-the-lines",
    "href": "posts/uniform-circle-sampling/index.html#blurring-the-lines",
    "title": "Uniform Sampling Inside a Circle",
    "section": "3.5 Blurring the lines",
    "text": "3.5 Blurring the lines\nHowever, with this approach, we will encounter a final problem. For small values of r_granularity, it becomes clear that the points are sampled on separate circles.\n\n\nCode\ntarget_r = 10\n\nr_granularity = 25\nc_granularity = 5\n\n# Function to determine the number of points based on the circumference\ndef points_based_on_circumference(r, granularity):\n    return int(np.ceil(2 * np.pi * r * granularity))\n\n# Arrays to store the sampled points\nx_points = []\ny_points = []\n\n# Number of different radii to sample\nfor _ in range(r_granularity):\n    r = np.random.uniform(0, target_r)\n\n    # Number of points to sample for this radius\n    num_points = points_based_on_circumference(r, c_granularity)\n\n    for _ in range(num_points):\n        # Sample and calculate vars\n        dep = np.random.uniform(-r, r)\n        indep = np.sqrt(r**2 - dep**2)\n\n        # Mirror on x-axis\n        if np.random.rand() &gt; 0.5:\n          indep = -indep\n\n        # Assign indep. and dep. var\n        x, y = (dep, indep) if np.random.rand() &gt; 0.5 else (indep, dep)\n\n        # Append the point to the arrays\n        x_points.append(x)\n        y_points.append(y)\n\n# Create the plot\nplt.figure(figsize=(6, 6))\nplt.scatter(x_points, y_points, color='turquoise', s=2)\n\n# Set the limits and labels\nplt.xlim(-target_r, target_r)\nplt.ylim(-target_r, target_r)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('target_r = 10, r_granularity = 25, c_granularity = 5')\nplt.grid(True)\n\n# Show the plot\nplt.show()\n\n\n\n\n\nTo solve this problem, we can replace the scaling factor, which is calculated using the circumference \\(c\\) with a probability. So instead of deterministically drawing \\(k = c_{\\text{sampled}} = 2 \\pi r_{\\text{sampled}}\\) points on a circle with radius \\(r_{\\text{sampled}}\\) we will only draw one point with probability \\(p = \\frac{c_{\\text{sampled}}}{c}\\).\n\ntarget_r = 1\n\nr_granularity = target_r * 10000\n\n# Function to determine the number of points based on the circumference\ndef p_acc_based_on_circumference(r):\n    return (2 * np.pi * r) / (2 * np.pi * target_r)\n\n# Arrays to store the sampled points\nx_points = []\ny_points = []\n\n# Number of different radii to sample\nfor _ in range(r_granularity):\n    r = np.random.uniform(0, target_r)\n\n    # Number of points to sample for this radius\n    p_acc = p_acc_based_on_circumference(r)\n\n    if np.random.rand() &lt; p_acc:\n        # Sample and calculate vars\n        dep = np.random.uniform(-r, r)\n        indep = np.sqrt(r**2 - dep**2)\n\n        # Mirror on x-axis\n        if np.random.rand() &gt; 0.5:\n          indep = -indep\n\n        # Assign indep. and dep. var\n        x, y = (dep, indep) if np.random.rand() &gt; 0.5 else (indep, dep)\n\n        # Append the point to the arrays\n        x_points.append(x)\n        y_points.append(y)\n\n# Create the plot\nplt.figure(figsize=(6, 6))\nplt.scatter(x_points, y_points, color='turquoise', s=2)\n\n# Set the limits and labels\nplt.xlim(-target_r, target_r)\nplt.ylim(-target_r, target_r)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Final algorithm')\nplt.grid(True)\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "posts/rnn-2/index.html",
    "href": "posts/rnn-2/index.html",
    "title": "Backpropagation in RNNs",
    "section": "",
    "text": "Visualization functions\ndef plot_zic_zac(X, Y, title, dotted=True):\n  figsize = len(X[0]) + 1\n  fig, axis = plt.subplots(1, len(X), figsize=(figsize * len(X), figsize))\n  if not isinstance(axis, np.ndarray):\n    axis = np.array([axis])\n\n  # Set aspect ratio and y-axis limits for all subplots\n  for ax in axis:\n      ax.set_aspect('auto', adjustable='box')\n      ax.set_ylim(0, 1)\n      ax.set_xlim(0, figsize)\n\n  space = np.linspace(0, figsize, figsize)\n\n  # Input values\n  for i, x in enumerate(X):\n    axis[i].plot(space, x + [np.nan], label=\"x\")\n\n  # Values to be predicted by the RNN\n  continuation_point = figsize / (figsize - 1) * (figsize - 2)\n  for i, y in enumerate(Y):\n    axis[i].plot([continuation_point, figsize], [X[i][-1],y], linestyle='dotted' if dotted else 'solid', label=\"y\")\n\n  plt.suptitle(title)\n  plt.legend()\n  plt.show()\n\ndef plot_simple(X, Y, title, dotted=True):\n    num_plots = len(X)\n    max_len = max([len(x) for x in X]) + 1\n    fig, axis = plt.subplots(1, num_plots, figsize=(max_len * num_plots, max_len))\n    if not isinstance(axis, np.ndarray):\n        axis = np.array([axis])\n\n    # Set aspect ratio and y-axis limits for all subplots\n    for ax in axis:\n        ax.set_aspect('auto', adjustable='box')\n        ax.set_ylim(0, 1)\n        ax.set_xlim(0, max_len)\n\n    # Input values\n    for i, x in enumerate(X):\n        space = np.linspace(0, max_len, len(x) + 1)\n        axis[i].plot(space, x + [np.nan], label=\"x\")\n\n    # Values to be predicted by the RNN\n    for i, y in enumerate(Y):\n        # rescale x-axis values\n        continuation_point = (len(X[i])-1) / len(X[i]) * max_len\n        axis[i].plot([continuation_point, max_len], [X[i][-1], y], linestyle='dotted' if dotted else 'solid', label=\"y\")\n\n    plt.suptitle(title)\n    plt.show()\nIn Part 1 of this series, we implemented an RNN that can predict simple patterns. However, we used parameters from a pretrained model and didn’t learn how to train it on our own. Also, our RNN failed to generalize onto more complex patterns like the following zigzag shape.\ndef lin(x, w, b=0):\n  return x*w+b\n\nclass MiniRnn():\n  w1 = 1.8 # weight linear layer\n  w2 = -0.5 # weight feedback loop\n  w3 = 1.1 # weight output layer\n  b = 0.0\n\n  def forward(self, X, i=0, f=0):\n    # Linear layer + non-linearity\n    l1 = lin(X[i], self.w1, self.b + f)\n    relu = abs(l1)\n\n    # Feedback loop\n    if(i+1 != len(X)):\n      f = relu * self.w2\n      return self.forward(X, i+1, f)\n\n    # Final output\n    l2 = lin(relu, self.w3, self.b)\n    return round(l2, 1)\nrnn = MiniRnn()\nTo make our model perform well on the zigzag dataset, we want to train it using backpropagation. But how does backpropagation work for RNNs? Will it solve our problem with the zigzag shaped data? The goal of this article is to give a concise answer to this question and to provide you with a small model that you can play around by yourself.\nIn this article we will discuss how to compute the gradients mathematically and afterward, we’ll implement these concepts in our model code - without the help of PyTorch."
  },
  {
    "objectID": "posts/rnn-2/index.html#forward",
    "href": "posts/rnn-2/index.html#forward",
    "title": "Backpropagation in RNNs",
    "section": "1 Forward",
    "text": "1 Forward\nFirst, we have to think about what the model is doing in a mathematical way, so we’ll translate the forward pass into equations. Let’s remember how the model was being displayed in the previous part of this series.\n\n\n\nFigure 1: Unrolled schema of an RNN that handles four inputs\n\n\nThe most important thing to note is, that we actually have multiple outputs (\\(y_0\\) to \\(y_2\\) and output value (y) in this image). Of course, we are only interested in the last one because this is our prediction, but to define a formula for the RNN we can’t forget about the previous ones. The simplest way to do this is to introduce a suffix for the which output we want to reference, called a “timestep”.\nIf you’re familiar with programming, you can imagine it like this: The outputs of our RNN are all collected in an array, say y. For example: y = [1, 2, 3, 4]. To get a specific output, you’d index into the array like so: y[0]. In this case, we’d take the model output \\(y\\) at timestep \\(0\\), which we can write as \\(y_0\\) or \\(y_t\\) more generally, \\(t\\) being shorthand for “timestep”.\nNow, let’s write down some formulas to define our model.\n\n\n\n\n\n\nTip\n\n\n\nIf you want to get as much value out of this article as possible, I recommend that you try to do it by yourself as a practise before continuing reading the solutions.\n\n\n\n1.1 Loss\nThe loss function, in our case MSE, can be used for a number of variables, but in our case we only have two, the prediction \\(\\hat{y}_t\\) and the label \\(y_t\\), so we define it as the following\n\\[\nL = MSE(y_t, \\hat{y}_t) =  \\frac{1}{2} \\cdot (y_t - \\hat{y}_t)^2\n\\tag{1}\\]\n\n\n1.2 Output state\nWe know that \\(y_t\\) is the label in our training data, but now we have to specify how \\(\\hat{y}_t\\) is being calculated from the input data \\(x\\) and the weights \\(w_{1-3}\\). Figure 3 displays a schematic overview of the weights and states inside our RNN.\n\n\n\nFigure 2: Schema of an RNN with labels for the weights\n\n\nLet’s translate this process back-to front into formulas. The last step of calculating \\(\\hat{y}\\) is pretty simple, it’s just multiplying the hidden state with \\(w_3\\).\n\\[\n\\hat{y}_t = h_t \\cdot w_3\n\\tag{2}\\]\n\n\n1.3 Hidden state\nOur definition of \\(\\hat{y}_t\\) includes \\(h_t\\), which we’ll try to define now. This is a little bit more complicated, but in essence we’re just doing the following steps:\n\nMultiply the input \\(x_t\\) with \\(w_1\\)\nAdd the result to the previous hidden state \\(h_{t-1}\\)\nApply the activation function, in our case \\(\\text{ReLU}\\)\n\n\\[\nh_t = ReLU(w_1 \\cdot x_t + h_{t-1} \\cdot w_2)\n\\tag{3}\\]"
  },
  {
    "objectID": "posts/rnn-2/index.html#sec-backward",
    "href": "posts/rnn-2/index.html#sec-backward",
    "title": "Backpropagation in RNNs",
    "section": "2 Backward",
    "text": "2 Backward\nGiven Equation 1, Equation 2 and Equation 3, we now want to find out how to calculate the derivatives for $w_{1-3} $step by step so we can implement the process in our model. I’ll try to provide footnotes to the relevant derivative rules where appropriate for readers who are new to the subject.\nThe gradients we’re interested in are those for the variables \\(w_{1-3}\\). Therefore, what we’re looking for is the following:\n\\[\n\\frac{\\partial L}{\\partial w_3};\n\\frac{\\partial L}{\\partial w_2};\n\\frac{\\partial L}{\\partial w_1}\n\\]\n\n2.1 Loss\nTo calculate these derivatives, we first need to derive the loss, in our case the MSE in respect to \\(\\hat{y}_t\\), because \\(\\hat{y}_t\\) is the model prediction and “leads to” all the weights. The derivative1 is pretty straight forward, as the exponent and the fraction cancel out nicely.1 Power rule\n\\[\n\\frac{{\\partial \\text{{L}}}}{{\\partial \\hat{y}_t}}(y_t, \\hat{y}_t) = 2 \\cdot \\frac{1}{2} \\left( y_t - \\hat{y}_t \\right) = y_t - \\hat{y}_t \\tag{4}\n\\]\nImplementing this in Python gives us the following code.\n\ndef mse(x, y):\n  return ((x - y)/2)**2\n\ndef d_mse(x, y):\n  return x - y\n\n\n\n2.2 Output state\nThis alone doesn’t suffice, as we have to continue deriving until we find \\(w_{1-3}\\). The third weight can be derived2 quickly, following from Equation 2.2 Product rule\n\\[\n\\frac{\\partial \\hat{y}_t}{\\partial w_3} = h_t\n\\]\n\n\n2.3 Hidden state\nThe last step is deriving \\(w_{1-2}\\), which are included in Equation 3. Applying the chain-rule, we get the following results (\\(ReLU'\\) means the derivative of the ReLU function).\n\\[\n\\frac{\\partial h_t}{\\partial w_1} = \\text{ReLU}'(h_t) \\cdot x_t\n\\]\n\\[\n\\frac{\\partial h_t}{\\partial w_2} = \\text{ReLU}'(h_t) \\cdot h_{t-1}\n\\]\n\n\n2.4 ReLU\n\\[\n\\text{{ReLU}}(x) = \\begin{cases} x, & \\text{if } x &gt; 0 \\\\ 0, & \\text{otherwise} \\end{cases}\n\\]\nConsidering both cases, we trivially calculate the derivatives3 for each of them.3 Line rule, Constant rule\n\\[\n\\text{ReLU}'(x) = \\begin{cases} 1, & \\text{if } x &gt; 0 \\\\ 0, & \\text{otherwise} \\end{cases}\n\\]\n\ndef relu(x):\n  return max(x,0)\n\ndef d_relu(x):\n  return x &gt; 0\n\n\n\n2.5 Timesteps\nAt this point, we know how to calculate the gradients when we’re at a specific timestep \\(h_t\\). However, we’d like to calculate the gradients in respect to the loss, so based on the chain rule we want to find out: \\[\n\\frac{\\partial L}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial w_3}\n\\] \\[\n\\frac{\\partial L}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial h_t} \\frac{\\partial h_t}{\\partial w_2};\n\\] \\[\n\\frac{\\partial L}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial h_t} \\frac{\\partial h_t}{\\partial w_1};\n\\]\nWe have already calculated all the derivatives, except \\(\\frac{\\partial \\hat{y}}{\\partial h_t}\\). What does this term mean exactly? Let’s have a look at the unrolled network once again.\n\n\n\nFigure 3: Schema of an RNN with labels for the weights\n\n\nConsidering Equation 2, the deriving4 in the case of the last hidden state \\(h_n\\) is straightforward.4 Product rule\n\\[\n\\frac{\\partial \\hat{y}}{\\partial h_{n}} = w_2\n\\]\nHowever, for all previous hidden states \\(h_t\\) the gradient depends on the following state \\(h_{t+1}\\). This is because the gradient “flows” from the loss to the weights. For example, to calculate the gradient for \\(h_2\\) (Figure 3), you need to first calculate the gradient for \\(h_3\\) because this is the only “path” that leads to the loss, which is the source of the gradient. So when calculating the gradients, we have to do it step by step, always depending on the later layers \\(h_{t+1}\\). This is also referred to “backpropagation through time”.\n\\[\nh_{t+1} = ReLU(h_t \\cdot w_2 + x_3 \\cdot w_1)\n\\]\n\\[\n\\frac{\\partial h_{t+1}}{h_{t}} = ReLU'(h_t) \\cdot w_2\n\\]\nAnd putting it all together, we recursively define the derivative in respect to \\(h_{t+1}\\). This calculation will be done until we arrive at \\(h_n\\), where we can stop.\n\\[\n\\frac{\\partial \\hat{y}}{\\partial h_t} = \\frac{\\partial \\hat{y}}{\\partial h_{t+1}} \\frac{\\partial h_{t+1}}{\\partial h_{t}}  = \\frac{\\partial \\hat{y}}{\\partial h_{t+1}} \\cdot ReLU'(h_t) \\cdot w_2\n\\]\n\n\n2.6 All together\nSo now we know how to “find a path” from \\(L\\) to any of \\(w_{1-3}\\). However, to calculate the derivative in respect to \\(w_{1-2}\\), we need to consider all the “paths” at once:\n\n\n\nFigure 4: Schema of the RNN depicting the need for multivariable function derivative\n\n\nThis is because the weights \\(w_{1,2}\\) are used during all timesteps for the calculation for \\(\\hat{y}\\). When calculating the derivatives in respect to the weights, we can’t just pick one path and calculate it for this path only, we have to consider all of them at the same time because all of the “paths” influence \\(L\\). Using the derivative rule for variables in a multivariable function, we finally arrive at the following equations.\n\\[\n\\frac{\\partial L}{\\partial w_2} = \\frac{\\partial L}{\\partial \\hat{y}} \\sum_t{\\frac{\\partial \\hat{y}}{\\partial h_t} \\frac{\\partial h_t}{\\partial w_2}} ;\n\\]\n\\[\n\\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial \\hat{y}} \\sum_t{\\frac{\\partial \\hat{y}}{\\partial h_t} \\frac{\\partial h_t}{\\partial w_1}} ;\n\\]"
  },
  {
    "objectID": "posts/rnn-2/index.html#implementation",
    "href": "posts/rnn-2/index.html#implementation",
    "title": "Backpropagation in RNNs",
    "section": "3 Implementation",
    "text": "3 Implementation\nTo implement all of this in Python, we’ll first refactor our MiniRNN, introducing the data structures for \\(h_t\\) and \\(x_t\\) which are necessary for calculating the partial derivatives.\n\n\nRefactoring MiniRnn\nclass MiniRnn():\n  # Initialize weights\n  w1 = torch.rand(1) # linear layer (W_xh)\n  w2 = torch.rand(1) # feedback loop (W_hh)\n  w3 = torch.rand(1) # output layer (W_hy)\n1  w1.g, w2.g, w3.g = 0.0, 0.0, 0.0\n\n  # Initialize value structures (used for derivatives)\n2  X = None\n3  h = {0: torch.tensor([0.0])}\n\n  def forward(self, X):\n    self.X = X\n\n    # Run through the RNN\n    for i, x in enumerate(X):\n      self.h[i + 1] = relu(self.w1 * x + self.w2 * self.h[i])\n      y = self.w3 * self.h[i + 1]\n\n    return y\n\n  # Utility functions for resetting the class\n  def clear_states(self):\n    self.h = {0:torch.tensor([0.0])}\n    self.X = None\n\n  def zero_grads(self):\n    self.w1.g, self.w2.g, self.w3.g = 0.0, 0.0, 0.0\n\n\n\n1\n\nInitialize the gradients to 0\n\n2\n\nData structure for saving the inputs \\(x_{t}\\)\n\n3\n\nData structure for saving the hidden states \\(h_{t}\\)\n\n\n\n\nFinally, we’ll implement the equations for the partial derivatives from Section 2 in our model to calculate the gradients in the backwards function55 Partly based on https://victorzhou.com/blog/intro-to-rnns/#7-the-backward-phase\n\nclass MiniRnn():\n  # Initialize weights\n  w1 = torch.rand(1) # linear layer (W_xh)\n  w2 = torch.rand(1) # feedback loop (W_hh)\n  w3 = torch.rand(1) # output layer (W_hy)\n  w1.g, w2.g, w3.g = 0.0, 0.0, 0.0\n\n  # Initialize value structures (used for derivatives)\n  X = None\n  h = {0: torch.tensor([0.0])}\n\n  def forward(self, X):\n    self.X = X\n\n    # Run through the RNN\n    for i, x in enumerate(X):\n      self.h[i + 1] = relu(self.w1 * x + self.w2 * self.h[i])\n      y = self.w3 * self.h[i + 1]\n\n    return y\n\n  def backward(self, loss_g):\n    n = len(self.X)\n1    self.w3g = self.h[n] * loss_g\n2    d_h = self.w3 * loss_g\n\n    for i in reversed(range(n)):\n3        tmp = d_h * d_relu(self.h[i+1])\n\n4        self.w2.g += tmp * self.h[i]\n5        self.w1.g += tmp * self.X[i]\n\n        d_h = tmp * self.w2\n\n  def clear_states(self):\n    self.h = {0:torch.tensor([0.0])}\n    self.X = None\n\n  def zero_grads(self):\n    self.w1.g, self.w2.g, self.w3.g = 0.0, 0.0, 0.0\n\n\n1\n\nCalculate \\(\\frac{\\partial L}{\\partial w_3}\\)\n\n2\n\nCalculate \\(\\frac{\\partial \\hat{y}}{\\partial h_{n}}\\)\n\n3\n\nIntroduce temporary variable for less redundant code\n\n4\n\nCalculate \\(\\frac{\\partial h_t}{\\partial w_2}\\)\n\n5\n\nCalculate \\(\\frac{\\partial h_t}{\\partial w_1}\\)\n\n\n\n\n\n3.1 Training\nTo try out whether our implementation is correct, we can train this model on the previous examples of rising and falling data series.\n\nrnn = MiniRnn()\n\nX = [[0, 0.5], [0.5,0.5],[1,0.5]]\ny = [1,0.5,0]\n\ndef train(model, epochs=20000, lr=0.001):\n  for e in range(epochs):\n    for i, x in enumerate(X):\n      d_loss = d_mse(model.forward(x), y[i])\n\n      model.backward(d_loss)\n      model.w1 -= model.w1.g * lr\n      model.w2 -= model.w2.g * lr\n      model.w3 -= model.w3.g * lr\n\n      model.zero_grads()\n\ntrain(rnn)\n\n\n\n3.2 Validation\nLet’s run our model on the examples from the previous article and see how it performs.\n\n\n\n\n\nFigure 5: Input validation data arrays with the expected continuation\n\n\n\n\n\n\n\n\n\nFigure 6: Input validation data arrays with the model prediction\n\n\n\n\nWe can also take a look at what the weights look like:\n\nrnn.w1, rnn.w2, rnn.w3\n\n(tensor([2.3161]), tensor([-0.4927]), tensor([0.8070]))"
  },
  {
    "objectID": "posts/rnn-2/index.html#zigzag-problem",
    "href": "posts/rnn-2/index.html#zigzag-problem",
    "title": "Backpropagation in RNNs",
    "section": "4 Zigzag problem",
    "text": "4 Zigzag problem\nNow let’s train our model on the zigzag shapes and see what it predicts on the data.\n\n\nTraining on zigzag data\nrnn = MiniRnn()\n\nX = [data_zic_zac, data_zic_zac2]\ny = [1,0]\n\ndef train(model, epochs=20000, lr=0.001):\n  for e in range(epochs):\n    for i, x in enumerate(X):\n      d_loss = d_mse(model.forward(x), y[i])\n\n      model.backward(d_loss)\n      model.w1 -= model.w1.g * lr\n      model.w2 -= model.w2.g * lr\n      model.w3 -= model.w3.g * lr\n\n      model.zero_grads()\n\ntrain(rnn)\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Zigzag validation data arrays with the model prediction\n\n\n\n\nThe predictions look better than the one with the previous weights, but they’re far from perfect, so we can’t really say that the model learned the shape.\nOf course, we could experiment with more shapes and all the hyperparameters, but at this point it’s probably best to switch to the successor of the RNN, the LSTM (Long-Term Short-Term Memory) architecture. These kinds of model are in a few ways similar to the RNN, but without some of its drawbacks. They should be able to capture more complicated shapes, and I’d like to introduce them in another article to you."
  },
  {
    "objectID": "posts/uninformed-search/index.html",
    "href": "posts/uninformed-search/index.html",
    "title": "Uninformed Search Algorithms",
    "section": "",
    "text": "Search is a very broad term in the field of artificial intelligence. What seems so intuitive to us as humans, such as finding an efficient route from city A to city B when looking at a map, is not so straightforward when we want a computer to do the same thing.\nAt very first glance, a simple thing such as search may seem irrelevant when we want to create artificial intelligence. It may seem boring, over-simplistic or useless to solve this problem, but it turns out that most of what’s considered artificial intelligence today is per definition just a search problem and in fact, it is solved by one of these simple search algorithms - gradient descent - which is just as simple as any of its relatives, be it from the informed or uninformed, global or local category.\n\n\n\n\n\n\nNote\n\n\n\nThis article is based on the chapter about uninformed search in “Artificial Intelligence: A Modern Approach, 4th Edition” by Stuart Russell and Peter Norvig.\n\n\n\n\nThe first important distinction to make for understanding search is to differentiate between informed and uninformed search. You can think of the former, like searching for your phone in your living room when you have no idea where you left it. The latter is like searching for it while giving it a call, so you hear the general direction where it might be.\nThere are different kinds of uninformed search algorithms, but the ones we’ll be focusing on in this article are Depth-First, Breadth-First and Uniform-Cost search. Each section will briefly introduce the concept and follow up with a concise Python implementation that you can copy and play around with.\n\n\n\nWe’ll start by defining an example scenario for our search. A common search problem is finding a path to a goal state, for example, you may wonder how to find the quickest way from your home to work.\n\n\nGraph initialization\nnodes = ['A', 'B', 'C', 'D', 'E', 'F']\nedges = [\n    ('A', 'B', 2),\n    ('A', 'C', 10),\n    ('B', 'C', 3),\n    ('B', 'D', 4),\n    ('C', 'E', 2),\n    ('D', 'F', 3),\n    ('E', 'F', 2),\n    ('E', 'B', 2)\n]\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# Create an empty graph\ngraph = nx.Graph()\n\n# Add nodes to the graph\ngraph.add_nodes_from(nodes)\n\n# Add edges with associated costs\nfor edge in edges:\n    graph.add_edge(edge[0], edge[1], cost=edge[2])\n\n\n\n\nVisualization functions\nimport colorsys\n\n# Generates a color palette from fully-saturated to unsaturated with the\n# specified amount of steps.\ndef generate_color_gradient(num_steps):\n    hue = 0.4  # Neon Green\n    lightness = 0.5\n    saturation_step = 1.0 / num_steps\n\n    colors = []\n    for i in range(num_steps):\n        # Calculate the current saturation\n        saturation = (i+1) * saturation_step\n\n        # Convert HSL to RGB\n        rgb_color = colorsys.hls_to_rgb(hue, lightness, saturation)\n\n        # Convert RGB values to 8-bit integers\n        rgb_color = tuple(int(value * 255) for value in rgb_color)\n\n        colors.append(rgb_color)\n\n    colors.reverse() # Saturated -&gt; Unsaturated\n    node_colors = [(r/255, g/255, b/255) for r, g, b in colors] # Conversion for networkx\n\n    return node_colors\n\n# Visualizes a graph and tints all visited nodes with a gradient (earliest to latest)\ndef visualize(graph, visited_nodes=[], start_node=\"A\", end_node=\"F\"):\n    pos = nx.spring_layout(graph, seed=42)  # or nx.circular_layout(graph)\n\n    labels = {edge[:2]: edge[2] for edge in edges}  # Dictionary for edge labels\n    color_array = generate_color_gradient(len(visited_nodes)) if visited_nodes else []\n\n    node_colors = []\n    for node in graph.nodes():\n      if node in visited_nodes:\n        # Tint visited nodes from earliest to latest visit\n        node_colors.append(color_array[visited_nodes.index(node)])\n      elif node == start_node or node == end_node:\n        # If there are no visited nodes, mark start and goal with main colors\n        node_colors.append(generate_color_gradient(1)[0])\n      else:\n        # Default color for nodes\n        node_colors.append('silver')\n\n    nx.draw(graph, pos, with_labels=True, node_size=500, node_color=node_colors, font_size=10, font_color='black')\n    nx.draw_networkx_edge_labels(graph, pos, edge_labels=labels)\n\n    plt.axis('off')\n    plt.show()\n\ndef evaluate(algorithm, graph):\n  visited = algorithm(graph)\n  visualize(graph, visited)\n\n\n\n\n\n\n\nFigure 1: Graph of locations A-F, with green locations (A,F) being the start- and goal-states. The edges represent the cost of transitioning from one state to another."
  },
  {
    "objectID": "posts/uninformed-search/index.html#informed-vs-uninformed",
    "href": "posts/uninformed-search/index.html#informed-vs-uninformed",
    "title": "Uninformed Search Algorithms",
    "section": "",
    "text": "The first important distinction to make for understanding search is to differentiate between informed and uninformed search. You can think of the former, like searching for your phone in your living room when you have no idea where you left it. The latter is like searching for it while giving it a call, so you hear the general direction where it might be.\nThere are different kinds of uninformed search algorithms, but the ones we’ll be focusing on in this article are Depth-First, Breadth-First and Uniform-Cost search. Each section will briefly introduce the concept and follow up with a concise Python implementation that you can copy and play around with."
  },
  {
    "objectID": "posts/uninformed-search/index.html#setup",
    "href": "posts/uninformed-search/index.html#setup",
    "title": "Uninformed Search Algorithms",
    "section": "",
    "text": "We’ll start by defining an example scenario for our search. A common search problem is finding a path to a goal state, for example, you may wonder how to find the quickest way from your home to work.\n\n\nGraph initialization\nnodes = ['A', 'B', 'C', 'D', 'E', 'F']\nedges = [\n    ('A', 'B', 2),\n    ('A', 'C', 10),\n    ('B', 'C', 3),\n    ('B', 'D', 4),\n    ('C', 'E', 2),\n    ('D', 'F', 3),\n    ('E', 'F', 2),\n    ('E', 'B', 2)\n]\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# Create an empty graph\ngraph = nx.Graph()\n\n# Add nodes to the graph\ngraph.add_nodes_from(nodes)\n\n# Add edges with associated costs\nfor edge in edges:\n    graph.add_edge(edge[0], edge[1], cost=edge[2])\n\n\n\n\nVisualization functions\nimport colorsys\n\n# Generates a color palette from fully-saturated to unsaturated with the\n# specified amount of steps.\ndef generate_color_gradient(num_steps):\n    hue = 0.4  # Neon Green\n    lightness = 0.5\n    saturation_step = 1.0 / num_steps\n\n    colors = []\n    for i in range(num_steps):\n        # Calculate the current saturation\n        saturation = (i+1) * saturation_step\n\n        # Convert HSL to RGB\n        rgb_color = colorsys.hls_to_rgb(hue, lightness, saturation)\n\n        # Convert RGB values to 8-bit integers\n        rgb_color = tuple(int(value * 255) for value in rgb_color)\n\n        colors.append(rgb_color)\n\n    colors.reverse() # Saturated -&gt; Unsaturated\n    node_colors = [(r/255, g/255, b/255) for r, g, b in colors] # Conversion for networkx\n\n    return node_colors\n\n# Visualizes a graph and tints all visited nodes with a gradient (earliest to latest)\ndef visualize(graph, visited_nodes=[], start_node=\"A\", end_node=\"F\"):\n    pos = nx.spring_layout(graph, seed=42)  # or nx.circular_layout(graph)\n\n    labels = {edge[:2]: edge[2] for edge in edges}  # Dictionary for edge labels\n    color_array = generate_color_gradient(len(visited_nodes)) if visited_nodes else []\n\n    node_colors = []\n    for node in graph.nodes():\n      if node in visited_nodes:\n        # Tint visited nodes from earliest to latest visit\n        node_colors.append(color_array[visited_nodes.index(node)])\n      elif node == start_node or node == end_node:\n        # If there are no visited nodes, mark start and goal with main colors\n        node_colors.append(generate_color_gradient(1)[0])\n      else:\n        # Default color for nodes\n        node_colors.append('silver')\n\n    nx.draw(graph, pos, with_labels=True, node_size=500, node_color=node_colors, font_size=10, font_color='black')\n    nx.draw_networkx_edge_labels(graph, pos, edge_labels=labels)\n\n    plt.axis('off')\n    plt.show()\n\ndef evaluate(algorithm, graph):\n  visited = algorithm(graph)\n  visualize(graph, visited)\n\n\n\n\n\n\n\nFigure 1: Graph of locations A-F, with green locations (A,F) being the start- and goal-states. The edges represent the cost of transitioning from one state to another."
  },
  {
    "objectID": "posts/rnn-1/index.html",
    "href": "posts/rnn-1/index.html",
    "title": "Introduction to RNNs",
    "section": "",
    "text": "Imagine how cool it would be if you could see the future. Or, at least, how stock prices develop in the next week. You would be rich very, very fast. But instead of doing all the work for yourself, how about developing a model that does the predictions for us? If we could somehow figure out how to build such a model, we’d never have to worry about money again. So let’s do it!\n\n\n\n\n\n\nNote\n\n\n\nThis notebook is based on Recurrent Neural Networks (RNNs), Clearly Explained!!! by StatQuest:\n\n\n\n\n\nLet’s take a look at some imaginary stock courses to get an idea of the data we’re working with.\n\n\n\n\n\nFigure 1: Examples for stock price changes (values are chosen purely arbitrary)\n\n\n\n\nIn this simplified example, we can see the stock of two companies change throughout the years. So how could we use a model to predict these changes?\n\n\n\nWhen we take a closer look at this kind of data, we can notice a couple of challenges that we have to solve.\n\n\nThe amount of data points for each stock can vary. In this example, there are 4 values for Googles, but only 3 values for OpenAIs stock. If you’ve worked with neural networks before, you know that this problem is not straightforward to solve. Usually, models expect a fixed number of inputs and produce a fixed number of outputs. However, our use case requires the model to work with different amounts of input data!\n\n\n\nThe values of our input data don’t necessarily form a straight line. In the example of OpenAI, the value decreases, then increases again. Because of this complexity, we won’t get very far with simple statistics like taking the mean or doing linear regression.\nSo how could we solve these problems?\n\n\n\n\n\n\nNote\n\n\n\nTake a moment and just brainstorm a couple of solutions. They don’t have to be perfect, but just ask yourself: How could you solve these two problems?"
  },
  {
    "objectID": "posts/rnn-1/index.html#stock-data",
    "href": "posts/rnn-1/index.html#stock-data",
    "title": "Introduction to RNNs",
    "section": "",
    "text": "Let’s take a look at some imaginary stock courses to get an idea of the data we’re working with.\n\n\n\n\n\nFigure 1: Examples for stock price changes (values are chosen purely arbitrary)\n\n\n\n\nIn this simplified example, we can see the stock of two companies change throughout the years. So how could we use a model to predict these changes?"
  },
  {
    "objectID": "posts/rnn-1/index.html#challenges",
    "href": "posts/rnn-1/index.html#challenges",
    "title": "Introduction to RNNs",
    "section": "",
    "text": "When we take a closer look at this kind of data, we can notice a couple of challenges that we have to solve.\n\n\nThe amount of data points for each stock can vary. In this example, there are 4 values for Googles, but only 3 values for OpenAIs stock. If you’ve worked with neural networks before, you know that this problem is not straightforward to solve. Usually, models expect a fixed number of inputs and produce a fixed number of outputs. However, our use case requires the model to work with different amounts of input data!\n\n\n\nThe values of our input data don’t necessarily form a straight line. In the example of OpenAI, the value decreases, then increases again. Because of this complexity, we won’t get very far with simple statistics like taking the mean or doing linear regression.\nSo how could we solve these problems?\n\n\n\n\n\n\nNote\n\n\n\nTake a moment and just brainstorm a couple of solutions. They don’t have to be perfect, but just ask yourself: How could you solve these two problems?"
  },
  {
    "objectID": "posts/rnn-1/index.html#setup",
    "href": "posts/rnn-1/index.html#setup",
    "title": "Introduction to RNNs",
    "section": "2.1 Setup",
    "text": "2.1 Setup\nFirst, we’ll introduce a couple of example stock data with more or less simple forms to work with:\n\nThree simple stocks, representing rising, falling and constant values\nA more complicated stock, which falls and rises\n\n\nstock_rising = [0, 0.5] # expected continuation: 1\nstock_constant = [0.5, 0.5] # expected continuation: 0.5\nstock_falling = [1, 0.5] # expected continuation: 0\nstock_curve = [1, 0.5, 0.5] # expected continuation: 1\n\n\n\n\n\n\n\nNote\n\n\n\nThe values in our dataset are normalized to 0 - 1.\n\n\nLet’s visualize the values, so we get a feeling of what’s going on.\n\n\nVisualization function\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_data(X, Y, title, dotted=True):\n    num_plots = len(X)\n    max_len = max([len(x) for x in X]) + 1\n    fig, axis = plt.subplots(1, num_plots, figsize=(max_len * num_plots, max_len))\n    if not isinstance(axis, np.ndarray):\n        axis = np.array([axis])\n\n    # Set aspect ratio and y-axis limits for all subplots\n    for ax in axis:\n        ax.set_aspect('auto', adjustable='box')\n        ax.set_ylim(0, 1)\n        ax.set_xlim(0, max_len)\n\n    # Input values\n    for i, x in enumerate(X):\n        space = np.linspace(0, max_len, len(x) + 1)\n        axis[i].plot(space, x + [np.nan], label=\"x\")\n\n    # Values to be predicted by the RNN\n    for i, y in enumerate(Y):\n        # rescale x-axis values\n        continuation_point = (len(X[i])-1) / len(X[i]) * max_len\n        axis[i].plot([continuation_point, max_len], [X[i][-1], y], linestyle='dotted' if dotted else 'solid', label=\"y\")\n\n    plt.suptitle(title)\n    plt.show()\n\n\n\n\n\n\n\nFigure 2: Input stock data arrays and the expected continuation"
  },
  {
    "objectID": "posts/rnn-1/index.html#rnn-1",
    "href": "posts/rnn-1/index.html#rnn-1",
    "title": "Introduction to RNNs",
    "section": "2.2 RNN",
    "text": "2.2 RNN\n\n2.2.1 Architecture\nGiven a sequence of input values \\(x\\), predicting an outcome \\(y\\) is not a problem. If you’ve worked with neural networks before, you’ve done it a thousand times. You define a sequence of layers and activation functions (to keep it simple we’ll use linear layers and relu only), as well as the number of inputs and outputs together with the correct weights, the model will solve the problem. The problem of the stock courses having a complex form1 can be easily solved with a neural network with enough layers.1 see Section 1.2.2\n\n\n\nFigure 3: Schema of a feed forward neural network with four inputs and one output\n\n\nThe problem that remains even in a classic feed forward neural network is, that it only works with a fixed number of inputs2. To make the model more flexible, RNNs use a “feedback loop”, which solves exactly that problem.2 see Section 1.2.1\nThink about how you make a prediction of one of these stock courses. If you’re like me, you will read the line from the beginning to the end and build up an overall feeling of the development. In the example of the Google stock, you could think: It starts out at $400, but it’s decreasing… and again, it’s decreasing - and so on. RNNs do a similar thing, while a normal feed-forward neural network looks at all the data points at the same time and comes to a conclusion all at once.\nImagine a more complex scenario: A stock with 10,000 values over multiple years. A neural network with a fixed input size would look at all the values at once, as if it had 10,000 eyes, and calculate the prediction. Intuitively, an RNN is much more like a human. It looks at each data point sequentially (“feedback loop”) and builds up an overall opinion (“hidden state”, \\(h\\)) of the stock, until it’s reached the last data point, at which it will output its prediction.\n\n\n\nFigure 4: Unrolled schema of an RNN that handles four inputs\n\n\nAs you can see, the RNN does the same thing over and over again, until it’s looked at all data points. This is why it can be summarized to be more concise, in a recursive version, which gives us the final architecture of an RNN:\n\n\n\nFigure 5: Concise schema of an RNN that handles four inputs. The input values are being passed sequentially, one by one)\n\n\n\n\n2.2.2 Implementation\nNow that we know how the architecture of an RNN looks like, let’s implement it in Python. Essentially, we want to implement the feedback loop, which consists of a linear layer and an activation function, and the output layer, which is another linear layer\n\ndef lin(x,w,b=0):\n    return x*w+b\n\ndef relu(x):\n    return max(x,0)\n\nclass MiniRnn():\n  w1 = 1.4\n  w2 = -0.5\n  w3 = 1.4\n    \n  def forward(self, X, i=0, h=0):\n1    l1 = lin(X[i], self.w1)\n      \n2    h = relu(l1 + h*self.w2)\n3    if(i+1 != len(X)):\n        return self.forward(X, i+1, h)\n    \n    # Output\n4    l2 = lin(h, self.w3)\n    return round(l2, 1)\n\nrnn = MiniRnn()\n\n\n1\n\nInput layer\n\n2\n\nHidden state & activation function (relu)\n\n3\n\nFeedback loop\n\n4\n\nOutput layer\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe weights w1, w2, w3 have been calculated using gradient descent. In the next part of this series, we will learn how to do that ourselves - for now we’ll just use them as they are.\n\n\nLet’s think about how the model will calculate its prediction.\n\nThe RNN iterates over all values in the input sequence (X) and starts with no memory (“hidden state”) at all because h=0 in the first iteration.\nAfter looking at a new value, it saves all its “thoughts” in the hidden state h, until it iterated over all elements.\nOnce it has looked at all values (and only then because this is the only return statement that terminates the recursion) the model forms its conclusion and returns the prediction\n\nIf you’re interested in a more formal definition of the RNN, check out part 2 of this series, where I will introduce some of the math related to RNNs.\n\n\n\n\n\n\nImportant\n\n\n\nIf you’ve watched the StatQuest video on RNNs, you may notice that relu activation function is being applied before its being multiplies with w2. If we followed the video, our implementation would be: h = max(l1 + h, 0) * w2. However, the implementation from the video is wrong, it is not how RNNs are defined.\nFor now, this is not too important, but we’ll see in part 2 why this makes a huge difference.\n\n\n\n\n2.2.3 Testing\nLet’s validate the model by looking at the outputs:\n\nr1 = rnn.forward(stock_rising)\nr2 = rnn.forward(stock_constant)\nr3 = rnn.forward(stock_falling)\nr4 = rnn.forward(stock_curve)\nr1,r2,r3,r4\n\n(1.0, 0.5, 0.0, 1.0)\n\n\n\n\n\n\n\nFigure 6: Input stock data arrays with the model prediction\n\n\n\n\nThe RNN can predict the next values for all the stocks, nice!\nTo understand each step in detail, here are the steps our RNN took in forward-pass for stock_falling.\n\nstock_falling\n\n[1, 0.5]\n\n\n\nl1 = relu(1 * 1.4 + 0 * -0.5)\nl2 = relu(0.5 * 1.4 + l1 * -0.5)\nl3 = 1.4 * l2\nprint(f\"Layers: l1: {l1} l2: {l2} l3: {l3}\\nResult: {l3}\")\n\nLayers: l1: 1.4 l2: 0.0 l3: 0.0\nResult: 0.0\n\n\nAs you can see, each layer uses the output from the previous layer and the same three weights are being shared across all calculations. Even if we passed in a sequence of 100 values, we’d only be using w1,w2 and w3 in the model.\n\n\n2.2.4 Validating\nUntil this point, I only used the examples from the StatQuest video. However, these are not the only values we should try out. What about other forms? It could also be that the model just learned to memorize the four input arrays and maps a single value onto each of them.\nTo validate if our RNN can actually generalize, let’s change the input data. We’ll still use values which are normalized to 0 - 1, but we’ll change their amplitude.\n\nsmall_rising = [0,0.1]\nsmall_constant = [0.1,0.1]\nsmall_falling = [1,0.9]\nsmall_curve = [0.2,00.1,0.1]\n\n\n\n\n\n\nFigure 7: Input validation data arrays with the expected continuation\n\n\n\n\nOur validation data still represents the same shapes, but with different values. This should make it harder for a model that just “fakes” the output by remembering the training data. Let’s see what our RNN does with it.\n\ns1 = rnn.forward(small_rising)\ns2 = rnn.forward(small_constant)\ns3 = rnn.forward(small_falling)\ns4 = rnn.forward(small_curve)\ns1,s2,s3,s4\n\n(0.2, 0.1, 0.8, 0.2)\n\n\n\n\n\n\n\nFigure 8: Input validation data arrays with the model prediction\n\n\n\n\nThe model can correctly predict all of our validation data! This means that it can generalize and find some basic patterns in the data and is not just remembering our training dataset. This is quite an achievement. With these few lines of code, we’re able to create a model that can predict any kind of rising / constant / falling sequence of values, without changing any parameter. Imagine how far this approach can go when we scale up the model and increase its complexity!"
  },
  {
    "objectID": "posts/simulated-annealing/index.html",
    "href": "posts/simulated-annealing/index.html",
    "title": "Simulated Annealing",
    "section": "",
    "text": "Life is full of solving problems. We are constantly searching for solutions, and in Artificial Intelligence there’s a whole subfield about search algorithms. In this article, I want to introduce the algorithm which fascinates me the most."
  },
  {
    "objectID": "posts/simulated-annealing/index.html#the-problem",
    "href": "posts/simulated-annealing/index.html#the-problem",
    "title": "Simulated Annealing",
    "section": "1 The problem",
    "text": "1 The problem\nSearch Problems are difficult to solve. But why is that? Essentially, it is because the things we hope to find are a lot less common than the things we do not want to find. Take, for example, a Rubik’s cube: It has 43 quintillion configurations, out of only 1 is the correct one. If we were to approach this problem without a strategy, the chances are 1:43 quintillion of finding the solution every time we make a turn. Entropy can be defined as the number of states that a system can have1. So in the example of the Rubik’s cube, it is low when we only consider a “solved” Rubik’s cube, but high when we consider an “unsolved” Rubik’s cube. Together with the fact that over time, entropy tends to a maximum2, it’s the reason that making random turns on the Rubik’s cube most likely will leave it in a worse state.1 this definition is simplistic and applies more specifically to the context of statistical mechanics2 The second law of thermodynamics\n\n\n\nSimplified overview over the problem: There are many “bad” states for the Rubik’s cube and only one “solved” state (red). The chances of picking it are really low.\n\n\nIt almost seems like most of our real-world problems exist because we want order (low entropy) while the world tends to move toward chaos (high entropy). If we could reverse these laws, even if it’s just for a moment, we could solve so many problems without any effort. If you had a button to inverse Entropy in our example, you could solve the Rubik’s cube by throwing it down the stairs.\nSimulated annealing is a search algorithm that was inspired by the annealing process in physics, which in fact leverages Entropy to solve problems. But how does it do it? Increasing Entropy, by its very definition, is just a consequence of probabilities. So to understand the magic, we have to look at the world from a statistical perspective because if we can change the probabilities, we can make Entropy work for - and not against us."
  },
  {
    "objectID": "posts/simulated-annealing/index.html#setup",
    "href": "posts/simulated-annealing/index.html#setup",
    "title": "Simulated Annealing",
    "section": "2 Setup",
    "text": "2 Setup\nBefore we try to understand how simulated annealing works, I want to simplify the problem and the tools we use to make it more approachable. When working with informed search algorithms like simulated annealing, you are working with the algorithm itself, which is a function, and also a heuristic (or cost) function. The cost function tells you how far away from your goal you are - not more, not less. It doesn’t say how to get closer, just how bad the current state is. And because it says how close we are to the goal, minimizing it is our main goal.\nBecause the Rubik’s cube is such a complicated example, let’s choose a simpler problem for now. Let’s just assume that our cost function3 is defined as follows:3 This function has been randomly chosen and includes a local and global minimum\n\\[f(x) = x^{2} + 10 * sin(x)+ 15\\]\n\ndef f(x):\n    return x**2 + 10 * np.sin(x) + 15\n\nFrom now on, we want to minimize the output of this cost function and therefore find a solution for \\(x\\) that makes \\(f(x)\\) as small as possible, ideally \\(0\\). We can plot how our cost function looks like:\n\n\n\n\n\nFigure 1: \\(f(x) = x^{2} + 10 * sin(x)+ 15\\) (plotted from -10 to 10)"
  },
  {
    "objectID": "posts/simulated-annealing/index.html#building-a-strategy",
    "href": "posts/simulated-annealing/index.html#building-a-strategy",
    "title": "Simulated Annealing",
    "section": "3 Building a strategy",
    "text": "3 Building a strategy\n\n3.1 Random\nTo solve this problem and to understand how simulated annealing does it so well, we should look at it from a statistical perspective. Perhaps the simplest approach to finding a solution is to pick a value randomly. How high would the probability of finding the global minimum at around -1.5 be? We can visualize this strategy using a probability density function (PDF).\n\ndef algorithm_a(f, iterations):\n  x_values = []\n  y_values = []\n  for t in range(iterations):\n1    x_values.append(random.uniform(-20, 20))\n    y_values.append(f(x_values[t]))\n\n  return x_values, y_values\n\n\n1\n\nAt each timestep we just select a random \\(x\\) for our function\n\n\n\n\n\n\n\n\n\nFigure 2: Probability density function for the ‘random’ strategy\n\n\n\n\nAs you can see, all values of x are equally likely to be picked as the solution, which is of course wrong. Ideally, we want our algorithm to find the solution more often than the wrong answers.\n\n\n3.2 Preference\nTo find the ideal solution more reliably, we could define our algorithm in a way, that it’s more likely that a cost-improvement will be accepted rather than a regression. The simplest way would be to accept worse solutions only sometimes, say 50% of the time:\n\ndef algorithm_b(f, iterations, initial_x):\n  x_values = [initial_x]\n  y_values = [f(initial_x)]\n\n  for t in range(iterations):\n    current_x, current_y = x_values[-1], y_values[-1]\n    new_x = random.uniform(-20, 20)\n    new_y = f(new_x)\n1    if new_y &lt; current_y or random.random() &lt; 0.50:\n      x_values.append(new_x)\n      y_values.append(new_y)\n\n  return x_values, y_values\n\n\n1\n\nOnly accept “worse” solutions 50% of the time\n\n\n\n\n\n\n\n\n\nFigure 3: Probability density function for the ‘preference’ strategy\n\n\n\n\nIt looks like this strategy helped the algorithm to identify the global minimum more reliably, but it’s still yielding many wrong results.\n\n\n3.3 Neighbors\nThe current implementation is inefficient because the progress we made at identifying the best solution will be reset on every time step. Imagine trying to solve a Rubik’s cube, but instead of slowly building up your solution, you choose an entirely new configuration every time you make a turn, instead of improving the current one. So instead of choosing an entirely new solution every time we make a move, let’s only consider neighboring solutions, that is solutions within a certain interval of the current one. Only considering neighboring states is one important concept that simulated annealing employs.\n\ndef algorithm_c(f, iterations, initial_x, p):\n  x_values = [initial_x]\n  y_values = [f(initial_x)]\n\n  for t in range(iterations):\n    current_x, current_y = x_values[-1], y_values[-1]\n1    new_x = current_x + random.uniform(-1, 1)\n    new_y = f(new_x)\n    if new_y &lt; current_y or random.random() &lt; p:\n      x_values.append(new_x)\n      y_values.append(new_y)\n\n  return x_values, y_values\n\n\n1\n\nOnly consider neighboring values of the current states by reducing the interval that we sample from to \\([-1, 1]\\), which are the neighboring states only. If we were to sample from \\([-20, 20]\\) like before, we’d lose our progress at every step.\n\n\n\n\n\n\n\n\n\nFigure 4: Probability density function for the ‘neighbour’ approach\n\n\n\n\nAs you can see, the probability that the algorithm yields the correct solution is a lot higher than for any other value. But it’s still not guaranteed, and a lot of the time it returns wrong answers. The variance is still too high. Ideally, we’d always want the highest point of the probability density function to be returned as our solution and ignore all the other values. However, we can’t just set the probability of accepting a worse state to 0 because this would lead to the local optimum quite often. Once we’re in the local optimum and don’t accept “worse” states anymore, there’d be no way out. So we have to find another approach to lead us to the global optimum.\nHow can we solve this? Instead of immediately setting the acceptance probability for worse states to 0%, we could start at 100% and gradually decrease it. Every time we decrease the probability, we become a little more deterministic and in that way we reject bad states more often gradually: At first, we reject the ridiculous states, like configurations of a Rubik’s cube that are completely mixed. After that, we lower the threshold of good states even further, so of all the not-so-bad states, we accept only the better ones.\nWe can imagine this process like filtering out the best solution gradually. The state space for the Rubik’s cube could be represented in the following image. At a high acceptance-probability, we do the rough work, filtering out all the terrible configurations, resulting in a better subset of configurations. After that, we refine our search, filtering out the even better states from our previous subset. We refine this process even further until we arrive at a 0% acceptance probability for worse states, at which point we just pick the perfect fit.\n\n\n\nFiltering step by step. Each rectangle represents a step in the filtering process which is getting refined gradually. The filtering continues until only the solution state remains.\n\n\nThis process works like a sieve: at each step we increase the quality and build upon our previous work. If we were to keep the acceptance-probability low, we’d have countless hit-or-miss results because we focus in on one particular configuration, before filtering out all the bad states. If we kept it fixed at a high value, we’d never arrive at an excellent solution because we’d never “filter” out the bad states.\nFrom now on, we’ll call this probability of accepting worse states “Temperature” because that’s the term used in the simulated annealing algorithm4. In our code, it’s written as t:4 Later we’ll expand this definition, but for now, it’s just called the Temperature.\n\ndef algorithm_d(f, iterations, initial_x):\n    x_values = [initial_x]\n    y_values = [f(initial_x)]\n1    t_values = [1]\n\n    for k in range(iterations):\n        current_x, current_y = x_values[-1], y_values[-1]\n        new_x = current_x + random.uniform(-1, 1)\n        new_y = f(new_x)\n2        if new_y &lt; current_y or random.random() &lt; t_values[-1]:\n            x_values.append(new_x)\n            y_values.append(new_y)\n\n3        t_values.append(1 - k / iterations)\n\n    return x_values, y_values, t_values\n\n\n1\n\nWe start with a temperature of 1 (100% acceptance probability)\n\n2\n\nThis change means the same as in our previous algorithms. Because we save the temperatures in an array (for visualization), we want to consider the latest entry.\n\n3\n\nWe decrease the temperature linearly\n\n\n\n\nThe results of this algorithm look as follows:\n\n\n\n\n\nFigure 5: Probability density function for the ‘falling-temperature’ approach\n\n\n\n\nWe can compare this to the solutions for different temperatures from algorithm_c in Section 3.3 with different fixed temperatures:\n\n\n\n\n\nFigure 6: Probability density function for the results of the ‘neighbors’ approach with different fixed temperatures\n\n\n\n\nLooking at this new PDF, we can notice a couple of things:\n\nThe results in Figure 5 are more reliable than the PDF with a fixed low temperature (0.01)\nThe results in Figure 5 are more concrete than the PDF with fixed high temperature (0.9)\n\nOur results are more reliable because we consider a wider array of possibilities than when starting with a really low acceptance probability, which would immediately focus in on a small section of the graph. Using algorithm_c with t=0.01 will lead to the global minimum quite often. But it’s also more concrete than the results we get from using a fixed high temperature. This is because once we reach a low temperature, we focus in on the details, refining our solution.\nWe can also take a look at how the results from our new algorithm develop over time. The following animation shows the PDF of the values picked by the algorithm in an interval of 10%. As expected, they start out very broad and incorrect, but over time it becomes more narrow and correct.\n\n\n\nProbability density function over time for a fractions of timesteps ‘falling-temperature-fractions’\n\n\n\n\n3.4 Quality\nAll the strategies we adapted so far lead to some good results. However, dropping the acceptance probability linearly is a simplification and is rarely the case in reality. You could think about what it’s like writing an essay: The outline probably takes less time than all the revisions and details. This is also known as the Pareto Rule (or 80-20 principle) and it shows that to be optimal, we need a more accurate approach for lowering the acceptance probability.\nTo solve this, we could come up with different temperature schedules, like a geometric decay. However, this doesn’t address the underlying problem and may only work for some cases.\nThus far, our acceptance probability was equal to the temperature when considering “worse” states. That means, that, no matter how bad the new state is, we will always accept it with a fixed probability (given by the current temperature). However, it could be quite efficient to take the quality of this worse state into consideration. For example, when solving a Rubik’s Cube, we prefer “bad” states over “terrible” states. So we should decrease the acceptance probability for states as they approach “terrible”.\nThe question becomes, how could we express this as a formula? Obviously, we want to integrate \\(\\Delta Cost\\) (the difference between y_old and y_new) into our acceptance probability, as this tells us how “bad” the proposed state is. However, \\(\\Delta Cost\\) can have any value in the range \\([0, \\inf[\\) so to normalize it as a probability, there’s a simple trick. We’ll just plug it into the function \\(e^{-x}\\).\n\n\n\n\n\nFigure 7: \\(e^{-x}\\) plotted from 0 to 5”\n\n\n\n\nThis normalizes the value of \\(\\Delta Cost\\) to \\([0, 1]\\) and \\(e^{- \\Delta Cost}\\) can therefore be used as a probability.\nThe only thing left to do it so integrate the temperature \\(T\\) into the formula. Just like \\(\\Delta Cost\\), we know that \\(T\\) can be in the range of \\([0, \\inf[\\), however in contrast to \\(\\Delta Cost\\), a high \\(T\\) should result in a high acceptance probability. To add \\(T\\) into the equation, we can divide \\(\\Delta Cost\\) in the exponent by \\(T\\). This can be expressed as the following formula:\n\\[\ne^{-\\frac{\\Delta Cost}{T}}\n\\]\nDividing by \\(T\\) effectively “weakens” the effect of \\(\\Delta C\\) when \\(T\\) is high, resulting in a high acceptance probability.\n\n3.4.1 Boltzmann distribution\nThe above explanation serves as an intuitive approach to simulated annealing. If you’re satisfied with this explanation, you can skip this section and jump right to the implementation in Section 3.4.3, if not, here’s my attempt to illustrate it with the theoretical background as well.\n\n\n\n\n\n\nNote\n\n\n\nTo be humble, I am not convinced that my understanding of the Boltzmann Distribution in simulated annealing is sufficient at this point. However, I decided to include this part in the article to encourage feedback and discussions about this concept so that I can ultimately learn and understand it better. My goal is therefore not to provide a perfect answer right now, but to improve this explanation in the future through your participation!\n\n\nLet’s think about what the acceptance probability is supposed to do: It should tell us how likely the state that we’re observing is getting us closer to the solution. And the solution in our case is the most likely outcome, as we can see in Figure 5. The thing we really want to know to be optimal is, how likely is it that our system is in the state that we’re observing? So if we know how likely a state is, then we know how good, that is how close to the solution it is.\nThe Boltzmann distribution gives us exactly that: it is a probability distribution that gives the probability that a system will be in a certain state as a function of that state’s energy and the temperature of the system5. It is defined as:5 Source: Wikipedia\n\\[\np_i \\propto e^{- \\frac{\\varepsilon_i}{kT}}\n\\]\n\n\\(\\varepsilon_i\\): Energy at a specific state \\(i\\)\n\nin our example “energy” = “cost” = y-value, or as previously defined: \\(\\Delta E = \\Delta Cost\\)\n\n\\(T\\): Temperature\n\\(k\\): Boltzmann constant, can be ignored and assumed to be \\(1\\)\n\n\n\n3.4.2 Boltzman factor\nThis is the entire distribution and ultimately, what we’re interested in. To calculate it, we use the Boltzmann factor, which is defined as:\n\\[\ne^{-\\frac{\\Delta E}{T}}\n\\]\n\n\\(\\Delta E\\): Difference in energy between two states\n\n\\(\\Delta E =\\) new_y - current_y in our code\n\n\nSo all we have to do now is plugging in the temperature and cost into the equation for the Boltzmann factor and use it as the acceptance probability when evaluating the states. This gives us the simulated annealing algorithm.\n\n\n3.4.3 Implementation\n\ndef simulated_annealing(f, iterations, initial_x):\n    x_values = [initial_x]\n    y_values = [f(initial_x)]\n    t_values = [1]\n\n    for k in range(iterations):\n        current_x, current_y = x_values[-1], y_values[-1]\n        new_x = current_x + random.uniform(-1, 1)\n        new_y = f(new_x)\n        \n        delta_e = new_y - current_y\n1        p = exp(-(delta_e)/t_values[-1])\n        \n        p_values.append(p)\n2        if delta_e &lt; 0 or random.random() &lt; p:\n            x_values.append(new_x)\n            y_values.append(new_y)\n\n        t_values.append(1 - k / iterations)\n\n    return x_values, y_values, t_values\n\n\n1\n\nThe Boltzmann factor, which is our acceptance probability for states that are worse than the current one\n\n2\n\nWe still always accept better states without asking, in the other cases we do so based on the Boltzmann factor. Together, this is called the “Metropolis acceptance criterion”\n\n\n\n\nRunning simulated annealing with our example problem yields the following result:\n\n\n\n\n\nFigure 8: Probability density function for the ‘simulated annealing’ algorithm\n\n\n\n\nThe probability of finding the solution with this approach is very high. It’s almost guaranteed now. By tuning the temperature schedule and running more iterations, these results would become even more apparent. We have therefore found a great solution to the problem."
  },
  {
    "objectID": "posts/simulated-annealing/index.html#summary",
    "href": "posts/simulated-annealing/index.html#summary",
    "title": "Simulated Annealing",
    "section": "4 Summary",
    "text": "4 Summary\nLeveraging the concepts discussed in this article, simulated annealing can be used not only to find solutions to simple problems as with our function \\(f(x)\\), but for any search problem, no matter how complex. As long as there’s a cost function and a temperature schedule, simulated annealing is guaranteed to find the optimal solution, given enough time and a temperature schedule that decreases slowly enough.\nThe concepts in this article have even more depth to them. For example, one could ask: why is the Boltzmann distribution defined the way it is? Or how could one find the optimal temperature for the algorithm? All these are questions that deserve their own time to discuss. This article should just give a well reasoned intuition of why it works at all.\nThank you, Vishal, Alex, and the rest of the FastAI MeetUp on Discord for providing feedback for this article."
  },
  {
    "objectID": "posts/dynamic-programming-1/index.html",
    "href": "posts/dynamic-programming-1/index.html",
    "title": "Value & Policy Iteration",
    "section": "",
    "text": "In this article, I implemented a simple example for value and policy iteration from scratch in Python.\nBecause implement the concepts from scratch, the only import we’ll use is NumPy.\nimport numpy as np"
  },
  {
    "objectID": "posts/dynamic-programming-1/index.html#mdp",
    "href": "posts/dynamic-programming-1/index.html#mdp",
    "title": "Value & Policy Iteration",
    "section": "1 MDP",
    "text": "1 MDP\nIn order to do value- and policy iteration, we’ll want to model the MDP. In this case, our “world” will be a 2D grid, where each cell has a reward value.\nTo make life easier, I’ll create a wrapper around the numpy array that contains the data, to introduce some convenient functions that simplify the code.\n\nclass Grid:\n    def __init__(self, matrix):\n        self.matrix = np.array(matrix)\n\n    def get_val(self, state):\n        r, c = self.coord_2_index(state)\n        return self.matrix[r, c]\n\n    def set_val(self, state, value):\n        r, c = self.coord_2_index(state)\n        self.matrix[r, c] = value\n\n    def coord_2_index(self, state):\n        x, y = state\n        return len(self.matrix) - y, x - 1\n\n    def equals(self, grid):\n        return np.array_equal(self.matrix, grid.matrix)\n\n    def display(self):\n        print(self.matrix)\n\nTo perform the updates using the Bellman equations, I’ll create another class modelling the MDP.\n\nclass Mdp:\n    def __init__(self, states, actions, gamma):\n        self.states = states\n        self.actions = actions\n        self.gamma = gamma\n\n    def T(s1, a, s2):\n        pass\n\n    def R(s1, a, s2):\n        pass\n\nOur grid-world example has a few specific transitions (NESW) and a simple reward function. So I’ll create a new class that can model a grid-world of any size provided by the user.\n\nclass MdpGrid(Mdp):\n    def __init__(self, grid, terminal_states, gamma):\n        self.grid = grid\n        self.terminal_states = terminal_states\n        states = [(x, y) for x in range(1, len(self.grid.matrix) + 1) for y in range(1, len(self.grid.matrix[0]) + 1)]\n        actions = [\"N\", \"E\", \"S\", \"W\"]\n\n        super().__init__(states, actions, gamma)\n\n    def T(self, s1, a, s2):\n        if s1 in self.terminal_states:\n            return 0\n\n        transitions = {\n            \"N\": (0, 1),\n            \"E\": (1, 0),\n            \"S\": (0, -1),\n            \"W\": (-1, 0),\n        }\n        x, y = s1\n        x2, y2 = s2\n        dx, dy = transitions[a]  # Deterministic actions assumed\n        return int((x + dx, y + dy) == s2)\n\n    def R(self, s1, a, s2):\n        return self.grid.get_val(s2)\n\nIn this notebook, we’ll experiment with a simple 4x4 grid. It has two rewards and ends as soon as the agent collects one of them.\n\n\n\nFigure 1: Visualization of the MdpGrid4x4 world\n\n\n\nclass MdpGrid4x4(MdpGrid):\n    def __init__(self, gamma):\n        matrix = [\n            [0, 0, 0, 1],\n            [0, 0, 0, -1],\n            [0, 0, 0, 0],\n            [0, 0, 0, 0],\n        ]\n        grid = Grid(matrix)\n        super().__init__(grid, [(4, 4), (4, 3)], gamma)"
  },
  {
    "objectID": "posts/dynamic-programming-1/index.html#value-iteration",
    "href": "posts/dynamic-programming-1/index.html#value-iteration",
    "title": "Value & Policy Iteration",
    "section": "2 Value Iteration",
    "text": "2 Value Iteration\n\\[\nV_{k+1}(s) \\leftarrow \\max_a \\sum_{s'} T(s, a, s') \\left[ R(s, a, s') + \\gamma V_k(s') \\right]\n\\]\n\nclass ValueIteration():\n    def __init__(self, mdp):\n        self.mdp = mdp\n\n    def V(self, s, k):\n        if k &lt;= 0: return 0\n\n        values = []\n\n        for s2 in self.mdp.states:\n            for a in self.mdp.actions:\n                # Calculate components\n                t = self.mdp.T(s, a, s2)\n                r = self.mdp.R(s, a, s2)\n                v2 = self.V(s2, k - 1) if t &gt; 0 else 0\n\n                # Insert components into formula\n                v = t * (r + self.mdp.gamma * v2)\n\n                # Save to results\n                values.append(v)\n\n        return max(values)\n\n    def run(self, k):\n        shape = self.mdp.grid.matrix.shape\n        values = Grid(np.zeros(shape))\n\n        for s in self.mdp.states:\n            values.set_val(s, np.around(self.V(s,k), 2))\n\n        return values"
  },
  {
    "objectID": "posts/dynamic-programming-1/index.html#policy-iteration",
    "href": "posts/dynamic-programming-1/index.html#policy-iteration",
    "title": "Value & Policy Iteration",
    "section": "3 Policy Iteration",
    "text": "3 Policy Iteration\n\\[\nV^{\\pi_{i}}_{k+1}(s) \\leftarrow \\sum_{s'} T(s, \\pi_{i}(s), s') \\left[ R(s, \\pi_{i}(s), s') + \\gamma V^{\\pi_{i}}_{k}(s') \\right]\n\\]\n\nclass PolicyIteration():\n    def __init__(self, mdp):\n        self.mdp = mdp\n\n    def policy_evaluation(self, s, pi, k):\n        if k &lt;= 0:\n            return 0\n        value_sum = 0\n        for s2 in self.mdp.states:\n            a = pi.get_val(s)\n            t = self.mdp.T(s, a, s2)\n            r = self.mdp.R(s, a, s2)\n            v2 = self.policy_evaluation(s2, pi, k - 1) if t &gt; 0 else 0\n            value_sum += t * (r + self.mdp.gamma * v2)\n        return value_sum\n\n    def policy_improvement(self, pi, eval_k):\n        new_pi = Grid(pi.matrix.copy())\n        for s in self.mdp.states:\n            action_values = []\n            for a in self.mdp.actions:\n                pi_copy = Grid(new_pi.matrix.copy())\n                pi_copy.set_val(s, a)\n                v = self.policy_evaluation(s, pi_copy, eval_k)\n                action_values.append((v, a))\n\n            best_action = max(action_values, key=lambda x: x[0])[1]\n            new_pi.set_val(s, best_action)\n\n        return new_pi\n\n    def run(self, pi, policy_iterations, value_iterations):\n        iter_pi = pi\n        for i in range(policy_iterations):\n            new_pi = self.policy_improvement(iter_pi, value_iterations)\n            if new_pi.equals(iter_pi):\n                break\n            iter_pi = new_pi\n        return iter_pi"
  },
  {
    "objectID": "posts/dynamic-programming-1/index.html#examples",
    "href": "posts/dynamic-programming-1/index.html#examples",
    "title": "Value & Policy Iteration",
    "section": "4 Examples",
    "text": "4 Examples\nLet’s initialize a new grid world and run policy and value iteration on both of them to see the results.\n\nmdp_4x4 = MdpGrid4x4(0.95)\n\n\nvalue_iter = ValueIteration(mdp_4x4)\n\nresulting_values = value_iter.run(5)\nresulting_values.display()\n\n[[0.9  0.95 1.   0.  ]\n [0.86 0.9  0.95 0.  ]\n [0.81 0.86 0.9  0.86]\n [0.   0.81 0.86 0.81]]\n\n\n\npolicy_iter = PolicyIteration(mdp_4x4)\n\nmy_pi_matrix = [[\"N\" for _ in range(4)] for _ in range(4)]\nmy_pi = Grid(my_pi_matrix)\n\nresulting_policy = policy_iter.run(my_pi, 15, 10)\nresulting_policy.display()\n\n[['E' 'E' 'E' 'N']\n ['N' 'N' 'N' 'N']\n ['N' 'N' 'N' 'W']\n ['N' 'N' 'N' 'N']]"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "On this blog I’m sharing project implementations and guides on topics related to machine learning."
  }
]