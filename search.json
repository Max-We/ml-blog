[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Uninformed Search Algorithms\n\n\n\n\n\nIntroduction to search algorithms, that are fundamental for understanding artificial intelligence.\n\n\n\n\n\n\nSep 23, 2023\n\n\nMaximilian Weichart\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to RNNs\n\n\n\n\n\nRecurrent Neural Networks are a fundamental concept to understand and offer strengths that uni-directional neural networks lack. How can we use one to predict stock prices?\n\n\n\n\n\n\nAug 12, 2023\n\n\nMaximilian Weichart\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/uninformed-search/index.html",
    "href": "posts/uninformed-search/index.html",
    "title": "Uninformed Search Algorithms",
    "section": "",
    "text": "Search is a very broad term in artificial intelligence. What seems so intuitive to us as humans, such as finding an efficient route from city A to city B when looking at a map, is not so straightforward when we want a computer to do the same thing.\nAt very first glance, a simple thing such as search may seem irrelevant when we want to create artificial intelligence. It may seem boring, over-simplistic or useless to solve this problem, but it turns out that most of what’s considered artificial intelligence today is per definition just a search problem and in fact, it is solved by one of these simple search algorithms - gradient descent - which is just as simple as any of its relatives, be it from the informed or uninformed, global or local category.\n\n\n\n\n\n\nNote\n\n\n\nThis article is based on the chapter about uninformed search in “Artificial Intelligence: A Modern Approach, 4th Edition” by Stuart Russell and Peter Norvig.\n\n\n\n\nThe first important distinction to make for understanding search is to differentiate between informed and uninformed search. You can think of the former, like searching for your phone in your living room when you have no idea where you left it. The latter is like searching for it while giving it a call, so you hear the general direction where it might be.\nThere are different kinds of uninformed search algorithms, but the ones we’ll be focusing on in this article are Depth-First, Breadth-First and Uniform-Cost search. Each section will briefly introduce the concept and follow up with a concise Python implementation that you can copy and play around with.\n\n\n\nWe’ll start by defining an example scenario for our search. A common search problem is finding a path to a goal state, for example, you may wonder how to find the quickest way from your home to work.\n\n\nGraph initialization\nnodes = ['A', 'B', 'C', 'D', 'E', 'F']\nedges = [\n    ('A', 'B', 2),\n    ('A', 'C', 10),\n    ('B', 'C', 3),\n    ('B', 'D', 4),\n    ('C', 'E', 2),\n    ('D', 'F', 3),\n    ('E', 'F', 2),\n    ('E', 'B', 2)\n]\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# Create an empty graph\ngraph = nx.Graph()\n\n# Add nodes to the graph\ngraph.add_nodes_from(nodes)\n\n# Add edges with associated costs\nfor edge in edges:\n    graph.add_edge(edge[0], edge[1], cost=edge[2])\n\n\n\n\nVisualization functions\nimport colorsys\n\n# Generates a color palette from fully-saturated to unsaturated with the\n# specified amount of steps.\ndef generate_color_gradient(num_steps):\n    hue = 0.4  # Neon Green\n    lightness = 0.5\n    saturation_step = 1.0 / num_steps\n\n    colors = []\n    for i in range(num_steps):\n        # Calculate the current saturation\n        saturation = (i+1) * saturation_step\n\n        # Convert HSL to RGB\n        rgb_color = colorsys.hls_to_rgb(hue, lightness, saturation)\n\n        # Convert RGB values to 8-bit integers\n        rgb_color = tuple(int(value * 255) for value in rgb_color)\n\n        colors.append(rgb_color)\n\n    colors.reverse() # Saturated -&gt; Unsaturated\n    node_colors = [(r/255, g/255, b/255) for r, g, b in colors] # Conversion for networkx\n\n    return node_colors\n\n# Visualizes a graph and tints all visited nodes with a gradient (earliest to latest)\ndef visualize(graph, visited_nodes=[], start_node=\"A\", end_node=\"F\"):\n    pos = nx.spring_layout(graph, seed=42)  # or nx.circular_layout(graph)\n\n    labels = {edge[:2]: edge[2] for edge in edges}  # Dictionary for edge labels\n    color_array = generate_color_gradient(len(visited_nodes)) if visited_nodes else []\n\n    node_colors = []\n    for node in graph.nodes():\n      if node in visited_nodes:\n        # Tint visited nodes from earliest to latest visit\n        node_colors.append(color_array[visited_nodes.index(node)])\n      elif node == start_node or node == end_node:\n        # If there are no visited nodes, mark start and goal with main colors\n        node_colors.append(generate_color_gradient(1)[0])\n      else:\n        # Default color for nodes\n        node_colors.append('silver')\n\n    nx.draw(graph, pos, with_labels=True, node_size=500, node_color=node_colors, font_size=10, font_color='black')\n    nx.draw_networkx_edge_labels(graph, pos, edge_labels=labels)\n\n    plt.axis('off')\n    plt.show()\n\ndef evaluate(algorithm, graph):\n  visited = algorithm(graph)\n  visualize(graph, visited)\n\n\n\n\n\n\n\nFigure 1: Graph of locations A-F, with green locations (A,F) being the start- and goal-states. The edges represent the cost of transitioning from one state to another."
  },
  {
    "objectID": "posts/uninformed-search/index.html#informed-vs-uninformed",
    "href": "posts/uninformed-search/index.html#informed-vs-uninformed",
    "title": "Uninformed Search Algorithms",
    "section": "",
    "text": "The first important distinction to make for understanding search is to differentiate between informed and uninformed search. You can think of the former, like searching for your phone in your living room when you have no idea where you left it. The latter is like searching for it while giving it a call, so you hear the general direction where it might be.\nThere are different kinds of uninformed search algorithms, but the ones we’ll be focusing on in this article are Depth-First, Breadth-First and Uniform-Cost search. Each section will briefly introduce the concept and follow up with a concise Python implementation that you can copy and play around with."
  },
  {
    "objectID": "posts/uninformed-search/index.html#setup",
    "href": "posts/uninformed-search/index.html#setup",
    "title": "Uninformed Search Algorithms",
    "section": "",
    "text": "We’ll start by defining an example scenario for our search. A common search problem is finding a path to a goal state, for example, you may wonder how to find the quickest way from your home to work.\n\n\nGraph initialization\nnodes = ['A', 'B', 'C', 'D', 'E', 'F']\nedges = [\n    ('A', 'B', 2),\n    ('A', 'C', 10),\n    ('B', 'C', 3),\n    ('B', 'D', 4),\n    ('C', 'E', 2),\n    ('D', 'F', 3),\n    ('E', 'F', 2),\n    ('E', 'B', 2)\n]\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# Create an empty graph\ngraph = nx.Graph()\n\n# Add nodes to the graph\ngraph.add_nodes_from(nodes)\n\n# Add edges with associated costs\nfor edge in edges:\n    graph.add_edge(edge[0], edge[1], cost=edge[2])\n\n\n\n\nVisualization functions\nimport colorsys\n\n# Generates a color palette from fully-saturated to unsaturated with the\n# specified amount of steps.\ndef generate_color_gradient(num_steps):\n    hue = 0.4  # Neon Green\n    lightness = 0.5\n    saturation_step = 1.0 / num_steps\n\n    colors = []\n    for i in range(num_steps):\n        # Calculate the current saturation\n        saturation = (i+1) * saturation_step\n\n        # Convert HSL to RGB\n        rgb_color = colorsys.hls_to_rgb(hue, lightness, saturation)\n\n        # Convert RGB values to 8-bit integers\n        rgb_color = tuple(int(value * 255) for value in rgb_color)\n\n        colors.append(rgb_color)\n\n    colors.reverse() # Saturated -&gt; Unsaturated\n    node_colors = [(r/255, g/255, b/255) for r, g, b in colors] # Conversion for networkx\n\n    return node_colors\n\n# Visualizes a graph and tints all visited nodes with a gradient (earliest to latest)\ndef visualize(graph, visited_nodes=[], start_node=\"A\", end_node=\"F\"):\n    pos = nx.spring_layout(graph, seed=42)  # or nx.circular_layout(graph)\n\n    labels = {edge[:2]: edge[2] for edge in edges}  # Dictionary for edge labels\n    color_array = generate_color_gradient(len(visited_nodes)) if visited_nodes else []\n\n    node_colors = []\n    for node in graph.nodes():\n      if node in visited_nodes:\n        # Tint visited nodes from earliest to latest visit\n        node_colors.append(color_array[visited_nodes.index(node)])\n      elif node == start_node or node == end_node:\n        # If there are no visited nodes, mark start and goal with main colors\n        node_colors.append(generate_color_gradient(1)[0])\n      else:\n        # Default color for nodes\n        node_colors.append('silver')\n\n    nx.draw(graph, pos, with_labels=True, node_size=500, node_color=node_colors, font_size=10, font_color='black')\n    nx.draw_networkx_edge_labels(graph, pos, edge_labels=labels)\n\n    plt.axis('off')\n    plt.show()\n\ndef evaluate(algorithm, graph):\n  visited = algorithm(graph)\n  visualize(graph, visited)\n\n\n\n\n\n\n\nFigure 1: Graph of locations A-F, with green locations (A,F) being the start- and goal-states. The edges represent the cost of transitioning from one state to another."
  },
  {
    "objectID": "posts/rnn-1/index.html",
    "href": "posts/rnn-1/index.html",
    "title": "Introduction to RNNs",
    "section": "",
    "text": "Imagine how cool it would be if you could see the future. Or, at least, how stock prices develop in the next week. You would be rich very, very fast. But instead of doing all the work for yourself, how about developing a model that does the predictions for us? If we could somehow figure out how to build such a model, we’d never have to worry about money again. So let’s do it!\n\n\n\n\n\n\nNote\n\n\n\nThis notebook is based on Recurrent Neural Networks (RNNs), Clearly Explained!!! by StatQuest:\n\n\n\n\n\nLet’s take a look at some imaginary stock courses to get an idea of the data we’re working with.\n\n\n\n\n\nFigure 1: Examples for stock price changes (values are chosen purely arbitrary)\n\n\n\n\nIn this simplified example, we can see the stock of two companies change throughout the years. So how could we use a model to predict these changes?\n\n\n\nWhen we take a closer look at this kind of data, we can notice a couple of challenges that we have to solve.\n\n\nThe amount of data points for each stock can vary. In this example, there are 4 values for Googles, but only 3 values for OpenAIs stock. If you’ve worked with neural networks before, you know that this problem is not straightforward to solve. Usually, models expect a fixed number of inputs and produce a fixed number of outputs. However, our use case requires the model to work with different amounts of input data!\n\n\n\nThe values of our input data don’t necessarily form a straight line. In the example of OpenAI, the value decreases, then increases again. Because of this complexity, we won’t get very far with simple statistics like taking the mean or doing linear regression.\nSo how could we solve these problems?\n\n\n\n\n\n\nNote\n\n\n\nTake a moment and just brainstorm a couple of solutions. They don’t have to be perfect, but just ask yourself: How could you solve these two problems?"
  },
  {
    "objectID": "posts/rnn-1/index.html#stock-data",
    "href": "posts/rnn-1/index.html#stock-data",
    "title": "Introduction to RNNs",
    "section": "",
    "text": "Let’s take a look at some imaginary stock courses to get an idea of the data we’re working with.\n\n\n\n\n\nFigure 1: Examples for stock price changes (values are chosen purely arbitrary)\n\n\n\n\nIn this simplified example, we can see the stock of two companies change throughout the years. So how could we use a model to predict these changes?"
  },
  {
    "objectID": "posts/rnn-1/index.html#challenges",
    "href": "posts/rnn-1/index.html#challenges",
    "title": "Introduction to RNNs",
    "section": "",
    "text": "When we take a closer look at this kind of data, we can notice a couple of challenges that we have to solve.\n\n\nThe amount of data points for each stock can vary. In this example, there are 4 values for Googles, but only 3 values for OpenAIs stock. If you’ve worked with neural networks before, you know that this problem is not straightforward to solve. Usually, models expect a fixed number of inputs and produce a fixed number of outputs. However, our use case requires the model to work with different amounts of input data!\n\n\n\nThe values of our input data don’t necessarily form a straight line. In the example of OpenAI, the value decreases, then increases again. Because of this complexity, we won’t get very far with simple statistics like taking the mean or doing linear regression.\nSo how could we solve these problems?\n\n\n\n\n\n\nNote\n\n\n\nTake a moment and just brainstorm a couple of solutions. They don’t have to be perfect, but just ask yourself: How could you solve these two problems?"
  },
  {
    "objectID": "posts/rnn-1/index.html#setup",
    "href": "posts/rnn-1/index.html#setup",
    "title": "Introduction to RNNs",
    "section": "2.1 Setup",
    "text": "2.1 Setup\nFirst, we’ll introduce a couple of example stock data with more or less simple forms to work with:\n\nThree simple stocks, representing rising, falling and constant values\nA more complicated stock, which falls and rises\n\n\nstock_rising = [0, 0.5] # expected continuation: 1\nstock_constant = [0.5, 0.5] # expected continuation: 0.5\nstock_falling = [1, 0.5] # expected continuation: 0\nstock_curve = [1, 0.5, 0.5] # expected continuation: 1\n\n\n\n\n\n\n\nNote\n\n\n\nThe values in our dataset are normalized to 0 - 1.\n\n\nLet’s visualize the values, so we get a feeling of what’s going on.\n\n\nVisualization function\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_data(X, Y, title, dotted=True):\n    num_plots = len(X)\n    max_len = max([len(x) for x in X]) + 1\n    fig, axis = plt.subplots(1, num_plots, figsize=(max_len * num_plots, max_len))\n    if not isinstance(axis, np.ndarray):\n        axis = np.array([axis])\n\n    # Set aspect ratio and y-axis limits for all subplots\n    for ax in axis:\n        ax.set_aspect('auto', adjustable='box')\n        ax.set_ylim(0, 1)\n        ax.set_xlim(0, max_len)\n\n    # Input values\n    for i, x in enumerate(X):\n        space = np.linspace(0, max_len, len(x) + 1)\n        axis[i].plot(space, x + [np.nan], label=\"x\")\n\n    # Values to be predicted by the RNN\n    for i, y in enumerate(Y):\n        # rescale x-axis values\n        continuation_point = (len(X[i])-1) / len(X[i]) * max_len\n        axis[i].plot([continuation_point, max_len], [X[i][-1], y], linestyle='dotted' if dotted else 'solid', label=\"y\")\n\n    plt.suptitle(title)\n    plt.show()\n\n\n\n\n\n\n\nFigure 2: Input stock data arrays and the expected continuation"
  },
  {
    "objectID": "posts/rnn-1/index.html#rnn-1",
    "href": "posts/rnn-1/index.html#rnn-1",
    "title": "Introduction to RNNs",
    "section": "2.2 RNN",
    "text": "2.2 RNN\n\n2.2.1 Architecture\nGiven a sequence of input values \\(x\\), predicting an outcome \\(y\\) is not a problem. If you’ve worked with neural networks before, you’ve done it a thousand times. You define a sequence of layers and activation functions (to keep it simple we’ll use linear layers and relu only), as well as the number of inputs and outputs together with the correct weights, the model will solve the problem. The problem of the stock courses having a complex form1 can be easily solved with a neural network with enough layers.1 see Section 1.2.2\n\n\n\nFigure 3: Schema of a feed forward neural network with four inputs and one output\n\n\nThe problem that remains even in a classic feed forward neural network is, that it only works with a fixed number of inputs2. To make the model more flexible, RNNs use a “feedback loop”, which solves exactly that problem.2 see Section 1.2.1\nThink about how you make a prediction of one of these stock courses. If you’re like me, you will read the line from the beginning to the end and build up an overall feeling of the development. In the example of the Google stock, you could think: It starts out at $400, but it’s decreasing… and again, it’s decreasing - and so on. RNNs do a similar thing, while a normal feed-forward neural network looks at all the data points at the same time and comes to a conclusion all at once.\nImagine a more complex scenario: A stock with 10,000 values over multiple years. A neural network with a fixed input size would look at all the values at once, as if it had 10,000 eyes, and calculate the prediction. Intuitively, an RNN is much more like a human. It looks at each data point sequentially (“feedback loop”) and builds up an overall opinion (“hidden state”, \\(h\\)) of the stock, until it’s reached the last data point, at which it will output its prediction.\n\n\n\nFigure 4: Unrolled schema of an RNN that handles four inputs\n\n\nAs you can see, the RNN does the same thing over and over again, until it’s looked at all data points. This is why it can be summarized to be more concise, in a recursive version, which gives us the final architecture of an RNN:\n\n\n\nFigure 5: Concise schema of an RNN that handles four inputs. The input values are being passed sequentially, one by one)\n\n\n\n\n2.2.2 Implementation\nNow that we know how the architecture of an RNN looks like, let’s implement it in Python. Essentially, we want to implement the feedback loop, which consists of a linear layer and an activation function, and the output layer, which is another linear layer\n\ndef lin(x,w,b=0):\n    return x*w+b\n\ndef relu(x):\n    return max(x,0)\n\nclass MiniRnn():\n  w1 = 1.4\n  w2 = -0.5\n  w3 = 1.4\n    \n  def forward(self, X, i=0, h=0):\n1    l1 = lin(X[i], self.w1)\n      \n2    h = relu(l1 + h*self.w2)\n3    if(i+1 != len(X)):\n        return self.forward(X, i+1, h)\n    \n    # Output\n4    l2 = lin(h, self.w3)\n    return round(l2, 1)\n\nrnn = MiniRnn()\n\n\n1\n\nInput layer\n\n2\n\nHidden state & activation function (relu)\n\n3\n\nFeedback loop\n\n4\n\nOutput layer\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe weights w1, w2, w3 have been calculated using gradient descent. In the next part of this series, we will learn how to do that ourselves - for now we’ll just use them as they are.\n\n\nLet’s think about how the model will calculate its prediction.\n\nThe RNN iterates over all values in the input sequence (X) and starts with no memory (“hidden state”) at all because h=0 in the first iteration.\nAfter looking at a new value, it saves all its “thoughts” in the hidden state h, until it iterated over all elements.\nOnce it has looked at all values (and only then because this is the only return statement that terminates the recursion) the model forms its conclusion and returns the prediction\n\nIf you’re interested in a more formal definition of the RNN, check out part 2 of this series, where I will introduce some of the math related to RNNs.\n\n\n\n\n\n\nImportant\n\n\n\nIf you’ve watched the StatQuest video on RNNs, you may notice that relu activation function is being applied before its being multiplies with w2. If we followed the video, our implementation would be: h = max(l1 + h, 0) * w2. However, the implementation from the video is wrong, it is not how RNNs are defined.\nFor now, this is not too important, but we’ll see in part 2 why this makes a huge difference.\n\n\n\n\n2.2.3 Testing\nLet’s validate the model by looking at the outputs:\n\nr1 = rnn.forward(stock_rising)\nr2 = rnn.forward(stock_constant)\nr3 = rnn.forward(stock_falling)\nr4 = rnn.forward(stock_curve)\nr1,r2,r3,r4\n\n(1.0, 0.5, 0.0, 1.0)\n\n\n\n\n\n\n\nFigure 6: Input stock data arrays with the model prediction\n\n\n\n\nThe RNN can predict the next values for all the stocks, nice!\nTo understand each step in detail, here are the steps our RNN took in forward-pass for stock_falling.\n\nstock_falling\n\n[1, 0.5]\n\n\n\nl1 = relu(1 * 1.4 + 0 * -0.5)\nl2 = relu(0.5 * 1.4 + l1 * -0.5)\nl3 = 1.4 * l2\nprint(f\"Layers: l1: {l1} l2: {l2} l3: {l3}\\nResult: {l3}\")\n\nLayers: l1: 1.4 l2: 0.0 l3: 0.0\nResult: 0.0\n\n\nAs you can see, each layer uses the output from the previous layer and the same three weights are being shared across all calculations. Even if we passed in a sequence of 100 values, we’d only be using w1,w2 and w3 in the model.\n\n\n2.2.4 Validating\nUntil this point, I only used the examples from the StatQuest video. However, these are not the only values we should try out. What about other forms? It could also be that the model just learned to memorize the four input arrays and maps a single value onto each of them.\nTo validate if our RNN can actually generalize, let’s change the input data. We’ll still use values which are normalized to 0 - 1, but we’ll change their amplitude.\n\nsmall_rising = [0,0.1]\nsmall_constant = [0.1,0.1]\nsmall_falling = [1,0.9]\nsmall_curve = [0.2,00.1,0.1]\n\n\n\n\n\n\nFigure 7: Input validation data arrays with the expected continuation\n\n\n\n\nOur validation data still represents the same shapes, but with different values. This should make it harder for a model that just “fakes” the output by remembering the training data. Let’s see what our RNN does with it.\n\ns1 = rnn.forward(small_rising)\ns2 = rnn.forward(small_constant)\ns3 = rnn.forward(small_falling)\ns4 = rnn.forward(small_curve)\ns1,s2,s3,s4\n\n(0.2, 0.1, 0.8, 0.2)\n\n\n\n\n\n\n\nFigure 8: Input validation data arrays with the model prediction\n\n\n\n\nThe model can correctly predict all of our validation data! This means that it can generalize and find some basic patterns in the data and is not just remembering our training dataset. This is quite an achievement. With these few lines of code, we’re able to create a model that can predict any kind of rising / constant / falling sequence of values, without changing any parameter. Imagine how far this approach can go when we scale up the model and increase its complexity!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "On this blog I’m sharing project implementations and guides on topics related to machine learning."
  }
]