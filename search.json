[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Simulated Annealing\n\n\n\n\n\nHow can we make Entropy work for - and not against us.\n\n\n\n\n\n\nNov 25, 2023\n\n\nMaximilian Weichart\n\n\n\n\n\n\n  \n\n\n\n\nUninformed Search Algorithms\n\n\n\n\n\nIntroduction to search algorithms - a fundamental for understanding artificial intelligence.\n\n\n\n\n\n\nSep 23, 2023\n\n\nMaximilian Weichart\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to RNNs\n\n\n\n\n\nRecurrent Neural Networks are a fundamental concept to understand and offer strengths that uni-directional neural networks lack. How can we use one to predict stock prices?\n\n\n\n\n\n\nAug 12, 2023\n\n\nMaximilian Weichart\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/simulated-annealing/index.html",
    "href": "posts/simulated-annealing/index.html",
    "title": "Simulated Annealing",
    "section": "",
    "text": "Life is full of solving problems. We are constantly searching for solutions, and in Artificial Intelligence there’s a whole subfield about search algorithms. In this article, I want to introduce the algorithm which fascinates me the most."
  },
  {
    "objectID": "posts/simulated-annealing/index.html#the-problem",
    "href": "posts/simulated-annealing/index.html#the-problem",
    "title": "Simulated Annealing",
    "section": "1 The problem",
    "text": "1 The problem\nSearch Problems are difficult to solve. But why is that? Essentially, it is because the things we hope to find are a lot less common than the things we do not want to find. Take, for example, a Rubik’s cube: It has 43 quintillion configurations, out of only 1 is the correct one. If we were to approach this problem without a strategy, the chances are 1:43 quintillion of finding the solution every time we make a turn. Entropy can be defined as the number of states that a system can have1. So in the example of the Rubik’s cube, it is low when we only consider a “solved” Rubik’s cube, but high when we consider an “unsolved” Rubik’s cube. Together with the fact that over time, entropy tends to a maximum2, it’s the reason that making random turns on the Rubik’s cube most likely will leave it in a worse state.1 this definition is simplistic and applies more specifically to the context of statistical mechanics2 The second law of thermodynamics\n\n\n\nSimplified overview over the problem: There are many “bad” states for the Rubik’s cube and only one “solved” state (red). The chances of picking it are really low.\n\n\nIt almost seems like most of our real-world problems exist because we want order (low entropy) while the world tends to move toward chaos (high entropy). If we could reverse these laws, even if it’s just for a moment, we could solve so many problems without any effort. If you had a button to inverse Entropy in our example, you could solve the Rubik’s cube by throwing it down the stairs.\nSimulated annealing is a search algorithm that was inspired by the annealing process in physics, which in fact leverages Entropy to solve problems. But how does it do it? Increasing Entropy, by its very definition, is just a consequence of probabilities. So to understand the magic, we have to look at the world from a statistical perspective because if we can change the probabilities, we can make Entropy work for - and not against us."
  },
  {
    "objectID": "posts/simulated-annealing/index.html#setup",
    "href": "posts/simulated-annealing/index.html#setup",
    "title": "Simulated Annealing",
    "section": "2 Setup",
    "text": "2 Setup\nBefore we try to understand how simulated annealing works, I want to simplify the problem and the tools we use to make it more approachable. When working with informed search algorithms like simulated annealing, you are working with the algorithm itself, which is a function, and also a heuristic (or cost) function. The cost function tells you how far away from your goal you are - not more, not less. It doesn’t say how to get closer, just how bad the current state is. And because it says how close we are to the goal, minimizing it is our main goal.\nBecause the Rubik’s cube is such a complicated example, let’s choose a simpler problem for now. Let’s just assume that our cost function3 is defined as follows:3 This function has been randomly chosen and includes a local and global minimum\n\\[f(x) = x^{2} + 10 * sin(x)+ 15\\]\n\ndef f(x):\n    return x**2 + 10 * np.sin(x) + 15\n\nFrom now on, we want to minimize the output of this cost function and therefore find a solution for \\(x\\) that makes \\(f(x)\\) as small as possible, ideally \\(0\\). We can plot how our cost function looks like:\n\n\n\n\n\nFigure 1: \\(f(x) = x^{2} + 10 * sin(x)+ 15\\) (plotted from -10 to 10)"
  },
  {
    "objectID": "posts/simulated-annealing/index.html#building-a-strategy",
    "href": "posts/simulated-annealing/index.html#building-a-strategy",
    "title": "Simulated Annealing",
    "section": "3 Building a strategy",
    "text": "3 Building a strategy\n\n3.1 Random\nTo solve this problem and to understand how simulated annealing does it so well, we should look at it from a statistical perspective. Perhaps the simplest approach to finding a solution is to pick a value randomly. How high would the probability of finding the global minimum at around -1.5 be? We can visualize this strategy using a probability density function (PDF).\n\ndef algorithm_a(f, iterations):\n  x_values = []\n  y_values = []\n  for t in range(iterations):\n1    x_values.append(random.uniform(-20, 20))\n    y_values.append(f(x_values[t]))\n\n  return x_values, y_values\n\n\n1\n\nAt each timestep we just select a random \\(x\\) for our function\n\n\n\n\n\n\n\n\n\nFigure 2: Probability density function for the ‘random’ strategy\n\n\n\n\nAs you can see, all values of x are equally likely to be picked as the solution, which is of course wrong. Ideally, we want our algorithm to find the solution more often than the wrong answers.\n\n\n3.2 Preference\nTo find the ideal solution more reliably, we could define our algorithm in a way, that it’s more likely that a cost-improvement will be accepted rather than a regression. The simplest way would be to accept worse solutions only sometimes, say 50% of the time:\n\ndef algorithm_b(f, iterations, initial_x):\n  x_values = [initial_x]\n  y_values = [f(initial_x)]\n\n  for t in range(iterations):\n    current_x, current_y = x_values[-1], y_values[-1]\n    new_x = random.uniform(-20, 20)\n    new_y = f(new_x)\n1    if new_y &lt; current_y or random.random() &lt; 0.50:\n      x_values.append(new_x)\n      y_values.append(new_y)\n\n  return x_values, y_values\n\n\n1\n\nOnly accept “worse” solutions 50% of the time\n\n\n\n\n\n\n\n\n\nFigure 3: Probability density function for the ‘preference’ strategy\n\n\n\n\nIt looks like this strategy helped the algorithm to identify the global minimum more reliably, but it’s still yielding many wrong results.\n\n\n3.3 Neighbors\nThe current implementation is inefficient because the progress we made at identifying the best solution will be reset on every time step. Imagine trying to solve a Rubik’s cube, but instead of slowly building up your solution, you choose an entirely new configuration every time you make a turn, instead of improving the current one. So instead of choosing an entirely new solution every time we make a move, let’s only consider neighboring solutions, that is solutions within a certain interval of the current one. Only considering neighboring states is one important concept that simulated annealing employs.\n\ndef algorithm_c(f, iterations, initial_x, p):\n  x_values = [initial_x]\n  y_values = [f(initial_x)]\n\n  for t in range(iterations):\n    current_x, current_y = x_values[-1], y_values[-1]\n1    new_x = current_x + random.uniform(-1, 1)\n    new_y = f(new_x)\n    if new_y &lt; current_y or random.random() &lt; p:\n      x_values.append(new_x)\n      y_values.append(new_y)\n\n  return x_values, y_values\n\n\n1\n\nOnly consider neighboring values of the current states by reducing the interval that we sample from to \\([-1, 1]\\), which are the neighboring states only. If we were to sample from \\([-20, 20]\\) like before, we’d lose our progress at every step.\n\n\n\n\n\n\n\n\n\nFigure 4: Probability density function for the ‘neighbour’ approach\n\n\n\n\nAs you can see, the probability that the algorithm yields the correct solution is a lot higher than for any other value. But it’s still not guaranteed, and a lot of the time it returns wrong answers. The variance is still too high. Ideally, we’d always want the highest point of the probability density function to be returned as our solution and ignore all the other values. However, we can’t just set the probability of accepting a worse state to 0 because this would lead to the local optimum quite often. Once we’re in the local optimum and don’t accept “worse” states anymore, there’d be no way out. So we have to find another approach to lead us to the global optimum.\nHow can we solve this? Instead of immediately setting the acceptance probability for worse states to 0%, we could start at 100% and gradually decrease it. Every time we decrease the probability, we become a little more deterministic and in that way we reject bad states more often gradually: At first, we reject the ridiculous states, like configurations of a Rubik’s cube that are completely mixed. After that, we lower the threshold of good states even further, so of all the not-so-bad states, we accept only the better ones.\nWe can imagine this process like filtering out the best solution gradually. The state space for the Rubik’s cube could be represented in the following image. At a high acceptance-probability, we do the rough work, filtering out all the terrible configurations, resulting in a better subset of configurations. After that, we refine our search, filtering out the even better states from our previous subset. We refine this process even further until we arrive at a 0% acceptance probability for worse states, at which point we just pick the perfect fit.\n\n\n\nFiltering step by step. Each rectangle represents a step in the filtering process which is getting refined gradually. The filtering continues until only the solution state remains.\n\n\nThis process works like a sieve: at each step we increase the quality and build upon our previous work. If we were to keep the acceptance-probability low, we’d have countless hit-or-miss results because we focus in on one particular configuration, before filtering out all the bad states. If we kept it fixed at a high value, we’d never arrive at an excellent solution because we’d never “filter” out the bad states.\nFrom now on, we’ll call this probability of accepting worse states “Temperature” because that’s the term used in the simulated annealing algorithm4. In our code, it’s written as t:4 Later we’ll expand this definition, but for now, it’s just called the Temperature.\n\ndef algorithm_d(f, iterations, initial_x):\n    x_values = [initial_x]\n    y_values = [f(initial_x)]\n1    t_values = [1]\n\n    for k in range(iterations):\n        current_x, current_y = x_values[-1], y_values[-1]\n        new_x = current_x + random.uniform(-1, 1)\n        new_y = f(new_x)\n2        if new_y &lt; current_y or random.random() &lt; t_values[-1]:\n            x_values.append(new_x)\n            y_values.append(new_y)\n\n3        t_values.append(1 - k / iterations)\n\n    return x_values, y_values, t_values\n\n\n1\n\nWe start with a temperature of 1 (100% acceptance probability)\n\n2\n\nThis change means the same as in our previous algorithms. Because we save the temperatures in an array (for visualization), we want to consider the latest entry.\n\n3\n\nWe decrease the temperature linearly\n\n\n\n\nThe results of this algorithm look as follows:\n\n\n\n\n\nFigure 5: Probability density function for the ‘falling-temperature’ approach\n\n\n\n\nWe can compare this to the solutions for different temperatures from algorithm_c in Section 3.3 with different fixed temperatures:\n\n\n\n\n\nFigure 6: Probability density function for the results of the ‘neighbors’ approach with different fixed temperatures\n\n\n\n\nLooking at this new PDF, we can notice a couple of things:\n\nThe results in Figure 5 are more reliable than the PDF with a fixed low temperature (0.01)\nThe results in Figure 5 are more concrete than the PDF with fixed high temperature (0.9)\n\nOur results are more reliable because we consider a wider array of possibilities than when starting with a really low acceptance probability, which would immediately focus in on a small section of the graph. Using algorithm_c with t=0.01 will lead to the global minimum quite often. But it’s also more concrete than the results we get from using a fixed high temperature. This is because once we reach a low temperature, we focus in on the details, refining our solution.\nWe can also take a look at how the results from our new algorithm develop over time. The following animation shows the PDF of the values picked by the algorithm in an interval of 10%. As expected, they start out very broad and incorrect, but over time it becomes more narrow and correct.\n\n\n\nProbability density function over time for a fractions of timesteps ‘falling-temperature-fractions’\n\n\n\n\n3.4 Quality\nAll the strategies we adapted so far lead to some good results. However, dropping the acceptance probability linearly is a simplification and is rarely the case in reality. You could think about what it’s like writing an essay: The outline probably takes less time than all the revisions and details. This is also known as the Pareto Rule (or 80-20 principle) and it shows that to be optimal, we need a more accurate approach for lowering the acceptance probability.\nTo solve this, we could come up with different temperature schedules, like a geometric decay. However, this doesn’t address the underlying problem and may only work for some cases.\nThus far, our acceptance probability was equal to the temperature when considering “worse” states. That means, that, no matter how bad the new state is, we will always accept it with a fixed probability (given by the current temperature). However, it could be quite efficient to take the quality of this worse state into consideration. For example, when solving a Rubik’s Cube, we prefer “bad” states over “terrible” states. So we should decrease the acceptance probability for states as they approach “terrible”.\nThe question becomes, how could we express this as a formula? Obviously, we want to integrate \\(\\Delta Cost\\) (the difference between y_old and y_new) into our acceptance probability, as this tells us how “bad” the proposed state is. However, \\(\\Delta Cost\\) can have any value in the range \\([0, \\inf[\\) so to normalize it as a probability, there’s a simple trick. We’ll just plug it into the function \\(e^{-x}\\).\n\n\n\n\n\nFigure 7: \\(e^{-x}\\) plotted from 0 to 5”\n\n\n\n\nThis normalizes the value of \\(\\Delta Cost\\) to \\([0, 1]\\) and \\(e^{- \\Delta Cost}\\) can therefore be used as a probability.\nThe only thing left to do it so integrate the temperature \\(T\\) into the formula. Just like \\(\\Delta Cost\\), we know that \\(T\\) can be in the range of \\([0, \\inf[\\), however in contrast to \\(\\Delta Cost\\), a high \\(T\\) should result in a high acceptance probability. To add \\(T\\) into the equation, we can divide \\(\\Delta Cost\\) in the exponent by \\(T\\). This can be expressed as the following formula:\n\\[\ne^{-\\frac{\\Delta Cost}{T}}\n\\]\nDividing by \\(T\\) effectively “weakens” the effect of \\(\\Delta C\\) when \\(T\\) is high, resulting in a high acceptance probability.\n\n3.4.1 Boltzmann distribution\nThe above explanation serves as an intuitive approach to simulated annealing. If you’re satisfied with this explanation, you can skip this section and jump right to the implementation in Section 3.4.3, if not, here’s my attempt to illustrate it with the theoretical background as well.\n\n\n\n\n\n\nNote\n\n\n\nTo be humble, I am not convinced that my understanding of the Boltzmann Distribution in simulated annealing is sufficient at this point. However, I decided to include this part in the article to encourage feedback and discussions about this concept so that I can ultimately learn and understand it better. My goal is therefore not to provide a perfect answer right now, but to improve this explanation in the future through your participation!\n\n\nLet’s think about what the acceptance probability is supposed to do: It should tell us how likely the state that we’re observing is getting us closer to the solution. And the solution in our case is the most likely outcome, as we can see in Figure 5. The thing we really want to know to be optimal is, how likely is it that our system is in the state that we’re observing? So if we know how likely a state is, then we know how good, that is how close to the solution it is.\nThe Boltzmann distribution gives us exactly that: it is a probability distribution that gives the probability that a system will be in a certain state as a function of that state’s energy and the temperature of the system5. It is defined as:5 Source: Wikipedia\n\\[\np_i \\propto e^{- \\frac{\\varepsilon_i}{kT}}\n\\]\n\n\\(\\varepsilon_i\\): Energy at a specific state \\(i\\)\n\nin our example “energy” = “cost” = y-value, or as previously defined: \\(\\Delta E = \\Delta Cost\\)\n\n\\(T\\): Temperature\n\\(k\\): Boltzmann constant, can be ignored and assumed to be \\(1\\)\n\n\n\n3.4.2 Boltzman factor\nThis is the entire distribution and ultimately, what we’re interested in. To calculate it, we use the Boltzmann factor, which is defined as:\n\\[\ne^{-\\frac{\\Delta E}{T}}\n\\]\n\n\\(\\Delta E\\): Difference in energy between two states\n\n\\(\\Delta E =\\) new_y - current_y in our code\n\n\nSo all we have to do now is plugging in the temperature and cost into the equation for the Boltzmann factor and use it as the acceptance probability when evaluating the states. This gives us the simulated annealing algorithm.\n\n\n3.4.3 Implementation\n\ndef simulated_annealing(f, iterations, initial_x):\n    x_values = [initial_x]\n    y_values = [f(initial_x)]\n    t_values = [1]\n\n    for k in range(iterations):\n        current_x, current_y = x_values[-1], y_values[-1]\n        new_x = current_x + random.uniform(-1, 1)\n        new_y = f(new_x)\n        \n        delta_e = new_y - current_y\n1        p = exp(-(delta_e)/t_values[-1])\n        \n        p_values.append(p)\n2        if delta_e &lt; 0 or random.random() &lt; p:\n            x_values.append(new_x)\n            y_values.append(new_y)\n\n        t_values.append(1 - k / iterations)\n\n    return x_values, y_values, t_values\n\n\n1\n\nThe Boltzmann factor, which is our acceptance probability for states that are worse than the current one\n\n2\n\nWe still always accept better states without asking, in the other cases we do so based on the Boltzmann factor. Together, this is called the “Metropolis acceptance criterion”\n\n\n\n\nRunning simulated annealing with our example problem yields the following result:\n\n\n\n\n\nFigure 8: Probability density function for the ‘simulated annealing’ algorithm\n\n\n\n\nThe probability of finding the solution with this approach is very high. It’s almost guaranteed now. By tuning the temperature schedule and running more iterations, these results would become even more apparent. We have therefore found a great solution to the problem."
  },
  {
    "objectID": "posts/simulated-annealing/index.html#summary",
    "href": "posts/simulated-annealing/index.html#summary",
    "title": "Simulated Annealing",
    "section": "4 Summary",
    "text": "4 Summary\nLeveraging the concepts discussed in this article, simulated annealing can be used not only to find solutions to simple problems as with our function \\(f(x)\\), but for any search problem, no matter how complex. As long as there’s a cost function and a temperature schedule, simulated annealing is guaranteed to find the optimal solution, given enough time and a temperature schedule that decreases slowly enough.\nThe concepts in this article have even more depth to them. For example, one could ask: why is the Boltzmann distribution defined the way it is? Or how could one find the optimal temperature for the algorithm? All these are questions that deserve their own time to discuss. This article should just give a well reasoned intuition of why it works at all.\nThank you, Vishal, Alex, and the rest of the FastAI MeetUp on Discord for providing feedback for this article."
  },
  {
    "objectID": "posts/rnn-1/index.html",
    "href": "posts/rnn-1/index.html",
    "title": "Introduction to RNNs",
    "section": "",
    "text": "Imagine how cool it would be if you could see the future. Or, at least, how stock prices develop in the next week. You would be rich very, very fast. But instead of doing all the work for yourself, how about developing a model that does the predictions for us? If we could somehow figure out how to build such a model, we’d never have to worry about money again. So let’s do it!\n\n\n\n\n\n\nNote\n\n\n\nThis notebook is based on Recurrent Neural Networks (RNNs), Clearly Explained!!! by StatQuest:\n\n\n\n\n\nLet’s take a look at some imaginary stock courses to get an idea of the data we’re working with.\n\n\n\n\n\nFigure 1: Examples for stock price changes (values are chosen purely arbitrary)\n\n\n\n\nIn this simplified example, we can see the stock of two companies change throughout the years. So how could we use a model to predict these changes?\n\n\n\nWhen we take a closer look at this kind of data, we can notice a couple of challenges that we have to solve.\n\n\nThe amount of data points for each stock can vary. In this example, there are 4 values for Googles, but only 3 values for OpenAIs stock. If you’ve worked with neural networks before, you know that this problem is not straightforward to solve. Usually, models expect a fixed number of inputs and produce a fixed number of outputs. However, our use case requires the model to work with different amounts of input data!\n\n\n\nThe values of our input data don’t necessarily form a straight line. In the example of OpenAI, the value decreases, then increases again. Because of this complexity, we won’t get very far with simple statistics like taking the mean or doing linear regression.\nSo how could we solve these problems?\n\n\n\n\n\n\nNote\n\n\n\nTake a moment and just brainstorm a couple of solutions. They don’t have to be perfect, but just ask yourself: How could you solve these two problems?"
  },
  {
    "objectID": "posts/rnn-1/index.html#stock-data",
    "href": "posts/rnn-1/index.html#stock-data",
    "title": "Introduction to RNNs",
    "section": "",
    "text": "Let’s take a look at some imaginary stock courses to get an idea of the data we’re working with.\n\n\n\n\n\nFigure 1: Examples for stock price changes (values are chosen purely arbitrary)\n\n\n\n\nIn this simplified example, we can see the stock of two companies change throughout the years. So how could we use a model to predict these changes?"
  },
  {
    "objectID": "posts/rnn-1/index.html#challenges",
    "href": "posts/rnn-1/index.html#challenges",
    "title": "Introduction to RNNs",
    "section": "",
    "text": "When we take a closer look at this kind of data, we can notice a couple of challenges that we have to solve.\n\n\nThe amount of data points for each stock can vary. In this example, there are 4 values for Googles, but only 3 values for OpenAIs stock. If you’ve worked with neural networks before, you know that this problem is not straightforward to solve. Usually, models expect a fixed number of inputs and produce a fixed number of outputs. However, our use case requires the model to work with different amounts of input data!\n\n\n\nThe values of our input data don’t necessarily form a straight line. In the example of OpenAI, the value decreases, then increases again. Because of this complexity, we won’t get very far with simple statistics like taking the mean or doing linear regression.\nSo how could we solve these problems?\n\n\n\n\n\n\nNote\n\n\n\nTake a moment and just brainstorm a couple of solutions. They don’t have to be perfect, but just ask yourself: How could you solve these two problems?"
  },
  {
    "objectID": "posts/rnn-1/index.html#setup",
    "href": "posts/rnn-1/index.html#setup",
    "title": "Introduction to RNNs",
    "section": "2.1 Setup",
    "text": "2.1 Setup\nFirst, we’ll introduce a couple of example stock data with more or less simple forms to work with:\n\nThree simple stocks, representing rising, falling and constant values\nA more complicated stock, which falls and rises\n\n\nstock_rising = [0, 0.5] # expected continuation: 1\nstock_constant = [0.5, 0.5] # expected continuation: 0.5\nstock_falling = [1, 0.5] # expected continuation: 0\nstock_curve = [1, 0.5, 0.5] # expected continuation: 1\n\n\n\n\n\n\n\nNote\n\n\n\nThe values in our dataset are normalized to 0 - 1.\n\n\nLet’s visualize the values, so we get a feeling of what’s going on.\n\n\nVisualization function\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_data(X, Y, title, dotted=True):\n    num_plots = len(X)\n    max_len = max([len(x) for x in X]) + 1\n    fig, axis = plt.subplots(1, num_plots, figsize=(max_len * num_plots, max_len))\n    if not isinstance(axis, np.ndarray):\n        axis = np.array([axis])\n\n    # Set aspect ratio and y-axis limits for all subplots\n    for ax in axis:\n        ax.set_aspect('auto', adjustable='box')\n        ax.set_ylim(0, 1)\n        ax.set_xlim(0, max_len)\n\n    # Input values\n    for i, x in enumerate(X):\n        space = np.linspace(0, max_len, len(x) + 1)\n        axis[i].plot(space, x + [np.nan], label=\"x\")\n\n    # Values to be predicted by the RNN\n    for i, y in enumerate(Y):\n        # rescale x-axis values\n        continuation_point = (len(X[i])-1) / len(X[i]) * max_len\n        axis[i].plot([continuation_point, max_len], [X[i][-1], y], linestyle='dotted' if dotted else 'solid', label=\"y\")\n\n    plt.suptitle(title)\n    plt.show()\n\n\n\n\n\n\n\nFigure 2: Input stock data arrays and the expected continuation"
  },
  {
    "objectID": "posts/rnn-1/index.html#rnn-1",
    "href": "posts/rnn-1/index.html#rnn-1",
    "title": "Introduction to RNNs",
    "section": "2.2 RNN",
    "text": "2.2 RNN\n\n2.2.1 Architecture\nGiven a sequence of input values \\(x\\), predicting an outcome \\(y\\) is not a problem. If you’ve worked with neural networks before, you’ve done it a thousand times. You define a sequence of layers and activation functions (to keep it simple we’ll use linear layers and relu only), as well as the number of inputs and outputs together with the correct weights, the model will solve the problem. The problem of the stock courses having a complex form1 can be easily solved with a neural network with enough layers.1 see Section 1.2.2\n\n\n\nFigure 3: Schema of a feed forward neural network with four inputs and one output\n\n\nThe problem that remains even in a classic feed forward neural network is, that it only works with a fixed number of inputs2. To make the model more flexible, RNNs use a “feedback loop”, which solves exactly that problem.2 see Section 1.2.1\nThink about how you make a prediction of one of these stock courses. If you’re like me, you will read the line from the beginning to the end and build up an overall feeling of the development. In the example of the Google stock, you could think: It starts out at $400, but it’s decreasing… and again, it’s decreasing - and so on. RNNs do a similar thing, while a normal feed-forward neural network looks at all the data points at the same time and comes to a conclusion all at once.\nImagine a more complex scenario: A stock with 10,000 values over multiple years. A neural network with a fixed input size would look at all the values at once, as if it had 10,000 eyes, and calculate the prediction. Intuitively, an RNN is much more like a human. It looks at each data point sequentially (“feedback loop”) and builds up an overall opinion (“hidden state”, \\(h\\)) of the stock, until it’s reached the last data point, at which it will output its prediction.\n\n\n\nFigure 4: Unrolled schema of an RNN that handles four inputs\n\n\nAs you can see, the RNN does the same thing over and over again, until it’s looked at all data points. This is why it can be summarized to be more concise, in a recursive version, which gives us the final architecture of an RNN:\n\n\n\nFigure 5: Concise schema of an RNN that handles four inputs. The input values are being passed sequentially, one by one)\n\n\n\n\n2.2.2 Implementation\nNow that we know how the architecture of an RNN looks like, let’s implement it in Python. Essentially, we want to implement the feedback loop, which consists of a linear layer and an activation function, and the output layer, which is another linear layer\n\ndef lin(x,w,b=0):\n    return x*w+b\n\ndef relu(x):\n    return max(x,0)\n\nclass MiniRnn():\n  w1 = 1.4\n  w2 = -0.5\n  w3 = 1.4\n    \n  def forward(self, X, i=0, h=0):\n1    l1 = lin(X[i], self.w1)\n      \n2    h = relu(l1 + h*self.w2)\n3    if(i+1 != len(X)):\n        return self.forward(X, i+1, h)\n    \n    # Output\n4    l2 = lin(h, self.w3)\n    return round(l2, 1)\n\nrnn = MiniRnn()\n\n\n1\n\nInput layer\n\n2\n\nHidden state & activation function (relu)\n\n3\n\nFeedback loop\n\n4\n\nOutput layer\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe weights w1, w2, w3 have been calculated using gradient descent. In the next part of this series, we will learn how to do that ourselves - for now we’ll just use them as they are.\n\n\nLet’s think about how the model will calculate its prediction.\n\nThe RNN iterates over all values in the input sequence (X) and starts with no memory (“hidden state”) at all because h=0 in the first iteration.\nAfter looking at a new value, it saves all its “thoughts” in the hidden state h, until it iterated over all elements.\nOnce it has looked at all values (and only then because this is the only return statement that terminates the recursion) the model forms its conclusion and returns the prediction\n\nIf you’re interested in a more formal definition of the RNN, check out part 2 of this series, where I will introduce some of the math related to RNNs.\n\n\n\n\n\n\nImportant\n\n\n\nIf you’ve watched the StatQuest video on RNNs, you may notice that relu activation function is being applied before its being multiplies with w2. If we followed the video, our implementation would be: h = max(l1 + h, 0) * w2. However, the implementation from the video is wrong, it is not how RNNs are defined.\nFor now, this is not too important, but we’ll see in part 2 why this makes a huge difference.\n\n\n\n\n2.2.3 Testing\nLet’s validate the model by looking at the outputs:\n\nr1 = rnn.forward(stock_rising)\nr2 = rnn.forward(stock_constant)\nr3 = rnn.forward(stock_falling)\nr4 = rnn.forward(stock_curve)\nr1,r2,r3,r4\n\n(1.0, 0.5, 0.0, 1.0)\n\n\n\n\n\n\n\nFigure 6: Input stock data arrays with the model prediction\n\n\n\n\nThe RNN can predict the next values for all the stocks, nice!\nTo understand each step in detail, here are the steps our RNN took in forward-pass for stock_falling.\n\nstock_falling\n\n[1, 0.5]\n\n\n\nl1 = relu(1 * 1.4 + 0 * -0.5)\nl2 = relu(0.5 * 1.4 + l1 * -0.5)\nl3 = 1.4 * l2\nprint(f\"Layers: l1: {l1} l2: {l2} l3: {l3}\\nResult: {l3}\")\n\nLayers: l1: 1.4 l2: 0.0 l3: 0.0\nResult: 0.0\n\n\nAs you can see, each layer uses the output from the previous layer and the same three weights are being shared across all calculations. Even if we passed in a sequence of 100 values, we’d only be using w1,w2 and w3 in the model.\n\n\n2.2.4 Validating\nUntil this point, I only used the examples from the StatQuest video. However, these are not the only values we should try out. What about other forms? It could also be that the model just learned to memorize the four input arrays and maps a single value onto each of them.\nTo validate if our RNN can actually generalize, let’s change the input data. We’ll still use values which are normalized to 0 - 1, but we’ll change their amplitude.\n\nsmall_rising = [0,0.1]\nsmall_constant = [0.1,0.1]\nsmall_falling = [1,0.9]\nsmall_curve = [0.2,00.1,0.1]\n\n\n\n\n\n\nFigure 7: Input validation data arrays with the expected continuation\n\n\n\n\nOur validation data still represents the same shapes, but with different values. This should make it harder for a model that just “fakes” the output by remembering the training data. Let’s see what our RNN does with it.\n\ns1 = rnn.forward(small_rising)\ns2 = rnn.forward(small_constant)\ns3 = rnn.forward(small_falling)\ns4 = rnn.forward(small_curve)\ns1,s2,s3,s4\n\n(0.2, 0.1, 0.8, 0.2)\n\n\n\n\n\n\n\nFigure 8: Input validation data arrays with the model prediction\n\n\n\n\nThe model can correctly predict all of our validation data! This means that it can generalize and find some basic patterns in the data and is not just remembering our training dataset. This is quite an achievement. With these few lines of code, we’re able to create a model that can predict any kind of rising / constant / falling sequence of values, without changing any parameter. Imagine how far this approach can go when we scale up the model and increase its complexity!"
  },
  {
    "objectID": "posts/uninformed-search/index.html",
    "href": "posts/uninformed-search/index.html",
    "title": "Uninformed Search Algorithms",
    "section": "",
    "text": "Search is a very broad term in the field of artificial intelligence. What seems so intuitive to us as humans, such as finding an efficient route from city A to city B when looking at a map, is not so straightforward when we want a computer to do the same thing.\nAt very first glance, a simple thing such as search may seem irrelevant when we want to create artificial intelligence. It may seem boring, over-simplistic or useless to solve this problem, but it turns out that most of what’s considered artificial intelligence today is per definition just a search problem and in fact, it is solved by one of these simple search algorithms - gradient descent - which is just as simple as any of its relatives, be it from the informed or uninformed, global or local category.\n\n\n\n\n\n\nNote\n\n\n\nThis article is based on the chapter about uninformed search in “Artificial Intelligence: A Modern Approach, 4th Edition” by Stuart Russell and Peter Norvig.\n\n\n\n\nThe first important distinction to make for understanding search is to differentiate between informed and uninformed search. You can think of the former, like searching for your phone in your living room when you have no idea where you left it. The latter is like searching for it while giving it a call, so you hear the general direction where it might be.\nThere are different kinds of uninformed search algorithms, but the ones we’ll be focusing on in this article are Depth-First, Breadth-First and Uniform-Cost search. Each section will briefly introduce the concept and follow up with a concise Python implementation that you can copy and play around with.\n\n\n\nWe’ll start by defining an example scenario for our search. A common search problem is finding a path to a goal state, for example, you may wonder how to find the quickest way from your home to work.\n\n\nGraph initialization\nnodes = ['A', 'B', 'C', 'D', 'E', 'F']\nedges = [\n    ('A', 'B', 2),\n    ('A', 'C', 10),\n    ('B', 'C', 3),\n    ('B', 'D', 4),\n    ('C', 'E', 2),\n    ('D', 'F', 3),\n    ('E', 'F', 2),\n    ('E', 'B', 2)\n]\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# Create an empty graph\ngraph = nx.Graph()\n\n# Add nodes to the graph\ngraph.add_nodes_from(nodes)\n\n# Add edges with associated costs\nfor edge in edges:\n    graph.add_edge(edge[0], edge[1], cost=edge[2])\n\n\n\n\nVisualization functions\nimport colorsys\n\n# Generates a color palette from fully-saturated to unsaturated with the\n# specified amount of steps.\ndef generate_color_gradient(num_steps):\n    hue = 0.4  # Neon Green\n    lightness = 0.5\n    saturation_step = 1.0 / num_steps\n\n    colors = []\n    for i in range(num_steps):\n        # Calculate the current saturation\n        saturation = (i+1) * saturation_step\n\n        # Convert HSL to RGB\n        rgb_color = colorsys.hls_to_rgb(hue, lightness, saturation)\n\n        # Convert RGB values to 8-bit integers\n        rgb_color = tuple(int(value * 255) for value in rgb_color)\n\n        colors.append(rgb_color)\n\n    colors.reverse() # Saturated -&gt; Unsaturated\n    node_colors = [(r/255, g/255, b/255) for r, g, b in colors] # Conversion for networkx\n\n    return node_colors\n\n# Visualizes a graph and tints all visited nodes with a gradient (earliest to latest)\ndef visualize(graph, visited_nodes=[], start_node=\"A\", end_node=\"F\"):\n    pos = nx.spring_layout(graph, seed=42)  # or nx.circular_layout(graph)\n\n    labels = {edge[:2]: edge[2] for edge in edges}  # Dictionary for edge labels\n    color_array = generate_color_gradient(len(visited_nodes)) if visited_nodes else []\n\n    node_colors = []\n    for node in graph.nodes():\n      if node in visited_nodes:\n        # Tint visited nodes from earliest to latest visit\n        node_colors.append(color_array[visited_nodes.index(node)])\n      elif node == start_node or node == end_node:\n        # If there are no visited nodes, mark start and goal with main colors\n        node_colors.append(generate_color_gradient(1)[0])\n      else:\n        # Default color for nodes\n        node_colors.append('silver')\n\n    nx.draw(graph, pos, with_labels=True, node_size=500, node_color=node_colors, font_size=10, font_color='black')\n    nx.draw_networkx_edge_labels(graph, pos, edge_labels=labels)\n\n    plt.axis('off')\n    plt.show()\n\ndef evaluate(algorithm, graph):\n  visited = algorithm(graph)\n  visualize(graph, visited)\n\n\n\n\n\n\n\nFigure 1: Graph of locations A-F, with green locations (A,F) being the start- and goal-states. The edges represent the cost of transitioning from one state to another."
  },
  {
    "objectID": "posts/uninformed-search/index.html#informed-vs-uninformed",
    "href": "posts/uninformed-search/index.html#informed-vs-uninformed",
    "title": "Uninformed Search Algorithms",
    "section": "",
    "text": "The first important distinction to make for understanding search is to differentiate between informed and uninformed search. You can think of the former, like searching for your phone in your living room when you have no idea where you left it. The latter is like searching for it while giving it a call, so you hear the general direction where it might be.\nThere are different kinds of uninformed search algorithms, but the ones we’ll be focusing on in this article are Depth-First, Breadth-First and Uniform-Cost search. Each section will briefly introduce the concept and follow up with a concise Python implementation that you can copy and play around with."
  },
  {
    "objectID": "posts/uninformed-search/index.html#setup",
    "href": "posts/uninformed-search/index.html#setup",
    "title": "Uninformed Search Algorithms",
    "section": "",
    "text": "We’ll start by defining an example scenario for our search. A common search problem is finding a path to a goal state, for example, you may wonder how to find the quickest way from your home to work.\n\n\nGraph initialization\nnodes = ['A', 'B', 'C', 'D', 'E', 'F']\nedges = [\n    ('A', 'B', 2),\n    ('A', 'C', 10),\n    ('B', 'C', 3),\n    ('B', 'D', 4),\n    ('C', 'E', 2),\n    ('D', 'F', 3),\n    ('E', 'F', 2),\n    ('E', 'B', 2)\n]\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# Create an empty graph\ngraph = nx.Graph()\n\n# Add nodes to the graph\ngraph.add_nodes_from(nodes)\n\n# Add edges with associated costs\nfor edge in edges:\n    graph.add_edge(edge[0], edge[1], cost=edge[2])\n\n\n\n\nVisualization functions\nimport colorsys\n\n# Generates a color palette from fully-saturated to unsaturated with the\n# specified amount of steps.\ndef generate_color_gradient(num_steps):\n    hue = 0.4  # Neon Green\n    lightness = 0.5\n    saturation_step = 1.0 / num_steps\n\n    colors = []\n    for i in range(num_steps):\n        # Calculate the current saturation\n        saturation = (i+1) * saturation_step\n\n        # Convert HSL to RGB\n        rgb_color = colorsys.hls_to_rgb(hue, lightness, saturation)\n\n        # Convert RGB values to 8-bit integers\n        rgb_color = tuple(int(value * 255) for value in rgb_color)\n\n        colors.append(rgb_color)\n\n    colors.reverse() # Saturated -&gt; Unsaturated\n    node_colors = [(r/255, g/255, b/255) for r, g, b in colors] # Conversion for networkx\n\n    return node_colors\n\n# Visualizes a graph and tints all visited nodes with a gradient (earliest to latest)\ndef visualize(graph, visited_nodes=[], start_node=\"A\", end_node=\"F\"):\n    pos = nx.spring_layout(graph, seed=42)  # or nx.circular_layout(graph)\n\n    labels = {edge[:2]: edge[2] for edge in edges}  # Dictionary for edge labels\n    color_array = generate_color_gradient(len(visited_nodes)) if visited_nodes else []\n\n    node_colors = []\n    for node in graph.nodes():\n      if node in visited_nodes:\n        # Tint visited nodes from earliest to latest visit\n        node_colors.append(color_array[visited_nodes.index(node)])\n      elif node == start_node or node == end_node:\n        # If there are no visited nodes, mark start and goal with main colors\n        node_colors.append(generate_color_gradient(1)[0])\n      else:\n        # Default color for nodes\n        node_colors.append('silver')\n\n    nx.draw(graph, pos, with_labels=True, node_size=500, node_color=node_colors, font_size=10, font_color='black')\n    nx.draw_networkx_edge_labels(graph, pos, edge_labels=labels)\n\n    plt.axis('off')\n    plt.show()\n\ndef evaluate(algorithm, graph):\n  visited = algorithm(graph)\n  visualize(graph, visited)\n\n\n\n\n\n\n\nFigure 1: Graph of locations A-F, with green locations (A,F) being the start- and goal-states. The edges represent the cost of transitioning from one state to another."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "On this blog I’m sharing project implementations and guides on topics related to machine learning."
  }
]